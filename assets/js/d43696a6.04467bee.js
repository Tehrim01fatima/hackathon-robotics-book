"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[912],{6906:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2/lesson-2-unity-robotics-hub","title":"Lesson 2 - Unity Robotics Hub - Human-Robot Interaction, Visualization, Immersive Environments","description":"Learning Objectives","source":"@site/docs/module-2/lesson-2-unity-robotics-hub.md","sourceDirName":"module-2","slug":"/module-2/lesson-2-unity-robotics-hub","permalink":"/hackathon-robotics-book/docs/module-2/lesson-2-unity-robotics-hub","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-2/lesson-2-unity-robotics-hub.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 2 - Unity Robotics Hub - Human-Robot Interaction, Visualization, Immersive Environments","sidebar_label":"Unity Robotics Hub"},"sidebar":"textbookSidebar","previous":{"title":"Gazebo Physics & Sensors","permalink":"/hackathon-robotics-book/docs/module-2/lesson-1-gazebo-physics-sensors"},"next":{"title":"Simulated Cameras & Sensor Fusion","permalink":"/hackathon-robotics-book/docs/module-2/lesson-3-simulated-cameras-sensor-fusion"}}');var o=i(4848),a=i(8453);const r={title:"Lesson 2 - Unity Robotics Hub - Human-Robot Interaction, Visualization, Immersive Environments",sidebar_label:"Unity Robotics Hub"},s="Lesson 2: Unity Robotics Hub - Human-Robot Interaction, Visualization, Immersive Environments",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Unity Robotics Hub Overview",id:"unity-robotics-hub-overview",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Human-Robot Interaction (HRI) in Unity",id:"human-robot-interaction-hri-in-unity",level:2},{value:"UI Design for Robot Control",id:"ui-design-for-robot-control",level:3},{value:"Interactive Robot Manipulation",id:"interactive-robot-manipulation",level:3},{value:"Photorealistic Environment Creation",id:"photorealistic-environment-creation",level:2},{value:"Environment Design Principles",id:"environment-design-principles",level:3},{value:"Creating Realistic Materials",id:"creating-realistic-materials",level:3},{value:"Lighting and Post-Processing",id:"lighting-and-post-processing",level:3},{value:"Unity-Ros Integration for Digital Twins",id:"unity-ros-integration-for-digital-twins",level:2},{value:"Synchronization Techniques",id:"synchronization-techniques",level:3},{value:"Visualization Tools",id:"visualization-tools",level:3},{value:"VR/AR Integration for Immersive HRI",id:"vrar-integration-for-immersive-hri",level:2},{value:"Performance Optimization for Real-time Simulation",id:"performance-optimization-for-real-time-simulation",level:2},{value:"Level of Detail (LOD) System",id:"level-of-detail-lod-system",level:3},{value:"Occlusion Culling",id:"occlusion-culling",level:3},{value:"Hands-on Exercise 2.2: Create a Unity HRI Interface",id:"hands-on-exercise-22-create-a-unity-hri-interface",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"lesson-2-unity-robotics-hub---human-robot-interaction-visualization-immersive-environments",children:"Lesson 2: Unity Robotics Hub - Human-Robot Interaction, Visualization, Immersive Environments"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Integrate Unity with ROS 2 for real-time robot simulation and visualization"}),"\n",(0,o.jsx)(e.li,{children:"Implement human-robot interaction (HRI) interfaces using Unity's UI and input systems"}),"\n",(0,o.jsx)(e.li,{children:"Create photorealistic environments for humanoid robot simulation and testing"}),"\n",(0,o.jsx)(e.li,{children:"Develop immersive visualization tools for robot teleoperation and monitoring"}),"\n",(0,o.jsx)(e.li,{children:"Connect Unity simulation to real robot hardware for digital twin applications"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Unity has emerged as a powerful platform for robotics development, particularly for creating high-fidelity visualizations and human-robot interaction (HRI) interfaces. The Unity Robotics Hub provides essential tools and packages that bridge the gap between Unity's real-time 3D capabilities and ROS 2's robotics framework. For humanoid robots, Unity offers unparalleled visualization quality and interaction possibilities that complement physics-based simulation environments like Gazebo."}),"\n",(0,o.jsx)(e.p,{children:"Unity's real-time rendering capabilities, extensive asset ecosystem, and cross-platform deployment options make it an ideal choice for creating immersive interfaces for humanoid robot operation, monitoring, and interaction. This lesson will explore how to leverage Unity's capabilities for robotics applications, focusing on visualization and HRI aspects that are crucial for humanoid robot systems."}),"\n",(0,o.jsx)(e.h2,{id:"unity-robotics-hub-overview",children:"Unity Robotics Hub Overview"}),"\n",(0,o.jsx)(e.p,{children:"The Unity Robotics Hub is a collection of packages, tools, and resources that facilitate robotics development in Unity. It provides the essential infrastructure for connecting Unity applications to ROS 2 networks."}),"\n",(0,o.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS TCP Connector"}),": Enables communication between Unity and ROS 2 nodes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robotics Library"}),": Provides utilities for common robotics tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Simulation Framework"}),": Tools for creating and managing robot simulations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"UI Toolkit"}),": Components for creating HRI interfaces"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:"graph TB\n    A[Unity Application] --\x3e B[ROS TCP Connector]\n    B --\x3e C{ROS 2 Network}\n    C --\x3e D[ROS 2 Nodes]\n    C --\x3e E[RViz Visualization]\n    C --\x3e F[Robot Hardware]\n    A --\x3e G[Robot Visualization]\n    A --\x3e H[HRI Interface]\n    D --\x3e I[Sensor Data]\n    D --\x3e J[Control Commands]\n    I --\x3e B\n    J --\x3e B\n"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.em,{children:"Figure 1: Unity Robotics Hub architecture showing the connection between Unity and ROS 2 ecosystem."})}),"\n",(0,o.jsx)(e.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,o.jsx)(e.p,{children:"To get started with Unity Robotics Hub:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Install Unity Hub and Unity 2022.3 LTS or newer"}),"\n",(0,o.jsx)(e.li,{children:"Create a new 3D project"}),"\n",(0,o.jsxs)(e.li,{children:["Install the Unity Robotics packages via the Package Manager:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"ROS TCP Connector"}),"\n",(0,o.jsx)(e.li,{children:"Unity Robotics Library"}),"\n",(0,o.jsx)(e.li,{children:"Unity Simulation (optional)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Example of initializing ROS communication in Unity\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class RobotController : MonoBehaviour\n{\n    private ROSConnection ros;\n\n    void Start()\n    {\n        // Connect to ROS\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<JointStateMsg>("joint_states");\n        ros.RegisterSubscriber<JointStateMsg>("joint_commands", OnJointCommand);\n    }\n\n    void OnJointCommand(JointStateMsg msg)\n    {\n        // Process joint commands from ROS\n        for (int i = 0; i < msg.name.Count; i++)\n        {\n            // Update joint positions in Unity\n            UpdateJoint(msg.name[i], msg.position[i]);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"human-robot-interaction-hri-in-unity",children:"Human-Robot Interaction (HRI) in Unity"}),"\n",(0,o.jsx)(e.p,{children:"Human-Robot Interaction design is crucial for humanoid robots, especially when they operate in human-centered environments. Unity provides powerful tools for creating intuitive and effective HRI interfaces."}),"\n",(0,o.jsx)(e.h3,{id:"ui-design-for-robot-control",children:"UI Design for Robot Control"}),"\n",(0,o.jsx)(e.p,{children:"Unity's UI system allows for creating sophisticated control interfaces:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing Unity.Robotics.ROSTCPConnector.ROSGeometry;\n\npublic class RobotControlPanel : MonoBehaviour\n{\n    [SerializeField] private Button walkButton;\n    [SerializeField] private Button stopButton;\n    [SerializeField] private Slider speedSlider;\n    [SerializeField] private Text statusText;\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        walkButton.onClick.AddListener(StartWalking);\n        stopButton.onClick.AddListener(StopRobot);\n        speedSlider.onValueChanged.AddListener(UpdateSpeed);\n    }\n\n    void StartWalking()\n    {\n        // Send command to ROS\n        var command = new TwistMsg();\n        command.linear.x = speedSlider.value;\n        ros.Publish("cmd_vel", command);\n        statusText.text = "Robot walking...";\n    }\n\n    void StopRobot()\n    {\n        var command = new TwistMsg();\n        command.linear.x = 0;\n        command.angular.z = 0;\n        ros.Publish("cmd_vel", command);\n        statusText.text = "Robot stopped";\n    }\n\n    void UpdateSpeed(float speed)\n    {\n        statusText.text = $"Speed: {speed:F2} m/s";\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"interactive-robot-manipulation",children:"Interactive Robot Manipulation"}),"\n",(0,o.jsx)(e.p,{children:"Unity allows for direct manipulation of robot components in the simulation:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class InteractiveJoint : MonoBehaviour\n{\n    [SerializeField] private string jointName;\n    [SerializeField] private float rotationSpeed = 10f;\n    [SerializeField] private float minAngle = -90f;\n    [SerializeField] private float maxAngle = 90f;\n\n    private bool isBeingDragged = false;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n    }\n\n    void OnMouseDown()\n    {\n        isBeingDragged = true;\n    }\n\n    void OnMouseDrag()\n    {\n        if (isBeingDragged)\n        {\n            // Calculate rotation based on mouse movement\n            Vector3 mouseDelta = new Vector3(Input.GetAxis("Mouse Y"), -Input.GetAxis("Mouse X"), 0) * rotationSpeed * Time.deltaTime;\n            transform.Rotate(mouseDelta, Space.Self);\n\n            // Clamp rotation to valid range\n            Vector3 currentRotation = transform.localEulerAngles;\n            currentRotation.x = Mathf.Clamp(currentRotation.x, minAngle, maxAngle);\n            transform.localEulerAngles = currentRotation;\n\n            // Send joint position to ROS\n            SendJointPosition();\n        }\n    }\n\n    void OnMouseUp()\n    {\n        isBeingDragged = false;\n    }\n\n    void SendJointPosition()\n    {\n        // Calculate joint angle and send to ROS\n        float jointAngle = transform.localEulerAngles.x;\n        if (jointAngle > 180) jointAngle -= 360; // Convert to -180 to 180 range\n\n        // Publish to ROS (simplified example)\n        var jointState = new JointStateMsg();\n        jointState.name = new System.Collections.Generic.List<string> { jointName };\n        jointState.position = new System.Collections.Generic.List<double> { jointAngle * Mathf.Deg2Rad };\n        ros.Publish("joint_commands", jointState);\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"photorealistic-environment-creation",children:"Photorealistic Environment Creation"}),"\n",(0,o.jsx)(e.p,{children:"Unity's rendering capabilities enable the creation of photorealistic environments for humanoid robot simulation and testing."}),"\n",(0,o.jsx)(e.h3,{id:"environment-design-principles",children:"Environment Design Principles"}),"\n",(0,o.jsx)(e.p,{children:"When creating environments for humanoid robots, consider:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Scale and Proportions"}),": Ensure environments match human-scale dimensions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation Spaces"}),": Design walkable areas with appropriate clearances"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interaction Points"}),": Place objects at reachable heights and positions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Lighting Conditions"}),": Simulate realistic lighting for perception testing"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"creating-realistic-materials",children:"Creating Realistic Materials"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Material configuration for realistic robot surfaces\nusing UnityEngine;\n\npublic class RobotMaterialSetup : MonoBehaviour\n{\n    [Header("Material Properties")]\n    [SerializeField] private Material robotBodyMaterial;\n    [SerializeField] private Material sensorMaterial;\n    [SerializeField] private Material jointMaterial;\n\n    [Header("Visual Effects")]\n    [SerializeField] private float metalness = 0.7f;\n    [SerializeField] private float smoothness = 0.8f;\n    [SerializeField] private Color baseColor = Color.gray;\n\n    void Start()\n    {\n        ConfigureRobotMaterials();\n    }\n\n    void ConfigureRobotMaterials()\n    {\n        if (robotBodyMaterial != null)\n        {\n            robotBodyMaterial.SetColor("_BaseColor", baseColor);\n            robotBodyMaterial.SetFloat("_Metallic", metalness);\n            robotBodyMaterial.SetFloat("_Smoothness", smoothness);\n        }\n\n        if (sensorMaterial != null)\n        {\n            sensorMaterial.SetColor("_BaseColor", Color.black);\n            sensorMaterial.SetFloat("_Metallic", 0.9f);\n            sensorMaterial.SetFloat("_Smoothness", 0.95f);\n        }\n\n        if (jointMaterial != null)\n        {\n            jointMaterial.SetColor("_BaseColor", Color.blue);\n            jointMaterial.SetFloat("_Metallic", 0.6f);\n            jointMaterial.SetFloat("_Smoothness", 0.7f);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"lighting-and-post-processing",children:"Lighting and Post-Processing"}),"\n",(0,o.jsx)(e.p,{children:"For photorealistic rendering, implement proper lighting and post-processing:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.Rendering;\nusing UnityEngine.Rendering.Universal;\n\npublic class EnvironmentLighting : MonoBehaviour\n{\n    [Header("Lighting Setup")]\n    [SerializeField] private Light mainLight;\n    [SerializeField] private Light[] fillLights;\n    [SerializeField] private Volume postProcessingVolume;\n\n    [Header("Time of Day")]\n    [SerializeField] private float timeOfDay = 12f; // 0-24 hours\n\n    void Update()\n    {\n        UpdateLightingBasedOnTime();\n    }\n\n    void UpdateLightingBasedOnTime()\n    {\n        // Calculate sun position based on time of day\n        float sunAngle = (timeOfDay - 6) * 15; // 15 degrees per hour\n        mainLight.transform.rotation = Quaternion.Euler(sunAngle, 0, 0);\n\n        // Adjust light intensity based on time\n        float intensity = Mathf.Clamp01(1f - Mathf.Abs(timeOfDay - 12f) / 6f);\n        mainLight.intensity = Mathf.Lerp(0.5f, 2f, intensity);\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"unity-ros-integration-for-digital-twins",children:"Unity-Ros Integration for Digital Twins"}),"\n",(0,o.jsx)(e.p,{children:"Creating a digital twin requires seamless synchronization between the Unity simulation and the real robot or physics simulation."}),"\n",(0,o.jsx)(e.h3,{id:"synchronization-techniques",children:"Synchronization Techniques"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class RobotDigitalTwin : MonoBehaviour\n{\n    [Header("ROS Topics")]\n    [SerializeField] private string jointStatesTopic = "joint_states";\n    [SerializeField] private string tfTopic = "tf";\n\n    [Header("Robot Configuration")]\n    [SerializeField] private List<Transform> jointTransforms;\n    [SerializeField] private List<string> jointNames;\n\n    private Dictionary<string, Transform> jointMap;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterSubscriber<JointStateMsg>(jointStatesTopic, OnJointStateReceived);\n\n        // Create joint mapping\n        jointMap = new Dictionary<string, Transform>();\n        for (int i = 0; i < jointNames.Count && i < jointTransforms.Count; i++)\n        {\n            jointMap[jointNames[i]] = jointTransforms[i];\n        }\n    }\n\n    void OnJointStateReceived(JointStateMsg jointState)\n    {\n        for (int i = 0; i < jointState.name.Count; i++)\n        {\n            string jointName = jointState.name[i];\n            double jointPosition = jointState.position[i];\n\n            if (jointMap.ContainsKey(jointName))\n            {\n                Transform jointTransform = jointMap[jointName];\n\n                // Apply joint position (assuming rotational joints)\n                jointTransform.localRotation = Quaternion.Euler(\n                    0, 0, (float)(jointPosition * Mathf.Rad2Deg)\n                );\n            }\n        }\n    }\n\n    // Send commands to real robot\n    public void SendJointCommand(string jointName, float position)\n    {\n        var jointCmd = new JointStateMsg();\n        jointCmd.name = new List<string> { jointName };\n        jointCmd.position = new List<double> { position };\n\n        ros.Publish("joint_commands", jointCmd);\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"visualization-tools",children:"Visualization Tools"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides powerful tools for visualizing robot data and sensor information:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class SensorVisualization : MonoBehaviour\n{\n    [Header("Laser Scan Visualization")]\n    [SerializeField] private GameObject laserScanPrefab;\n    [SerializeField] private Transform scanOrigin;\n    [SerializeField] private Color scanColor = Color.red;\n\n    [Header("Camera Feed")]\n    [SerializeField] private RawImage cameraFeedImage;\n    [SerializeField] private string cameraTopic = "camera/image_raw";\n\n    private List<GameObject> scanPoints;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterSubscriber<LaserScanMsg>("scan", OnLaserScanReceived);\n\n        scanPoints = new List<GameObject>();\n    }\n\n    void OnLaserScanReceived(LaserScanMsg scan)\n    {\n        // Clear previous scan points\n        foreach (GameObject point in scanPoints)\n        {\n            Destroy(point);\n        }\n        scanPoints.Clear();\n\n        // Create new scan points\n        for (int i = 0; i < scan.ranges.Count; i++)\n        {\n            float range = (float)scan.ranges[i];\n            if (range < scan.range_max && range > scan.range_min)\n            {\n                float angle = (float)scan.angle_min + i * (float)scan.angle_increment;\n\n                Vector3 pointPos = scanOrigin.position +\n                    new Vector3(\n                        range * Mathf.Cos(angle),\n                        0,\n                        range * Mathf.Sin(angle)\n                    );\n\n                GameObject point = Instantiate(laserScanPrefab, pointPos, Quaternion.identity);\n                point.GetComponent<Renderer>().material.color = scanColor;\n                scanPoints.Add(point);\n            }\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"vrar-integration-for-immersive-hri",children:"VR/AR Integration for Immersive HRI"}),"\n",(0,o.jsx)(e.p,{children:"Unity's VR and AR capabilities enable immersive interaction with humanoid robots:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'#if UNITY_EDITOR || UNITY_STANDALONE\nusing UnityEngine.XR;\nusing UnityEngine.XR.Interaction.Toolkit;\n#endif\n\npublic class VRRobotController : MonoBehaviour\n{\n    [Header("VR Controllers")]\n    [SerializeField] private XRNode controllerNode;\n    [SerializeField] private XRBaseController leftController;\n    [SerializeField] private XRBaseController rightController;\n\n    [Header("Robot Control")]\n    [SerializeField] private Transform robotRoot;\n    [SerializeField] private float moveSpeed = 1f;\n    [SerializeField] private float rotateSpeed = 50f;\n\n    private InputDevice controller;\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n    }\n\n    void Update()\n    {\n        controller = InputDevices.GetDeviceAtXRNode(controllerNode);\n\n        if (controller.isValid)\n        {\n            // Get controller input\n            Vector2 primaryAxis = Vector2.zero;\n            controller.TryGetFeatureValue(CommonUsages.primary2DAxis, out primaryAxis);\n\n            // Move robot based on controller input\n            Vector3 movement = new Vector3(primaryAxis.x, 0, primaryAxis.y) * moveSpeed * Time.deltaTime;\n            robotRoot.Translate(movement, Space.World);\n\n            // Send movement commands to ROS\n            SendRobotCommands(primaryAxis);\n        }\n    }\n\n    void SendRobotCommands(Vector2 input)\n    {\n        var twist = new TwistMsg();\n        twist.linear.x = input.y * moveSpeed;\n        twist.angular.z = input.x * rotateSpeed;\n\n        ros.Publish("cmd_vel", twist);\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization-for-real-time-simulation",children:"Performance Optimization for Real-time Simulation"}),"\n",(0,o.jsx)(e.p,{children:"For real-time humanoid robot simulation in Unity, performance optimization is crucial:"}),"\n",(0,o.jsx)(e.h3,{id:"level-of-detail-lod-system",children:"Level of Detail (LOD) System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\n[RequireComponent(typeof(LODGroup))]\npublic class RobotLODController : MonoBehaviour\n{\n    [Header("LOD Configuration")]\n    [SerializeField] private float[] lodDistances = { 10f, 30f, 60f };\n    [SerializeField] private Renderer[] highDetailRenderers;\n    [SerializeField] private Renderer[] lowDetailRenderers;\n\n    private LODGroup lodGroup;\n    private Camera mainCamera;\n\n    void Start()\n    {\n        lodGroup = GetComponent<LODGroup>();\n        mainCamera = Camera.main;\n\n        SetupLOD();\n    }\n\n    void SetupLOD()\n    {\n        LOD[] lods = new LOD[lodDistances.Length];\n\n        for (int i = 0; i < lodDistances.Length; i++)\n        {\n            float screenRelativeTransitionHeight = lodDistances[i] /\n                (mainCamera.farClipPlane * 0.5f);\n\n            // Create combined renderer array for each LOD level\n            Renderer[] renderers = GetRenderersForLOD(i);\n            lods[i] = new LOD(screenRelativeTransitionHeight, renderers);\n        }\n\n        lodGroup.SetLODs(lods);\n        lodGroup.RecalculateBounds();\n    }\n\n    Renderer[] GetRenderersForLOD(int lodLevel)\n    {\n        // Return appropriate renderers based on LOD level\n        if (lodLevel == 0) return highDetailRenderers; // Highest detail\n        else return lowDetailRenderers; // Lower detail\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"occlusion-culling",children:"Occlusion Culling"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class RobotOcclusionCulling : MonoBehaviour\n{\n    [Header("Occlusion Settings")]\n    [SerializeField] private float updateInterval = 0.1f;\n    [SerializeField] private LayerMask occlusionMask = -1;\n\n    private WaitForSeconds updateWait;\n    private Renderer[] robotRenderers;\n\n    void Start()\n    {\n        updateWait = new WaitForSeconds(updateInterval);\n        robotRenderers = GetComponentsInChildren<Renderer>();\n\n        StartCoroutine(OcclusionCheckRoutine());\n    }\n\n    System.Collections.IEnumerator OcclusionCheckRoutine()\n    {\n        while (true)\n        {\n            CheckOcclusion();\n            yield return updateWait;\n        }\n    }\n\n    void CheckOcclusion()\n    {\n        Camera cam = Camera.main;\n        foreach (Renderer renderer in robotRenderers)\n        {\n            if (renderer != null)\n            {\n                // Check if renderer is visible to camera\n                bool isVisible = GeometryUtility.TestPlanesAABB(\n                    GeometryUtility.CalculateFrustumPlanes(cam),\n                    renderer.bounds\n                );\n\n                renderer.enabled = isVisible;\n            }\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercise-22-create-a-unity-hri-interface",children:"Hands-on Exercise 2.2: Create a Unity HRI Interface"}),"\n",(0,o.jsx)(e.p,{children:"Create a Unity scene with a humanoid robot model and implement basic HRI controls:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Create a new Unity 3D project"}),"\n",(0,o.jsx)(e.li,{children:"Import the Unity Robotics packages via Package Manager"}),"\n",(0,o.jsx)(e.li,{children:"Create a basic humanoid robot model (or import a URDF via Unity's URDF Importer)"}),"\n",(0,o.jsxs)(e.li,{children:["Implement a basic HRI interface with:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Robot movement controls"}),"\n",(0,o.jsx)(e.li,{children:"Joint position sliders"}),"\n",(0,o.jsx)(e.li,{children:"Sensor visualization"}),"\n",(0,o.jsx)(e.li,{children:"Basic ROS communication"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Example HRI interface controller\nusing UnityEngine;\nusing UnityEngine.UI;\nusing System.Collections.Generic;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.ROSGeometry;\n\npublic class BasicHRIController : MonoBehaviour\n{\n    [Header("UI Elements")]\n    [SerializeField] private Button moveForwardButton;\n    [SerializeField] private Button moveBackwardButton;\n    [SerializeField] private Button turnLeftButton;\n    [SerializeField] private Button turnRightButton;\n    [SerializeField] private Slider[] jointSliders;\n    [SerializeField] private Text statusText;\n\n    [Header("Robot Configuration")]\n    [SerializeField] private List<string> jointNames;\n    [SerializeField] private Transform[] jointTransforms;\n\n    private ROSConnection ros;\n    private readonly string cmdVelTopic = "cmd_vel";\n    private readonly string jointCmdTopic = "joint_commands";\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        SetupUI();\n        statusText.text = "HRI Interface Ready";\n    }\n\n    void SetupUI()\n    {\n        moveForwardButton.onClick.AddListener(() => MoveRobot(1, 0));\n        moveBackwardButton.onClick.AddListener(() => MoveRobot(-1, 0));\n        turnLeftButton.onClick.AddListener(() => MoveRobot(0, 1));\n        turnRightButton.onClick.AddListener(() => MoveRobot(0, -1));\n\n        // Setup joint sliders\n        for (int i = 0; i < jointSliders.Length && i < jointNames.Count; i++)\n        {\n            int index = i; // Capture for closure\n            jointSliders[i].onValueChanged.AddListener((value) =>\n                UpdateJointPosition(index, value));\n        }\n    }\n\n    void MoveRobot(float linear, float angular)\n    {\n        var twist = new TwistMsg();\n        twist.linear = new Vector3Msg { x = linear * 0.5, y = 0, z = 0 };\n        twist.angular = new Vector3Msg { x = 0, y = 0, z = angular * 0.5 };\n\n        ros.Publish(cmdVelTopic, twist);\n        statusText.text = $"Moving: Linear={linear}, Angular={angular}";\n    }\n\n    void UpdateJointPosition(int jointIndex, float position)\n    {\n        if (jointIndex < jointNames.Count && jointIndex < jointTransforms.Length)\n        {\n            // Update Unity visualization\n            jointTransforms[jointIndex].localRotation =\n                Quaternion.Euler(0, 0, position * 180);\n\n            // Send to ROS\n            var jointState = new JointStateMsg();\n            jointState.name = new List<string> { jointNames[jointIndex] };\n            jointState.position = new List<double> { position * Mathf.PI };\n\n            ros.Publish(jointCmdTopic, jointState);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity Robotics Hub"})," provides essential tools for connecting Unity to ROS 2 networks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human-Robot Interaction"})," interfaces can be created using Unity's powerful UI system"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Photorealistic environments"})," enable high-fidelity simulation and visualization"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Digital twin synchronization"})," requires careful management of data flow between Unity and ROS"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Performance optimization"})," is crucial for real-time humanoid robot simulation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"VR/AR integration"})," enables immersive HRI experiences for complex robot systems"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"How does Unity's rendering quality compare to traditional robotics simulators for perception development?"}),"\n",(0,o.jsx)(e.li,{children:"What are the advantages and disadvantages of using Unity vs. Gazebo for humanoid robot simulation?"}),"\n",(0,o.jsx)(e.li,{children:"How can Unity's VR capabilities enhance human-robot interaction for complex tasks?"}),"\n",(0,o.jsx)(e.li,{children:"What challenges arise when synchronizing Unity visualizations with real-time robot data?"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,o.jsxs)(e.p,{children:["Unity Technologies. (2023). ",(0,o.jsx)(e.em,{children:"Unity Robotics Hub Documentation"}),". Retrieved from ",(0,o.jsx)(e.a,{href:"https://docs.unity3d.com/Packages/com.unity.robotics.hub@latest",children:"https://docs.unity3d.com/Packages/com.unity.robotics.hub@latest"})]}),"\n",(0,o.jsxs)(e.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,o.jsx)(e.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,o.jsxs)(e.p,{children:["Vasquez, D., Sallam, S., Haverporth, C., & Arras, K. O. (2012). Human-aware robot navigation: A survey. ",(0,o.jsx)(e.em,{children:"Robotics and Autonomous Systems"}),", 61(12), 1726-1743. ",(0,o.jsx)(e.a,{href:"https://doi.org/10.1016/j.robot.2012.09.011",children:"https://doi.org/10.1016/j.robot.2012.09.011"})]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This lesson explored Unity Robotics Hub and its applications for human-robot interaction, visualization, and immersive environment creation for humanoid robots. We covered the integration of Unity with ROS 2, HRI interface design, photorealistic environment creation, and digital twin synchronization techniques. Unity's capabilities complement physics-based simulators like Gazebo by providing high-fidelity visualization and interaction possibilities essential for humanoid robot development."}),"\n",(0,o.jsx)(e.p,{children:"In the next lesson, we'll explore simulated cameras and sensor fusion techniques, building on both the Gazebo physics simulation and Unity visualization capabilities we've learned about."})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);