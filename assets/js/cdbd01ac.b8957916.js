"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[217],{7151:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-2/lesson-3-simulated-cameras-sensor-fusion","title":"Lesson 3 - Simulated Cameras & Sensor Fusion - RGB-D Cameras, Data Fusion, Perception","description":"Learning Objectives","source":"@site/docs/module-2/lesson-3-simulated-cameras-sensor-fusion.md","sourceDirName":"module-2","slug":"/module-2/lesson-3-simulated-cameras-sensor-fusion","permalink":"/hackathon-robotics-book/docs/module-2/lesson-3-simulated-cameras-sensor-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-2/lesson-3-simulated-cameras-sensor-fusion.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 3 - Simulated Cameras & Sensor Fusion - RGB-D Cameras, Data Fusion, Perception","sidebar_label":"Simulated Cameras & Sensor Fusion"},"sidebar":"textbookSidebar","previous":{"title":"Unity Robotics Hub","permalink":"/hackathon-robotics-book/docs/module-2/lesson-2-unity-robotics-hub"},"next":{"title":"Overview","permalink":"/hackathon-robotics-book/docs/module-3/"}}');var a=s(4848),t=s(8453);const r={title:"Lesson 3 - Simulated Cameras & Sensor Fusion - RGB-D Cameras, Data Fusion, Perception",sidebar_label:"Simulated Cameras & Sensor Fusion"},o="Lesson 3: Simulated Cameras & Sensor Fusion - RGB-D Cameras, Data Fusion, Perception",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"RGB-D Camera Simulation in Gazebo",id:"rgb-d-camera-simulation-in-gazebo",level:2},{value:"Camera Sensor Configuration",id:"camera-sensor-configuration",level:3},{value:"Camera Intrinsics and Extrinsics",id:"camera-intrinsics-and-extrinsics",level:3},{value:"Multiple Camera Systems",id:"multiple-camera-systems",level:3},{value:"RGB-D Camera Simulation in Unity",id:"rgb-d-camera-simulation-in-unity",level:2},{value:"Sensor Fusion Fundamentals",id:"sensor-fusion-fundamentals",level:2},{value:"Kalman Filter for Sensor Fusion",id:"kalman-filter-for-sensor-fusion",level:3},{value:"Particle Filter for Non-linear Systems",id:"particle-filter-for-non-linear-systems",level:3},{value:"Multi-Sensor Fusion Architecture",id:"multi-sensor-fusion-architecture",level:2},{value:"ROS-based Sensor Fusion Node",id:"ros-based-sensor-fusion-node",level:3},{value:"Perception Pipeline Integration",id:"perception-pipeline-integration",level:2},{value:"Hands-on Exercise 2.3: Implement a Simple Sensor Fusion System",id:"hands-on-exercise-23-implement-a-simple-sensor-fusion-system",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lesson-3-simulated-cameras--sensor-fusion---rgb-d-cameras-data-fusion-perception",children:"Lesson 3: Simulated Cameras & Sensor Fusion - RGB-D Cameras, Data Fusion, Perception"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement realistic RGB-D camera simulation in both Gazebo and Unity environments"}),"\n",(0,a.jsx)(n.li,{children:"Design and implement sensor fusion algorithms combining multiple sensor inputs"}),"\n",(0,a.jsx)(n.li,{children:"Create perception pipelines that process fused sensor data for humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the accuracy and reliability of sensor fusion systems"}),"\n",(0,a.jsx)(n.li,{children:"Apply sensor fusion techniques to improve robot localization and mapping"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Sensor fusion is a critical technology for humanoid robots operating in complex, dynamic environments. By combining data from multiple sensors\u2014cameras, LiDAR, IMU, force/torque sensors, and others\u2014robots can achieve more robust and accurate perception than would be possible with any single sensor modality. This lesson explores the simulation of RGB-D cameras and the implementation of sensor fusion algorithms that enable humanoid robots to understand and navigate their environment effectively."}),"\n",(0,a.jsx)(n.p,{children:"RGB-D cameras provide both color (RGB) and depth (D) information, making them particularly valuable for humanoid robots that need to perceive and interact with objects in 3D space. The fusion of RGB-D data with other sensor modalities creates a comprehensive understanding of the environment that enables complex manipulation and navigation tasks."}),"\n",(0,a.jsx)(n.h2,{id:"rgb-d-camera-simulation-in-gazebo",children:"RGB-D Camera Simulation in Gazebo"}),"\n",(0,a.jsx)(n.p,{children:"RGB-D cameras are essential sensors for humanoid robots, providing both visual and depth information that enables object recognition, scene understanding, and navigation. Gazebo provides realistic simulation of RGB-D cameras with configurable parameters that match real-world sensors."}),"\n",(0,a.jsx)(n.h3,{id:"camera-sensor-configuration",children:"Camera Sensor Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Simulated RGB-D camera configuration --\x3e\n<gazebo reference="rgbd_camera_link">\n  <sensor name="rgbd_camera" type="depth">\n    <update_rate>30</update_rate>\n    <camera name="head_camera">\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="rgbd_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <baseline>0.2</baseline>\n      <always_on>true</always_on>\n      <update_rate>30.0</update_rate>\n      <camera_name>camera</camera_name>\n      <frame_name>camera_depth_optical_frame</frame_name>\n      <point_cloud_topic>depth/points</point_cloud_topic>\n      <depth_image_topic>depth/image_raw</depth_image_topic>\n      <depth_image_camera_info_topic>depth/camera_info</depth_image_camera_info_topic>\n      <image_topic>depth/image_rect_color</image_topic>\n      <camera_info_topic>depth/camera_info</camera_info_topic>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"camera-intrinsics-and-extrinsics",children:"Camera Intrinsics and Extrinsics"}),"\n",(0,a.jsx)(n.p,{children:"Proper camera calibration is essential for accurate depth estimation and 3D reconstruction:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Camera calibration parameters --\x3e\n<sensor name="calibrated_camera" type="camera">\n  <camera name="calibrated_camera">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n    <distortion>\n      <k1>0.0</k1>\n      <k2>0.0</k2>\n      <k3>0.0</k3>\n      <p1>0.0</p1>\n      <p2>0.0</p2>\n      <center>320 240</center>\n    </distortion>\n  </camera>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"multiple-camera-systems",children:"Multiple Camera Systems"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots, multiple cameras provide enhanced perception capabilities:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Stereo camera setup --\x3e\n<link name="stereo_camera_left_link">\n  <visual>\n    <geometry>\n      <box size="0.02 0.04 0.02"/>\n    </geometry>\n  </visual>\n</link>\n\n<link name="stereo_camera_right_link">\n  <visual>\n    <geometry>\n      <box size="0.02 0.04 0.02"/>\n    </geometry>\n  </visual>\n</link>\n\n<joint name="stereo_left_joint" type="fixed">\n  <parent link="head_link"/>\n  <child link="stereo_camera_left_link"/>\n  <origin xyz="0.05 0.06 0.0" rpy="0 0 0"/>\n</joint>\n\n<joint name="stereo_right_joint" type="fixed">\n  <parent link="head_link"/>\n  <child link="stereo_camera_right_link"/>\n  <origin xyz="0.05 -0.06 0.0" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Left camera --\x3e\n<gazebo reference="stereo_camera_left_link">\n  <sensor name="stereo_left_camera" type="camera">\n    <camera name="left_cam">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image><width>640</width><height>480</height></image>\n      <clip><near>0.1</near><far>10.0</far></clip>\n    </camera>\n    <plugin name="left_camera_controller" filename="libgazebo_ros_camera.so">\n      <frame_name>stereo_camera_left_optical_frame</frame_name>\n      <topic_name>stereo/left/image_raw</topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n\n\x3c!-- Right camera --\x3e\n<gazebo reference="stereo_camera_right_link">\n  <sensor name="stereo_right_camera" type="camera">\n    <camera name="right_cam">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image><width>640</width><height>480</height></image>\n      <clip><near>0.1</near><far>10.0</far></clip>\n    </camera>\n    <plugin name="right_camera_controller" filename="libgazebo_ros_camera.so">\n      <frame_name>stereo_camera_right_optical_frame</frame_name>\n      <topic_name>stereo/right/image_raw</topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"rgb-d-camera-simulation-in-unity",children:"RGB-D Camera Simulation in Unity"}),"\n",(0,a.jsx)(n.p,{children:"Unity provides high-quality camera simulation capabilities that can complement Gazebo's physics-based simulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing ROS2;\nusing System.Collections;\n\npublic class RGBCameraSimulator : MonoBehaviour\n{\n    [Header("Camera Configuration")]\n    [SerializeField] private Camera rgbCamera;\n    [SerializeField] private int imageWidth = 640;\n    [SerializeField] private int imageHeight = 480;\n    [SerializeField] private float maxDepth = 10.0f;\n\n    [Header("ROS Topics")]\n    [SerializeField] private string rgbTopic = "/camera/rgb/image_raw";\n    [SerializeField] private string depthTopic = "/camera/depth/image_raw";\n\n    private ROSConnection ros;\n    private RenderTexture rgbTexture;\n    private RenderTexture depthTexture;\n    private Texture2D rgbTexture2D;\n    private Texture2D depthTexture2D;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        SetupCameras();\n        StartCoroutine(CaptureImages());\n    }\n\n    void SetupCameras()\n    {\n        // Create render textures for RGB and depth\n        rgbTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        depthTexture = new RenderTexture(imageWidth, imageHeight, 24);\n\n        rgbCamera.targetTexture = rgbTexture;\n        rgbCamera.depth = -1; // Ensure it renders after other cameras\n\n        // Set up depth camera (using Unity\'s depth buffer)\n        GameObject depthCameraGO = new GameObject("Depth Camera");\n        depthCameraGO.transform.SetParent(transform);\n        depthCameraGO.transform.localPosition = Vector3.zero;\n        depthCameraGO.transform.localRotation = Quaternion.identity;\n\n        Camera depthCamera = depthCameraGO.AddComponent<Camera>();\n        depthCamera.CopyFrom(rgbCamera);\n        depthCamera.targetTexture = depthTexture;\n        depthCamera.depth = -1;\n        depthCamera.clearFlags = CameraClearFlags.SolidColor;\n        depthCamera.backgroundColor = Color.white; // Far depth = white = max distance\n    }\n\n    IEnumerator CaptureImages()\n    {\n        while (true)\n        {\n            yield return new WaitForEndOfFrame();\n\n            // Capture RGB image\n            RenderTexture.active = rgbTexture;\n            if (rgbTexture2D == null)\n                rgbTexture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n            rgbTexture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n            rgbTexture2D.Apply();\n\n            // Capture depth image\n            RenderTexture.active = depthTexture;\n            if (depthTexture2D == null)\n                depthTexture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n            depthTexture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n            depthTexture2D.Apply();\n\n            // Convert and publish to ROS\n            PublishImages();\n        }\n    }\n\n    void PublishImages()\n    {\n        // Convert Texture2D to ROS Image message format\n        byte[] rgbData = rgbTexture2D.EncodeToJPG();\n\n        // Create and publish RGB image message\n        var rgbMsg = new Sensor_msgs.Image();\n        rgbMsg.header = new Std_msgs.Header();\n        rgbMsg.header.frame_id = "camera_rgb_optical_frame";\n        rgbMsg.height = (uint)imageHeight;\n        rgbMsg.width = (uint)imageWidth;\n        rgbMsg.encoding = "rgb8";\n        rgbMsg.is_bigendian = 0;\n        rgbMsg.step = (uint)(imageWidth * 3); // 3 bytes per pixel\n        rgbMsg.data = rgbData;\n\n        ros.Publish(rgbTopic, rgbMsg);\n\n        // For depth, we would need to convert the depth buffer properly\n        // This is a simplified example\n        byte[] depthData = depthTexture2D.EncodeToEXR(); // Better for depth data\n\n        var depthMsg = new Sensor_msgs.Image();\n        depthMsg.header = new Std_msgs.Header();\n        depthMsg.header.frame_id = "camera_depth_optical_frame";\n        depthMsg.height = (uint)imageHeight;\n        depthMsg.width = (uint)imageWidth;\n        depthMsg.encoding = "32FC1"; // 32-bit float per pixel\n        depthMsg.is_bigendian = 0;\n        depthMsg.step = (uint)(imageWidth * 4); // 4 bytes per float\n        depthMsg.data = depthData;\n\n        ros.Publish(depthTopic, depthMsg);\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-fundamentals",children:"Sensor Fusion Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to produce more accurate, reliable, and comprehensive information than could be achieved by using a single sensor. For humanoid robots, effective sensor fusion is essential for robust perception and navigation."}),"\n",(0,a.jsx)(n.h3,{id:"kalman-filter-for-sensor-fusion",children:"Kalman Filter for Sensor Fusion"}),"\n",(0,a.jsx)(n.p,{children:"The Kalman filter is a widely used algorithm for sensor fusion, particularly for combining data from sensors with different characteristics:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, dt, n_states, n_obs):\n        self.dt = dt\n        self.n_states = n_states  # Number of state variables\n        self.n_obs = n_obs        # Number of observations\n\n        # State transition matrix (for constant velocity model)\n        self.F = np.eye(n_states)\n        for i in range(n_states//2):\n            self.F[i, i + n_states//2] = dt\n\n        # Observation matrix\n        self.H = np.zeros((n_obs, n_states))\n        for i in range(n_obs):\n            self.H[i, i] = 1.0\n\n        # Process noise covariance\n        self.Q = np.eye(n_states) * 0.1\n\n        # Observation noise covariance\n        self.R = np.eye(n_obs) * 1.0\n\n        # Error covariance matrix\n        self.P = np.eye(n_states)\n\n        # State vector\n        self.x = np.zeros(n_states)\n\n    def predict(self):\n        """Prediction step"""\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        return self.x\n\n    def update(self, z):\n        """Update step with observation z"""\n        # Innovation\n        y = z - self.H @ self.x\n\n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.x = self.x + K @ y\n        self.P = (np.eye(self.n_states) - K @ self.H) @ self.P\n\n        return self.x\n\n# Example usage for fusing IMU and camera data\nclass RobotPoseEstimator:\n    def __init__(self):\n        # State: [x, y, z, vx, vy, vz] - position and velocity\n        self.kf = KalmanFilter(dt=0.01, n_states=6, n_obs=3)  # 3D position\n\n    def update_with_camera(self, camera_pos):\n        """Update with camera-based position estimate"""\n        self.kf.update(np.array(camera_pos))\n\n    def update_with_imu(self, acceleration, dt):\n        """Update with IMU-based velocity estimate"""\n        # Integrate acceleration to get velocity change\n        vel_change = np.array(acceleration) * dt\n        current_vel = self.kf.x[3:6]  # Current velocity\n        new_vel = current_vel + vel_change\n\n        # Update state with new velocity\n        self.kf.x[3:6] = new_vel\n\n        # Predict next position\n        self.kf.x[:3] += new_vel * self.kf.dt\n\n        return self.kf.x[:3]  # Return position estimate\n'})}),"\n",(0,a.jsx)(n.h3,{id:"particle-filter-for-non-linear-systems",children:"Particle Filter for Non-linear Systems"}),"\n",(0,a.jsx)(n.p,{children:"For non-linear systems, particle filters provide a robust alternative to Kalman filters:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport random\n\nclass ParticleFilter:\n    def __init__(self, n_particles, state_dim, process_noise, measurement_noise):\n        self.n_particles = n_particles\n        self.state_dim = state_dim\n        self.process_noise = process_noise\n        self.measurement_noise = measurement_noise\n\n        # Initialize particles randomly\n        self.particles = np.random.randn(n_particles, state_dim)\n        self.weights = np.ones(n_particles) / n_particles\n\n    def predict(self, control_input):\n        """Predict step: move particles according to motion model"""\n        # Add process noise to each particle\n        noise = np.random.normal(0, self.process_noise, self.particles.shape)\n        self.particles += noise\n\n        # Apply motion model (simplified)\n        self.particles[:, :3] += control_input[:3]  # Position update\n        self.particles[:, 3:] += control_input[3:]  # Velocity update\n\n    def update(self, measurement):\n        """Update step: weight particles based on measurement likelihood"""\n        # Calculate likelihood of each particle given measurement\n        for i in range(self.n_particles):\n            # Calculate distance between particle and measurement\n            particle_meas = self.particles[i, :len(measurement)]\n            diff = measurement - particle_meas\n\n            # Calculate likelihood (Gaussian)\n            likelihood = np.exp(-0.5 * np.sum(diff**2) / self.measurement_noise**2)\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        self.weights += 1e-300  # Avoid division by zero\n        self.weights /= np.sum(self.weights)\n\n    def resample(self):\n        """Resample particles based on weights"""\n        # Systematic resampling\n        indices = []\n        cumulative_sum = np.cumsum(self.weights)\n        u = np.random.uniform(0, 1/self.n_particles)\n\n        i = 0\n        for j in range(self.n_particles):\n            while cumulative_sum[i] < u:\n                i += 1\n            indices.append(i)\n            u += 1/self.n_particles\n\n        self.particles = self.particles[indices]\n        self.weights = np.ones(self.n_particles) / self.n_particles\n\n    def estimate(self):\n        """Get state estimate as weighted average of particles"""\n        return np.average(self.particles, axis=0, weights=self.weights)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"multi-sensor-fusion-architecture",children:"Multi-Sensor Fusion Architecture"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots, a typical sensor fusion architecture combines multiple sensor types:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[RGB-D Camera] --\x3e D[Fusion Algorithm]\n    B[LiDAR] --\x3e D\n    C[IMU] --\x3e D\n    E[Force/Torque Sensors] --\x3e D\n    F[Encoders] --\x3e D\n\n    D --\x3e G[Perception Output]\n    G --\x3e H[Localization]\n    G --\x3e I[Mapping]\n    G --\x3e J[Object Detection]\n\n    K[Control System] --\x3e L[Sensor Fusion Manager]\n    L --\x3e D\n    H --\x3e L\n    I --\x3e L\n    J --\x3e L\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Figure 1: Multi-sensor fusion architecture for humanoid robot perception."})}),"\n",(0,a.jsx)(n.h3,{id:"ros-based-sensor-fusion-node",children:"ROS-based Sensor Fusion Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, Imu, JointState\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Header\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Subscribers for different sensors\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.rgb_callback, 10)\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth/image_raw', self.depth_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.joint_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_callback, 10)\n\n        # Publisher for fused data\n        self.pose_pub = self.create_publisher(PoseStamped, '/robot_pose', 10)\n        self.fused_cloud_pub = self.create_publisher(PointCloud2, '/fused_pointcloud', 10)\n\n        # Internal state\n        self.latest_rgb = None\n        self.latest_depth = None\n        self.latest_imu = None\n        self.latest_joints = None\n\n        # Timer for fusion processing\n        self.timer = self.create_timer(0.033, self.fusion_callback)  # 30Hz\n\n    def rgb_callback(self, msg):\n        self.latest_rgb = msg\n        self.process_sensor_data('rgb', msg)\n\n    def depth_callback(self, msg):\n        self.latest_depth = msg\n        self.process_sensor_data('depth', msg)\n\n    def imu_callback(self, msg):\n        self.latest_imu = msg\n        self.process_sensor_data('imu', msg)\n\n    def joint_callback(self, msg):\n        self.latest_joints = msg\n        self.process_sensor_data('joints', msg)\n\n    def process_sensor_data(self, sensor_type, msg):\n        \"\"\"Process incoming sensor data\"\"\"\n        # Convert sensor data to internal representation\n        if sensor_type == 'imu':\n            # Extract orientation and angular velocity\n            orientation = np.array([\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z,\n                msg.orientation.w\n            ])\n\n            angular_vel = np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ])\n\n            # Store processed data\n            self.imu_data = {\n                'orientation': orientation,\n                'angular_vel': angular_vel,\n                'linear_acc': np.array([\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ])\n            }\n\n    def fusion_callback(self):\n        \"\"\"Main fusion processing callback\"\"\"\n        if all(data is not None for data in\n               [self.latest_rgb, self.latest_depth, self.latest_imu, self.latest_joints]):\n\n            # Perform sensor fusion\n            fused_pose = self.perform_fusion()\n\n            # Publish results\n            pose_msg = PoseStamped()\n            pose_msg.header = Header()\n            pose_msg.header.stamp = self.get_clock().now().to_msg()\n            pose_msg.header.frame_id = 'map'\n            pose_msg.pose.position.x = fused_pose[0]\n            pose_msg.pose.position.y = fused_pose[1]\n            pose_msg.pose.position.z = fused_pose[2]\n\n            self.pose_pub.publish(pose_msg)\n\n    def perform_fusion(self):\n        \"\"\"Perform the actual sensor fusion\"\"\"\n        # This is where the fusion algorithm would be implemented\n        # For example, using a Kalman filter, particle filter, or other method\n\n        # Placeholder: simple weighted average of available data\n        pose_estimate = np.zeros(3)\n\n        # Process IMU data for orientation\n        if hasattr(self, 'imu_data'):\n            # Integrate IMU data for position estimate\n            # This is a simplified example\n            pass\n\n        # Process camera data for visual odometry\n        # Process LiDAR data for localization\n        # Combine all estimates with appropriate weights\n\n        return pose_estimate\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"perception-pipeline-integration",children:"Perception Pipeline Integration"}),"\n",(0,a.jsx)(n.p,{children:"A complete perception pipeline integrates sensor fusion with higher-level perception tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom scipy.spatial import KDTree\nimport open3d as o3d\n\nclass PerceptionPipeline:\n    def __init__(self):\n        self.camera_intrinsics = None\n        self.fusion_estimator = None\n        self.object_detector = None\n        self.scene_segmenter = None\n\n    def process_rgbd_data(self, rgb_image, depth_image):\n        \"\"\"Process RGB-D data through the perception pipeline\"\"\"\n        # Convert depth image to point cloud\n        point_cloud = self.depth_to_pointcloud(depth_image, self.camera_intrinsics)\n\n        # Apply sensor fusion for pose estimation\n        robot_pose = self.fusion_estimator.estimate_pose()\n\n        # Detect objects in the scene\n        objects = self.detect_objects(rgb_image, point_cloud)\n\n        # Segment the scene into navigable and non-navigable areas\n        navigable_areas = self.segment_scene(point_cloud, robot_pose)\n\n        return {\n            'objects': objects,\n            'navigable_areas': navigable_areas,\n            'robot_pose': robot_pose,\n            'point_cloud': point_cloud\n        }\n\n    def depth_to_pointcloud(self, depth_image, intrinsics):\n        \"\"\"Convert depth image to 3D point cloud\"\"\"\n        height, width = depth_image.shape\n        cx, cy = intrinsics['cx'], intrinsics['cy']\n        fx, fy = intrinsics['fx'], intrinsics['fy']\n\n        # Generate coordinate grids\n        x_grid, y_grid = np.meshgrid(np.arange(width), np.arange(height))\n\n        # Convert to 3D coordinates\n        x_3d = (x_grid - cx) * depth_image / fx\n        y_3d = (y_grid - cy) * depth_image / fy\n        z_3d = depth_image\n\n        # Stack to create point cloud\n        points = np.stack([x_3d, y_3d, z_3d], axis=-1)\n\n        # Remove invalid points (where depth is 0 or invalid)\n        valid_mask = (z_3d > 0) & (z_3d < 10.0)  # Valid depth range\n        valid_points = points[valid_mask]\n\n        return valid_points\n\n    def detect_objects(self, rgb_image, point_cloud):\n        \"\"\"Detect and classify objects in the scene\"\"\"\n        # Apply object detection to RGB image\n        detections = self.object_detector.detect(rgb_image)\n\n        # Associate 3D points with detected objects\n        objects = []\n        for detection in detections:\n            # Get 3D bounding box from point cloud\n            bbox_3d = self.get_3d_bbox(detection, point_cloud)\n\n            objects.append({\n                'class': detection['class'],\n                'confidence': detection['confidence'],\n                'bbox_2d': detection['bbox'],\n                'bbox_3d': bbox_3d,\n                'centroid': np.mean(bbox_3d, axis=0)\n            })\n\n        return objects\n\n    def segment_scene(self, point_cloud, robot_pose):\n        \"\"\"Segment scene into navigable and non-navigable areas\"\"\"\n        # Create KDTree for efficient nearest neighbor search\n        tree = KDTree(point_cloud)\n\n        # Define navigable area around robot\n        robot_pos = robot_pose[:3]  # Assuming pose is [x, y, z, ...]\n\n        # Find ground plane using RANSAC\n        ground_plane = self.estimate_ground_plane(point_cloud)\n\n        # Segment point cloud into ground, obstacles, and free space\n        ground_points = self.filter_ground_points(point_cloud, ground_plane)\n        obstacle_points = self.filter_obstacle_points(point_cloud, ground_plane)\n\n        return {\n            'ground': ground_points,\n            'obstacles': obstacle_points,\n            'free_space': self.estimate_free_space(obstacle_points)\n        }\n\n    def estimate_ground_plane(self, point_cloud, distance_threshold=0.05):\n        \"\"\"Estimate ground plane using RANSAC\"\"\"\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(point_cloud)\n\n        plane_model, inliers = pcd.segment_plane(\n            distance_threshold=distance_threshold,\n            ransac_n=3,\n            num_iterations=1000\n        )\n\n        return plane_model\n"})}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercise-23-implement-a-simple-sensor-fusion-system",children:"Hands-on Exercise 2.3: Implement a Simple Sensor Fusion System"}),"\n",(0,a.jsx)(n.p,{children:"Create a ROS 2 package that implements a basic sensor fusion system:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create a new package:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python sensor_fusion_tutorial\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsxs)(n.li,{children:["Create the sensor fusion node ",(0,a.jsx)(n.code,{children:"sensor_fusion_tutorial/sensor_fusion_tutorial/fusion_node.py"}),":"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, Range, JointState\nfrom geometry_msgs.msg import Vector3Stamped, PoseWithCovarianceStamped\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\nclass SimpleSensorFusion(Node):\n    def __init__(self):\n        super().__init__('simple_sensor_fusion')\n\n        # Initialize Kalman filter for position estimation\n        # State: [x, vx, y, vy, z, vz] - position and velocity in 3D\n        self.kf = KalmanFilter(dim_x=6, dim_z=3)\n\n        # State transition matrix (constant velocity model)\n        dt = 0.05  # 20Hz update rate\n        self.kf.F = np.array([\n            [1, dt, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0],\n            [0, 0, 1, dt, 0, 0],\n            [0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 1, dt],\n            [0, 0, 0, 0, 0, 1]\n        ])\n\n        # Measurement function (we can only measure position)\n        self.kf.H = np.array([\n            [1, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0],\n            [0, 0, 0, 0, 1, 0]\n        ])\n\n        # Covariance matrices\n        self.kf.P *= 100  # Initial uncertainty\n        self.kf.R = np.eye(3) * 0.1  # Measurement noise\n        self.kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.1, block_size=3)\n\n        # Subscriptions\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.range_sub = self.create_subscription(\n            Range, '/range_finder', self.range_callback, 10)\n        self.joint_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_callback, 10)\n\n        # Publisher for fused estimate\n        self.pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped, '/fused_pose', 10)\n\n        # Timer for fusion update\n        self.timer = self.create_timer(0.05, self.fusion_update)\n\n        # Sensor data storage\n        self.latest_imu = None\n        self.latest_range = None\n        self.latest_joints = None\n\n        self.get_logger().info('Simple Sensor Fusion Node Started')\n\n    def imu_callback(self, msg):\n        self.latest_imu = msg\n\n    def range_callback(self, msg):\n        self.latest_range = msg\n\n    def joint_callback(self, msg):\n        self.latest_joints = msg\n\n    def fusion_update(self):\n        \"\"\"Main fusion update loop\"\"\"\n        if self.latest_imu is not None:\n            # Extract linear acceleration from IMU\n            acc = np.array([\n                self.latest_imu.linear_acceleration.x,\n                self.latest_imu.linear_acceleration.y,\n                self.latest_imu.linear_acceleration.z\n            ])\n\n            # Predict state using IMU data\n            self.kf.predict()\n\n            # If we have range data, use it as measurement\n            if self.latest_range is not None:\n                # Convert range to z-position (simplified)\n                z_pos = self.latest_range.range\n                measurement = np.array([0, 0, z_pos])  # Only using z from range\n\n                # Update filter with measurement\n                self.kf.update(measurement)\n\n        # Publish current estimate\n        self.publish_estimate()\n\n    def publish_estimate(self):\n        \"\"\"Publish the current state estimate\"\"\"\n        msg = PoseWithCovarianceStamped()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = 'map'\n\n        # Fill pose with current estimate\n        state = self.kf.x\n        msg.pose.pose.position.x = float(state[0])\n        msg.pose.pose.position.y = float(state[2])\n        msg.pose.pose.position.z = float(state[4])\n\n        # Fill covariance matrix\n        cov_matrix = self.kf.P.flatten().tolist()\n        msg.pose.covariance = cov_matrix\n\n        self.pose_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SimpleSensorFusion()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsx)(n.li,{children:"Test the fusion system with simulated sensors:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start the fusion node\nros2 run sensor_fusion_tutorial fusion_node\n\n# Terminal 2: Publish simulated IMU data\nros2 topic pub /imu/data sensor_msgs/msg/Imu '{linear_acceleration: {x: 0.1, y: 0.0, z: 9.8}, angular_velocity: {x: 0.0, y: 0.0, z: 0.0}, orientation: {w: 1.0}}' -r 20\n\n# Terminal 3: Publish simulated range data\nros2 topic pub /range_finder sensor_msgs/msg/Range '{radiation_type: 0, field_of_view: 0.1, min_range: 0.0, max_range: 10.0, range: 1.5}' -r 10\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"RGB-D cameras"})," provide both color and depth information essential for 3D scene understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor fusion algorithms"})," (Kalman filters, particle filters) combine multiple sensor inputs for robust perception"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-sensor architectures"})," integrate data from cameras, LiDAR, IMU, and other sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception pipelines"})," process fused sensor data for higher-level tasks like object detection and scene segmentation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time performance"})," requires careful optimization of fusion algorithms and data processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How does the accuracy of sensor fusion compare to individual sensors in different environmental conditions?"}),"\n",(0,a.jsx)(n.li,{children:"What are the computational requirements for real-time sensor fusion on humanoid robot platforms?"}),"\n",(0,a.jsx)(n.li,{children:"How can sensor fusion algorithms be made robust to sensor failures or outliers?"}),"\n",(0,a.jsx)(n.li,{children:"What role does sensor calibration play in the effectiveness of fusion systems?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,a.jsxs)(n.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,a.jsx)(n.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,a.jsxs)(n.p,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,a.jsx)(n.em,{children:"Probabilistic robotics"}),". MIT Press."]}),"\n",(0,a.jsxs)(n.p,{children:["Murphy, K. P. (2012). ",(0,a.jsx)(n.em,{children:"Machine learning: A probabilistic perspective"}),". MIT Press."]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This lesson covered the simulation of RGB-D cameras and the implementation of sensor fusion techniques for humanoid robots. We explored camera simulation in both Gazebo and Unity, learned about Kalman and particle filters for sensor fusion, and implemented a complete perception pipeline. Sensor fusion is essential for humanoid robots to achieve robust perception in complex environments by combining multiple sensor modalities effectively."}),"\n",(0,a.jsx)(n.p,{children:"In the next module, we'll explore NVIDIA Isaac Sim for photorealistic simulation and Isaac ROS for hardware-accelerated perception, building on the digital twin concepts we've learned in this module."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var i=s(6540);const a={},t=i.createContext(a);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);