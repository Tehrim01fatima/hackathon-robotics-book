"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[118],{3502:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3/lesson-2-isaac-ros-perception-vslam","title":"Lesson 2 - Isaac ROS & Perception VSLAM - Hardware-Accelerated Perception, Visual SLAM","description":"Learning Objectives","source":"@site/docs/module-3/lesson-2-isaac-ros-perception-vslam.md","sourceDirName":"module-3","slug":"/module-3/lesson-2-isaac-ros-perception-vslam","permalink":"/hackathon-robotics-book/docs/module-3/lesson-2-isaac-ros-perception-vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-3/lesson-2-isaac-ros-perception-vslam.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 2 - Isaac ROS & Perception VSLAM - Hardware-Accelerated Perception, Visual SLAM","sidebar_label":"Isaac ROS & Perception VSLAM"},"sidebar":"textbookSidebar","previous":{"title":"Isaac Sim Basics","permalink":"/hackathon-robotics-book/docs/module-3/lesson-1-isaac-sim-basics"},"next":{"title":"Nav2 Path Planning for Humanoids","permalink":"/hackathon-robotics-book/docs/module-3/lesson-3-nav2-path-planning-humanoids"}}');var i=a(4848),o=a(8453);const r={title:"Lesson 2 - Isaac ROS & Perception VSLAM - Hardware-Accelerated Perception, Visual SLAM",sidebar_label:"Isaac ROS & Perception VSLAM"},t="Lesson 2: Isaac ROS & Perception VSLAM - Hardware-Accelerated Perception, Visual SLAM",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Isaac ROS Architecture and Components",id:"isaac-ros-architecture-and-components",level:2},{value:"Core Isaac ROS Packages",id:"core-isaac-ros-packages",level:3},{value:"Hardware Acceleration Framework",id:"hardware-acceleration-framework",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installing Isaac ROS",id:"installing-isaac-ros",level:3},{value:"Docker-based Installation (Recommended)",id:"docker-based-installation-recommended",level:3},{value:"Isaac ROS Perception Pipelines",id:"isaac-ros-perception-pipelines",level:2},{value:"Image Processing Pipeline",id:"image-processing-pipeline",level:3},{value:"Object Detection with Isaac ROS",id:"object-detection-with-isaac-ros",level:3},{value:"Visual SLAM Implementation with Isaac ROS",id:"visual-slam-implementation-with-isaac-ros",level:2},{value:"Isaac ROS VSLAM Node",id:"isaac-ros-vslam-node",level:3},{value:"Isaac ROS Integration with Nav2",id:"isaac-ros-integration-with-nav2",level:2},{value:"Performance Optimization with Isaac ROS",id:"performance-optimization-with-isaac-ros",level:2},{value:"GPU Memory Management",id:"gpu-memory-management",level:3},{value:"Pipeline Optimization",id:"pipeline-optimization",level:3},{value:"Isaac ROS Launch Files",id:"isaac-ros-launch-files",level:2},{value:"Hands-on Exercise 3.2: Deploy Isaac ROS Perception Pipeline",id:"hands-on-exercise-32-deploy-isaac-ros-perception-pipeline",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-2-isaac-ros--perception-vslam---hardware-accelerated-perception-visual-slam",children:"Lesson 2: Isaac ROS & Perception VSLAM - Hardware-Accelerated Perception, Visual SLAM"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Install and configure Isaac ROS packages for hardware-accelerated perception"}),"\n",(0,i.jsx)(n.li,{children:"Implement Visual SLAM (VSLAM) systems for humanoid robot navigation"}),"\n",(0,i.jsx)(n.li,{children:"Deploy perception pipelines using Isaac ROS for real-time object detection and tracking"}),"\n",(0,i.jsx)(n.li,{children:"Integrate Isaac ROS with existing ROS 2 navigation systems"}),"\n",(0,i.jsx)(n.li,{children:"Optimize perception performance using NVIDIA GPU acceleration"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS represents NVIDIA's collection of hardware-accelerated perception and navigation packages that run on ROS 2. These packages leverage NVIDIA GPUs to provide real-time processing of sensor data, including image processing, point cloud operations, and AI inference. For humanoid robots, Isaac ROS enables sophisticated perception capabilities that are essential for autonomous operation in complex environments."}),"\n",(0,i.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) is a critical technology that allows robots to build maps of unknown environments while simultaneously tracking their location within those maps. Combined with Isaac ROS's hardware acceleration, VSLAM systems can operate in real-time on humanoid robots, enabling autonomous navigation and interaction with the environment."}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-architecture-and-components",children:"Isaac ROS Architecture and Components"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS is designed as a collection of modular, hardware-accelerated packages that integrate seamlessly with the ROS 2 ecosystem. Each package is optimized to leverage NVIDIA GPU capabilities for specific perception tasks."}),"\n",(0,i.jsx)(n.h3,{id:"core-isaac-ros-packages",children:"Core Isaac ROS Packages"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Isaac ROS] --\x3e B[Image Pipelines]\n    A --\x3e C[Point Cloud Processing]\n    A --\x3e D[AI Inference]\n    A --\x3e E[VSLAM Systems]\n    A --\x3e F[Sensor Processing]\n\n    B --\x3e G[Image Rectification]\n    B --\x3e H[Image Undistortion]\n    B --\x3e I[Image Preprocessing]\n\n    C --\x3e J[Point Cloud Filtering]\n    C --\x3e K[Point Cloud Registration]\n    C --\x3e L[Surface Reconstruction]\n\n    D --\x3e M[Object Detection]\n    D --\x3e N[Semantic Segmentation]\n    D --\x3e O[Depth Estimation]\n\n    E --\x3e P[Visual Odometry]\n    E --\x3e Q[Loop Closure]\n    E --\x3e R[Map Building]\n\n    F --\x3e S[Camera Drivers]\n    F --\x3e T[LiDAR Processing]\n    F --\x3e U[IMU Integration]\n\n    G --\x3e V[ROS 2 Interface]\n    M --\x3e V\n    P --\x3e V\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Figure 1: Isaac ROS architecture showing the modular packages and their integration with ROS 2."})}),"\n",(0,i.jsx)(n.h3,{id:"hardware-acceleration-framework",children:"Hardware Acceleration Framework"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS packages utilize NVIDIA's hardware acceleration technologies:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CUDA"}),": For parallel processing on GPU"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TensorRT"}),": For optimized AI model inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenCV GPU"}),": For accelerated computer vision operations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenGL/Vulkan"}),": For graphics-accelerated operations"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before installing Isaac ROS, ensure your system meets the requirements:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check NVIDIA GPU and driver\nnvidia-smi\n\n# Verify CUDA installation\nnvcc --version\n\n# Check ROS 2 Humble installation\nros2 --version\n"})}),"\n",(0,i.jsx)(n.h3,{id:"installing-isaac-ros",children:"Installing Isaac ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Add NVIDIA package repository\ncurl -sL https://nvidia.github.io/nvidia-ros2-package-repos.deb | sudo apt-key add -\nsudo add-apt-repository "deb https://nvidia.github.io/nvidia-ros2/$(lsb_release -cs)/$(dpkg --print-architecture) ."\nsudo apt-get update\n\n# Install Isaac ROS packages\nsudo apt-get install ros-humble-isaac-ros-perception\nsudo apt-get install ros-humble-isaac-ros-nav2\nsudo apt-get install ros-humble-isaac-ros-visual-slam\nsudo apt-get install ros-humble-isaac-ros-message-bridge\n'})}),"\n",(0,i.jsx)(n.h3,{id:"docker-based-installation-recommended",children:"Docker-based Installation (Recommended)"}),"\n",(0,i.jsx)(n.p,{children:"For easier setup and consistency, use Isaac ROS Docker containers:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Pull Isaac ROS Docker image\ndocker pull nvcr.io/nvidia/isaac-ros:latest\n\n# Run Isaac ROS container with GPU support\ndocker run --gpus all -it --rm \\\n  --network host \\\n  --env="DISPLAY" \\\n  --volume="/tmp/.X11-unix:/tmp/.X11-unix:rw" \\\n  --name isaac_ros_env \\\n  nvcr.io/nvidia/isaac-ros:latest\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-perception-pipelines",children:"Isaac ROS Perception Pipelines"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS provides optimized perception pipelines that leverage GPU acceleration for real-time processing."}),"\n",(0,i.jsx)(n.h3,{id:"image-processing-pipeline",children:"Image Processing Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacImageProcessor(Node):\n    def __init__(self):\n        super().__init__('isaac_image_processor')\n\n        # Create subscription to camera feed\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for processed image\n        self.publisher = self.create_publisher(\n            Image,\n            '/camera/image_processed',\n            10\n        )\n\n        self.bridge = CvBridge()\n\n        # Initialize CUDA for GPU acceleration\n        if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n            self.cuda_enabled = True\n            self.get_logger().info('CUDA acceleration enabled')\n        else:\n            self.cuda_enabled = False\n            self.get_logger().warn('CUDA acceleration not available')\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Process image using GPU acceleration if available\n        if self.cuda_enabled:\n            processed_image = self.gpu_process_image(cv_image)\n        else:\n            processed_image = self.cpu_process_image(cv_image)\n\n        # Convert back to ROS Image and publish\n        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')\n        processed_msg.header = msg.header\n        self.publisher.publish(processed_msg)\n\n    def gpu_process_image(self, image):\n        \"\"\"Process image using GPU acceleration\"\"\"\n        # Upload image to GPU\n        gpu_image = cv2.cuda_GpuMat()\n        gpu_image.upload(image)\n\n        # Apply Gaussian blur on GPU\n        gpu_blur = cv2.cuda.GaussianBlur(gpu_image, (0, 0), sigmaX=1.0, sigmaY=1.0)\n\n        # Convert to grayscale on GPU\n        gpu_gray = cv2.cuda.cvtColor(gpu_blur, cv2.COLOR_BGR2GRAY)\n\n        # Download result from GPU\n        result = gpu_gray.download()\n\n        return result\n\n    def cpu_process_image(self, image):\n        \"\"\"Process image using CPU (fallback)\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 1.0)\n        return blurred\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = IsaacImageProcessor()\n    rclpy.spin(processor)\n    processor.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-with-isaac-ros",children:"Object Detection with Isaac ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image as PILImage\n\nclass IsaacObjectDetector(Node):\n    def __init__(self):\n        super().__init__('isaac_object_detector')\n\n        # Load pre-trained model (YOLOv5, Detectron2, etc.)\n        self.model = self.load_model()\n\n        # Ensure model runs on GPU if available\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Subscriptions and publishers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/object_detections', 10)\n\n        self.bridge = CvBridge()\n\n    def load_model(self):\n        \"\"\"Load pre-trained object detection model\"\"\"\n        # Example with YOLOv5\n        import yolov5  # Assuming yolov5 package is available\n        model = yolov5.load('yolov5s.pt')  # Load model\n        return model\n\n    def image_callback(self, msg):\n        # Convert ROS Image to PIL Image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        pil_image = PILImage.fromarray(cv_image)\n\n        # Preprocess image\n        transform = transforms.Compose([\n            transforms.Resize((640, 640)),\n            transforms.ToTensor()\n        ])\n\n        input_tensor = transform(pil_image).unsqueeze(0).to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            results = self.model(input_tensor)\n\n        # Process results\n        detections = self.process_detections(results, msg.header)\n\n        # Publish detections\n        self.detection_pub.publish(detections)\n\n    def process_detections(self, results, header):\n        \"\"\"Process detection results and create Detection2DArray message\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        # Assuming results format from YOLOv5\n        for *box, conf, cls in results.xyxy[0].tolist():\n            detection = Detection2D()\n            detection.header = header\n\n            # Set bounding box\n            bbox = BoundingBox2D()\n            bbox.center.x = (box[0] + box[2]) / 2.0\n            bbox.center.y = (box[1] + box[3]) / 2.0\n            bbox.size_x = abs(box[2] - box[0])\n            bbox.size_y = abs(box[3] - box[1])\n            detection.bbox = bbox\n\n            # Set hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(int(cls))\n            hypothesis.hypothesis.score = conf\n            detection.results.append(hypothesis)\n\n            detections_msg.detections.append(detection)\n\n        return detections_msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    detector = IsaacObjectDetector()\n    rclpy.spin(detector)\n    detector.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"visual-slam-implementation-with-isaac-ros",children:"Visual SLAM Implementation with Isaac ROS"}),"\n",(0,i.jsx)(n.p,{children:"Visual SLAM is critical for humanoid robots to navigate unknown environments. Isaac ROS provides optimized VSLAM packages that leverage GPU acceleration."}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-vslam-node",children:"Isaac ROS VSLAM Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass IsaacVSLAMNode(Node):\n    def __init__(self):\n        super().__init__('isaac_vslam')\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # SLAM components\n        self.orb = cv2.cuda.SIFT_create() if cv2.cuda.getCudaEnabledDeviceCount() > 0 else cv2.SIFT_create()\n        self.bf = cv2.BFMatcher()\n        self.kp_descriptors = []\n        self.current_pose = np.eye(4)\n        self.map_points = []\n        self.frame_count = 0\n\n        # ROS interfaces\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.info_callback, 10)\n\n        self.pose_pub = self.create_publisher(PoseStamped, '/slam/pose', 10)\n        self.odom_pub = self.create_publisher(Odometry, '/slam/odometry', 10)\n        self.map_pub = self.create_publisher(MarkerArray, '/slam/map', 10)\n\n        self.bridge = CvBridge()\n\n    def info_callback(self, msg):\n        \"\"\"Receive camera calibration parameters\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images for SLAM\"\"\"\n        if self.camera_matrix is None:\n            return\n\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')\n\n        # Feature detection and matching\n        new_pose = self.process_frame(cv_image)\n\n        if new_pose is not None:\n            self.current_pose = new_pose\n            self.publish_pose_and_odom(msg.header)\n\n    def process_frame(self, image):\n        \"\"\"Process a single frame for visual SLAM\"\"\"\n        # Detect features\n        if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n            # Use GPU for feature detection if available\n            gpu_image = cv2.cuda_GpuMat()\n            gpu_image.upload(image)\n            keypoints_gpu, descriptors_gpu = self.orb.detectAndCompute(gpu_image, None)\n            keypoints = cv2.cuda.downloadKeypoints(keypoints_gpu)\n            descriptors = descriptors_gpu.download() if descriptors_gpu is not None else None\n        else:\n            # Fallback to CPU\n            keypoints, descriptors = self.orb.detectAndCompute(image, None)\n\n        if descriptors is None or len(keypoints) < 10:\n            return self.current_pose\n\n        # Store current frame data\n        current_frame = {\n            'keypoints': keypoints,\n            'descriptors': descriptors,\n            'image': image\n        }\n\n        # Match with previous frames\n        if len(self.kp_descriptors) > 0:\n            prev_frame = self.kp_descriptors[-1]\n            matches = self.bf.match(prev_frame['descriptors'], descriptors)\n\n            # Sort matches by distance\n            matches = sorted(matches, key=lambda x: x.distance)\n\n            # Extract matched points\n            if len(matches) >= 10:\n                src_pts = np.float32([prev_frame['keypoints'][m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n                dst_pts = np.float32([keypoints[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n                # Estimate motion using Essential matrix\n                E, mask = cv2.findEssentialMat(dst_pts, src_pts, self.camera_matrix,\n                                              method=cv2.RANSAC, threshold=1.0)\n\n                if E is not None:\n                    # Recover pose\n                    _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)\n\n                    # Create transformation matrix\n                    T = np.eye(4)\n                    T[:3, :3] = R\n                    T[:3, 3] = t.flatten()\n\n                    # Update current pose\n                    new_pose = self.current_pose @ T\n\n                    # Store frame for next iteration\n                    self.kp_descriptors.append(current_frame)\n                    if len(self.kp_descriptors) > 10:  # Keep only recent frames\n                        self.kp_descriptors.pop(0)\n\n                    return new_pose\n\n        # Store frame even if no pose update\n        self.kp_descriptors.append(current_frame)\n        if len(self.kp_descriptors) > 10:\n            self.kp_descriptors.pop(0)\n\n        return self.current_pose\n\n    def publish_pose_and_odom(self, header):\n        \"\"\"Publish pose and odometry messages\"\"\"\n        # Create and publish PoseStamped\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = 'map'\n\n        # Extract position and orientation from transformation matrix\n        position = self.current_pose[:3, 3]\n        pose_msg.pose.position.x = position[0]\n        pose_msg.pose.position.y = position[1]\n        pose_msg.pose.position.z = position[2]\n\n        # Convert rotation matrix to quaternion\n        R = self.current_pose[:3, :3]\n        qw = np.sqrt(1 + R[0,0] + R[1,1] + R[2,2]) / 2\n        qx = (R[2,1] - R[1,2]) / (4 * qw)\n        qy = (R[0,2] - R[2,0]) / (4 * qw)\n        qz = (R[1,0] - R[0,1]) / (4 * qw)\n\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n        pose_msg.pose.orientation.w = qw\n\n        self.pose_pub.publish(pose_msg)\n\n        # Create and publish Odometry\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = 'map'\n        odom_msg.child_frame_id = 'base_link'\n        odom_msg.pose.pose = pose_msg.pose\n\n        self.odom_pub.publish(odom_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vslam_node = IsaacVSLAMNode()\n    rclpy.spin(vslam_node)\n    vslam_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-integration-with-nav2",children:"Isaac ROS Integration with Nav2"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS seamlessly integrates with Nav2 for advanced navigation capabilities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport tf2_ros\nimport numpy as np\n\nclass IsaacNav2Integrator(Node):\n    def __init__(self):\n        super().__init__(\'isaac_nav2_integrator\')\n\n        # Action client for Nav2 navigation\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # TF2 buffer for coordinate transforms\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # SLAM pose subscription\n        self.slam_pose_sub = self.create_subscription(\n            PoseStamped, \'/slam/pose\', self.slam_pose_callback, 10)\n\n        # Current robot pose\n        self.current_pose = None\n\n    def slam_pose_callback(self, msg):\n        """Update current robot pose from SLAM"""\n        self.current_pose = msg.pose\n\n    def navigate_to_pose(self, x, y, theta=0.0):\n        """Send navigation goal to Nav2"""\n        # Wait for action server\n        self.nav_client.wait_for_server()\n\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = float(x)\n        goal_msg.pose.pose.position.y = float(y)\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from math import sin, cos\n        s = sin(theta / 2.0)\n        c = cos(theta / 2.0)\n        goal_msg.pose.pose.orientation.x = 0.0\n        goal_msg.pose.pose.orientation.y = 0.0\n        goal_msg.pose.pose.orientation.z = s\n        goal_msg.pose.pose.orientation.w = c\n\n        # Send goal\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        """Handle navigation goal response"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Goal rejected\')\n            return\n\n        self.get_logger().info(\'Goal accepted\')\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f\'Navigation result: {result}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    integrator = IsaacNav2Integrator()\n\n    # Example: Navigate to a specific pose\n    integrator.navigate_to_pose(5.0, 5.0, 0.0)\n\n    rclpy.spin(integrator)\n    integrator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization-with-isaac-ros",children:"Performance Optimization with Isaac ROS"}),"\n",(0,i.jsx)(n.p,{children:"To maximize the benefits of Isaac ROS, proper optimization is essential:"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-memory-management",children:"GPU Memory Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport gc\n\nclass OptimizedIsaacNode(Node):\n    def __init__(self):\n        super().__init__(\'optimized_isaac_node\')\n\n        # Set GPU memory fraction if needed\n        if torch.cuda.is_available():\n            # Limit GPU memory usage to prevent OOM errors\n            torch.cuda.set_per_process_memory_fraction(0.8)  # Use 80% of GPU memory\n\n    def cleanup_gpu_memory(self):\n        """Clean up GPU memory"""\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-optimization",children:"Pipeline Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Optimized perception pipeline using Isaac ROS extensions\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\n\nclass OptimizedPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('optimized_perception_pipeline')\n\n        # Define QoS profiles for optimal performance\n        qos_profile = QoSProfile(\n            depth=1,\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST\n        )\n\n        # Create optimized subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.optimized_image_callback,\n            qos_profile\n        )\n\n    def optimized_image_callback(self, msg):\n        \"\"\"Optimized image processing callback\"\"\"\n        # Skip frames if processing is behind\n        if hasattr(self, '_processing') and self._processing:\n            return  # Skip this frame\n\n        self._processing = True\n        try:\n            # Process image\n            self.process_image(msg)\n        finally:\n            self._processing = False\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-launch-files",children:"Isaac ROS Launch Files"}),"\n",(0,i.jsx)(n.p,{children:"Create launch files to properly configure Isaac ROS components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- isaac_perception.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    # Isaac ROS Image Pipeline\n    image_pipeline = Node(\n        package='isaac_ros_image_pipeline',\n        executable='isaac_ros_image_pipeline',\n        name='image_pipeline',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'input_width': 640},\n            {'input_height': 480},\n            {'output_width': 640},\n            {'output_height': 480}\n        ],\n        remappings=[\n            ('image_raw', '/camera/image_raw'),\n            ('image_rect', '/camera/image_rect')\n        ]\n    )\n\n    # Isaac ROS VSLAM\n    vslam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='isaac_ros_visual_slam_node',\n        name='visual_slam',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'enable_occupancy_map': True},\n            {'occupancy_map_resolution': 0.05},\n            {'occupancy_map_size': 100}\n        ],\n        remappings=[\n            ('/visual_slam/image', '/camera/image_rect'),\n            ('/visual_slam/camera_info', '/camera/camera_info')\n        ]\n    )\n\n    # Isaac ROS Object Detection\n    detection_node = Node(\n        package='isaac_ros_detectnet',\n        executable='isaac_ros_detectnet',\n        name='object_detection',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'model_name': 'detectnet_coco'},\n            {'confidence_threshold': 0.5}\n        ],\n        remappings=[\n            ('image', '/camera/image_rect'),\n            ('detections', '/object_detections')\n        ]\n    )\n\n    return LaunchDescription([\n        image_pipeline,\n        vslam_node,\n        detection_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercise-32-deploy-isaac-ros-perception-pipeline",children:"Hands-on Exercise 3.2: Deploy Isaac ROS Perception Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete Isaac ROS perception system with VSLAM:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Install Isaac ROS packages"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS perception packages\nsudo apt install ros-humble-isaac-ros-perception\nsudo apt install ros-humble-isaac-ros-visual-slam\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a perception pipeline launch file"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# perception_pipeline.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Image processing node\n        Node(\n            package='isaac_ros_image_proc',\n            executable='isaac_ros_image_proc',\n            name='image_proc',\n            parameters=[{'use_sim_time': True}],\n            remappings=[('image_raw', '/camera/image_raw')]\n        ),\n\n        # VSLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam',\n            parameters=[{'use_sim_time': True}],\n            remappings=[\n                ('/visual_slam/image', '/camera/image_rect'),\n                ('/visual_slam/camera_info', '/camera/camera_info')\n            ]\n        )\n    ])\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Launch the pipeline"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch perception pipeline\nros2 launch perception_pipeline.launch.py\n\n# Terminal 2: View results\nrviz2\n\n# Terminal 3: Publish test images (if not using real camera)\nros2 topic pub /camera/image_raw sensor_msgs/msg/Image --field header.frame_id=camera\n"})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS"})," provides hardware-accelerated perception packages optimized for NVIDIA GPUs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual SLAM"})," enables real-time mapping and localization for humanoid robot navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU acceleration"})," significantly improves processing performance for perception tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nav2 integration"})," allows seamless combination of perception and navigation systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance optimization"})," is crucial for real-time operation on humanoid robots"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"How does GPU acceleration in Isaac ROS compare to CPU-based perception systems?"}),"\n",(0,i.jsx)(n.li,{children:"What are the advantages of using VSLAM over traditional wheel odometry for humanoid robots?"}),"\n",(0,i.jsx)(n.li,{children:"How can Isaac ROS perception pipelines be optimized for different computational constraints?"}),"\n",(0,i.jsx)(n.li,{children:"What challenges arise when integrating VSLAM with humanoid robot locomotion systems?"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,i.jsxs)(n.p,{children:["NVIDIA Corporation. (2023). ",(0,i.jsx)(n.em,{children:"Isaac ROS Documentation"}),". Retrieved from ",(0,i.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/released/",children:"https://nvidia-isaac-ros.github.io/released/"})]}),"\n",(0,i.jsxs)(n.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,i.jsx)(n.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,i.jsxs)(n.p,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,i.jsx)(n.em,{children:"Probabilistic robotics"}),". MIT Press."]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This lesson covered Isaac ROS and Visual SLAM implementation for humanoid robots. We explored Isaac ROS architecture, installation, perception pipelines, and VSLAM systems that leverage GPU acceleration for real-time processing. The integration of Isaac ROS with Nav2 enables sophisticated perception and navigation capabilities essential for autonomous humanoid robot operation."}),"\n",(0,i.jsx)(n.p,{children:"In the next lesson, we'll explore Nav2 path planning specifically designed for humanoid robots, building on the perception and SLAM capabilities we've learned about."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>t});var s=a(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);