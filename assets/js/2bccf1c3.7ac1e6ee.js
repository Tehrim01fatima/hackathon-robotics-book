"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[464],{1151:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/lesson-1-whisper-speech-to-command","title":"Lesson 1 - Whisper Speech-to-Command - Voice Processing, Natural Language Understanding","description":"Learning Objectives","source":"@site/docs/module-4/lesson-1-whisper-speech-to-command.md","sourceDirName":"module-4","slug":"/module-4/lesson-1-whisper-speech-to-command","permalink":"/hackathon-robotics-book/docs/module-4/lesson-1-whisper-speech-to-command","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-4/lesson-1-whisper-speech-to-command.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 1 - Whisper Speech-to-Command - Voice Processing, Natural Language Understanding","sidebar_label":"Whisper Speech-to-Command"},"sidebar":"textbookSidebar","previous":{"title":"Overview","permalink":"/hackathon-robotics-book/docs/module-4/"},"next":{"title":"LLM Cognitive Planning","permalink":"/hackathon-robotics-book/docs/module-4/lesson-2-llm-cognitive-planning"}}');var s=i(4848),a=i(8453);const r={title:"Lesson 1 - Whisper Speech-to-Command - Voice Processing, Natural Language Understanding",sidebar_label:"Whisper Speech-to-Command"},t="Lesson 1: Whisper Speech-to-Command - Voice Processing, Natural Language Understanding",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Whisper Architecture and Capabilities",id:"whisper-architecture-and-capabilities",level:2},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"Whisper for Robotics Applications",id:"whisper-for-robotics-applications",level:3},{value:"Whisper Installation and Setup",id:"whisper-installation-and-setup",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Docker-based Installation (Recommended)",id:"docker-based-installation-recommended",level:3},{value:"Real-time Audio Processing Pipeline",id:"real-time-audio-processing-pipeline",level:2},{value:"Audio Capture and Preprocessing",id:"audio-capture-and-preprocessing",level:3},{value:"Noise Reduction and Audio Enhancement",id:"noise-reduction-and-audio-enhancement",level:2},{value:"Audio Preprocessing Pipeline",id:"audio-preprocessing-pipeline",level:3},{value:"Voice Command Grammar and Recognition",id:"voice-command-grammar-and-recognition",level:2},{value:"Command Grammar Definition",id:"command-grammar-definition",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Caching and Batching",id:"caching-and-batching",level:3},{value:"Voice Command System Launch Configuration",id:"voice-command-system-launch-configuration",level:2},{value:"Hands-on Exercise 4.1: Implement a Whisper Voice Command System",id:"hands-on-exercise-41-implement-a-whisper-voice-command-system",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-1-whisper-speech-to-command---voice-processing-natural-language-understanding",children:"Lesson 1: Whisper Speech-to-Command - Voice Processing, Natural Language Understanding"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Install and configure Whisper for speech recognition in humanoid robot applications"}),"\n",(0,s.jsx)(n.li,{children:"Process real-time audio input for voice command recognition"}),"\n",(0,s.jsx)(n.li,{children:"Implement noise reduction and audio preprocessing for robot environments"}),"\n",(0,s.jsx)(n.li,{children:"Design voice command grammars and vocabularies for humanoid robot control"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper with ROS 2 for real-time voice command processing"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate and optimize speech recognition performance in noisy environments"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Whisper, developed by OpenAI, represents a breakthrough in automatic speech recognition (ASR) technology, offering robust performance across multiple languages and challenging acoustic conditions. For humanoid robots, Whisper enables natural human-robot interaction through voice commands, allowing users to control robots using natural language rather than specialized interfaces or physical controls."}),"\n",(0,s.jsx)(n.p,{children:"In the context of humanoid robots, voice processing systems must handle unique challenges: environmental noise from robot motors and actuators, varying acoustic conditions in different environments, and the need for real-time processing to maintain natural interaction flow. This lesson explores how to implement Whisper-based speech-to-command systems specifically designed for humanoid robot applications."}),"\n",(0,s.jsx)(n.h2,{id:"whisper-architecture-and-capabilities",children:"Whisper Architecture and Capabilities"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is built on a transformer-based architecture that combines an encoder for audio processing and a decoder for text generation. The model is trained on a large dataset of audio-text pairs, enabling it to handle various accents, background noise, and speaking styles."}),"\n",(0,s.jsx)(n.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Whisper Models] --\x3e B[Tiny - 39M params]\n    A --\x3e C[Base - 74M params]\n    A --\x3e D[Small - 244M params]\n    A --\x3e E[Medium - 769M params]\n    A --\x3e F[Large - 1550M params]\n\n    B --\x3e G[Fast, Low accuracy]\n    C --\x3e H[Good speed/accuracy]\n    D --\x3e I[Good speed/accuracy]\n    E --\x3e J[High accuracy, slower]\n    F --\x3e K[Highest accuracy, slowest]\n\n    G --\x3e L[Simple commands]\n    H --\x3e M[Basic control]\n    I --\x3e N[Complex instructions]\n    J --\x3e O[Detailed tasks]\n    K --\x3e P[Natural conversation]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Figure 1: Whisper model variants showing the trade-off between size, speed, and accuracy for different humanoid robot applications."})}),"\n",(0,s.jsx)(n.h3,{id:"whisper-for-robotics-applications",children:"Whisper for Robotics Applications"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots, the choice of Whisper model variant depends on:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational resources"}),": Larger models require more GPU memory and processing power"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency requirements"}),": Real-time interaction demands faster processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy needs"}),": Complex commands require higher accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vocabulary complexity"}),": Simple vs. complex command sets"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"whisper-installation-and-setup",children:"Whisper Installation and Setup"}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before installing Whisper for humanoid robot applications, ensure your system meets the requirements:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install Python dependencies\npip install openai-whisper\npip install torch torchvision torchaudio\npip install pyaudio  # For audio input\npip install sounddevice  # For audio processing\npip install numpy scipy\n"})}),"\n",(0,s.jsx)(n.h3,{id:"docker-based-installation-recommended",children:"Docker-based Installation (Recommended)"}),"\n",(0,s.jsx)(n.p,{children:"For consistent deployment across different systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile for Whisper-based voice processing\nFROM nvidia/cuda:11.8-devel-ubuntu22.04\n\nRUN apt-get update && apt-get install -y \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    portaudio19-dev \\\n    ffmpeg \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\nRUN pip3 install openai-whisper\nRUN pip3 install pyaudio sounddevice numpy scipy\n\nWORKDIR /app\nCOPY . .\nCMD ["python3", "voice_processor.py"]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-audio-processing-pipeline",children:"Real-time Audio Processing Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots, real-time audio processing is essential for natural interaction. The pipeline must handle audio capture, preprocessing, and recognition with minimal latency."}),"\n",(0,s.jsx)(n.h3,{id:"audio-capture-and-preprocessing",children:"Audio Capture and Preprocessing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nimport whisper\nimport torch\nfrom collections import deque\nimport time\n\nclass WhisperVoiceProcessor(Node):\n    def __init__(self):\n        super().__init__('whisper_voice_processor')\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Audio chunk size\n        self.channels = 1  # Mono audio\n        self.format = pyaudio.paFloat32\n\n        # Whisper model\n        self.model = whisper.load_model(\"small\")  # Adjust based on resources\n        self.is_cuda_available = torch.cuda.is_available()\n\n        # Audio buffer for continuous processing\n        self.audio_buffer = deque(maxlen=int(self.rate * 2))  # 2 seconds buffer\n        self.recording = False\n        self.audio_queue = queue.Queue()\n\n        # ROS publishers\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\n        self.status_pub = self.create_publisher(String, '/voice_status', 10)\n\n        # Initialize audio stream\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk,\n            stream_callback=self.audio_callback\n        )\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info('Whisper Voice Processor initialized')\n\n    def audio_callback(self, in_data, frame_count, time_info, status):\n        \"\"\"Callback for audio input stream\"\"\"\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(in_data, dtype=np.float32)\n\n        # Add to buffer for processing\n        for sample in audio_data:\n            self.audio_buffer.append(sample)\n\n        # Add to processing queue\n        self.audio_queue.put(audio_data.copy())\n\n        return (None, pyaudio.paContinue)\n\n    def process_audio(self):\n        \"\"\"Process audio chunks for voice commands\"\"\"\n        while rclpy.ok():\n            try:\n                # Get audio chunk from queue\n                audio_chunk = self.audio_queue.get(timeout=0.1)\n\n                # Check if audio chunk has sufficient energy (voice activity detection)\n                if self.is_voice_present(audio_chunk):\n                    self.get_logger().info('Voice activity detected')\n                    self.status_pub.publish(String(data='listening'))\n\n                    # Collect audio for command (accumulate 2 seconds of audio)\n                    command_audio = self.collect_command_audio()\n\n                    if len(command_audio) > 0:\n                        # Process with Whisper\n                        command = self.transcribe_audio(command_audio)\n\n                        if command and self.is_command_valid(command):\n                            self.get_logger().info(f'Command recognized: {command}')\n                            self.command_pub.publish(String(data=command))\n                            self.status_pub.publish(String(data='command_received'))\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error processing audio: {e}')\n\n    def is_voice_present(self, audio_chunk):\n        \"\"\"Simple voice activity detection\"\"\"\n        # Calculate energy of audio chunk\n        energy = np.mean(np.abs(audio_chunk))\n        threshold = 0.01  # Adjust based on environment\n        return energy > threshold\n\n    def collect_command_audio(self, duration=2.0):\n        \"\"\"Collect audio for command recognition\"\"\"\n        samples_needed = int(self.rate * duration)\n        collected_audio = []\n\n        start_time = time.time()\n        while len(collected_audio) < samples_needed and (time.time() - start_time) < duration:\n            try:\n                chunk = self.audio_queue.get(timeout=0.1)\n                collected_audio.extend(chunk)\n            except queue.Empty:\n                continue\n\n        return np.array(collected_audio)\n\n    def transcribe_audio(self, audio_data):\n        \"\"\"Transcribe audio using Whisper\"\"\"\n        try:\n            # Convert to appropriate format for Whisper\n            audio_tensor = torch.from_numpy(audio_data.astype(np.float32))\n\n            # Move to GPU if available\n            if self.is_cuda_available:\n                audio_tensor = audio_tensor.cuda()\n\n            # Transcribe using Whisper\n            result = self.model.transcribe(audio_tensor, fp16=self.is_cuda_available)\n            return result[\"text\"].strip()\n        except Exception as e:\n            self.get_logger().error(f'Whisper transcription error: {e}')\n            return None\n\n    def is_command_valid(self, command):\n        \"\"\"Validate if the recognized command is a valid robot command\"\"\"\n        # Define valid command patterns\n        valid_keywords = [\n            'move', 'walk', 'go', 'stop', 'turn', 'look', 'find', 'grasp', 'pick', 'place',\n            'hello', 'help', 'follow', 'come', 'back', 'forward', 'left', 'right'\n        ]\n\n        # Check if command contains valid keywords\n        command_lower = command.lower()\n        for keyword in valid_keywords:\n            if keyword in command_lower:\n                return True\n\n        return False\n\n    def destroy_node(self):\n        \"\"\"Clean up audio resources\"\"\"\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = WhisperVoiceProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"noise-reduction-and-audio-enhancement",children:"Noise Reduction and Audio Enhancement"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots operate in challenging acoustic environments with motor noise, ambient sounds, and varying acoustic conditions. Effective noise reduction is crucial for reliable voice command recognition."}),"\n",(0,s.jsx)(n.h3,{id:"audio-preprocessing-pipeline",children:"Audio Preprocessing Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import scipy.signal as signal\nimport numpy as np\nfrom scipy import ndimage\n\nclass AudioPreprocessor:\n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n\n        # Design noise reduction filters\n        self.high_pass_filter = self.design_high_pass_filter()\n        self.low_pass_filter = self.design_low_pass_filter()\n\n    def design_high_pass_filter(self, cutoff=80):\n        """Design high-pass filter to remove low-frequency noise"""\n        nyquist = self.sample_rate / 2\n        normalized_cutoff = cutoff / nyquist\n        b, a = signal.butter(4, normalized_cutoff, btype=\'high\', analog=False)\n        return b, a\n\n    def design_low_pass_filter(self, cutoff=3400):\n        """Design low-pass filter to remove high-frequency noise"""\n        nyquist = self.sample_rate / 2\n        normalized_cutoff = cutoff / nyquist\n        b, a = signal.butter(4, normalized_cutoff, btype=\'low\', analog=False)\n        return b, a\n\n    def preprocess_audio(self, audio_data):\n        """Apply preprocessing to audio data"""\n        # Apply high-pass filter to remove DC offset and low-frequency noise\n        filtered_audio = signal.filtfilt(*self.high_pass_filter, audio_data)\n\n        # Apply low-pass filter to remove high-frequency noise\n        filtered_audio = signal.filtfilt(*self.low_pass_filter, filtered_audio)\n\n        # Normalize audio\n        filtered_audio = filtered_audio / np.max(np.abs(filtered_audio))\n\n        # Apply noise reduction (simple spectral subtraction)\n        enhanced_audio = self.spectral_subtraction(filtered_audio)\n\n        return enhanced_audio\n\n    def spectral_subtraction(self, audio_data, noise_frames=100):\n        """Simple noise reduction using spectral subtraction"""\n        # Estimate noise spectrum from initial frames\n        noise_segment = audio_data[:noise_frames]\n        noise_spectrum = np.abs(np.fft.fft(noise_segment))\n\n        # Apply spectral subtraction\n        audio_spectrum = np.fft.fft(audio_data)\n        enhanced_spectrum = np.maximum(\n            np.abs(audio_spectrum) - noise_spectrum[:len(audio_spectrum)],\n            0.1 * np.abs(audio_spectrum)\n        )\n\n        # Reconstruct signal\n        enhanced_audio = np.real(np.fft.ifft(enhanced_spectrum * np.exp(1j * np.angle(audio_spectrum))))\n\n        return enhanced_audio\n'})}),"\n",(0,s.jsx)(n.h2,{id:"voice-command-grammar-and-recognition",children:"Voice Command Grammar and Recognition"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robot control, it's important to define structured voice command grammars that can be reliably recognized and parsed."}),"\n",(0,s.jsx)(n.h3,{id:"command-grammar-definition",children:"Command Grammar Definition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Structure for parsed voice commands\"\"\"\n    action: str\n    target: Optional[str] = None\n    direction: Optional[str] = None\n    distance: Optional[float] = None\n    speed: Optional[str] = None\n\nclass CommandGrammar:\n    def __init__(self):\n        # Define command patterns\n        self.patterns = {\n            'move': [\n                r'move\\s+(?P<direction>forward|backward|left|right|up|down)',\n                r'go\\s+(?P<direction>forward|backward|left|right)',\n                r'walk\\s+(?P<direction>forward|backward|left|right)',\n                r'(?P<direction>forward|backward|left|right)\\s+(?P<distance>\\d+(?:\\.\\d+)?)\\s*(?:meters?|m)',\n            ],\n            'turn': [\n                r'turn\\s+(?P<direction>left|right)',\n                r'rotate\\s+(?P<direction>left|right)',\n                r'pivot\\s+(?P<direction>left|right)',\n            ],\n            'locate': [\n                r'find\\s+(?P<target>[\\w\\s]+)',\n                r'look\\s+for\\s+(?P<target>[\\w\\s]+)',\n                r'search\\s+for\\s+(?P<target>[\\w\\s]+)',\n            ],\n            'grasp': [\n                r'pick\\s+up\\s+(?P<target>[\\w\\s]+)',\n                r'grasp\\s+(?P<target>[\\w\\s]+)',\n                r'grab\\s+(?P<target>[\\w\\s]+)',\n                r'get\\s+(?P<target>[\\w\\s]+)',\n            ],\n            'stop': [\n                r'stop',\n                r'halt',\n                r'freeze',\n            ],\n            'follow': [\n                r'follow\\s+(?P<target>me|[\\w\\s]+)',\n                r'come\\s+with\\s+(?P<target>me|[\\w\\s]+)',\n            ]\n        }\n\n    def parse_command(self, text: str) -> Optional[VoiceCommand]:\n        \"\"\"Parse voice command text into structured command\"\"\"\n        text_lower = text.lower().strip()\n\n        for action, patterns in self.patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    groups = match.groupdict()\n                    return VoiceCommand(\n                        action=action,\n                        target=groups.get('target'),\n                        direction=groups.get('direction'),\n                        distance=self._extract_distance(groups, text_lower),\n                        speed=groups.get('speed', 'normal')\n                    )\n\n        return None\n\n    def _extract_distance(self, groups, text):\n        \"\"\"Extract distance value from command\"\"\"\n        if 'distance' in groups:\n            try:\n                return float(groups['distance'])\n            except ValueError:\n                pass\n\n        # Look for distance in the full text\n        distance_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:meters?|m|cm|centimeters?)', text)\n        if distance_match:\n            distance_val = float(distance_match.group(1))\n            unit = distance_match.group(0).split()[-1]\n            if 'cm' in unit or 'centimeters' in unit:\n                distance_val /= 100.0  # Convert cm to meters\n            return distance_val\n\n        return None\n\n# Example usage\ngrammar = CommandGrammar()\ncommand = grammar.parse_command(\"Move forward 2 meters\")\nprint(f\"Parsed command: {command}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.p,{children:"The Whisper voice processing system must integrate seamlessly with ROS 2 for real-time robot control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom action_msgs.msg import GoalStatus\nfrom builtin_interfaces.msg import Duration\nimport json\n\nclass VoiceCommandIntegrator(Node):\n    def __init__(self):\n        super().__init__('voice_command_integrator')\n\n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, '/voice_command', self.voice_callback, 10)\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, '/voice_status', 10)\n\n        # Command grammar\n        self.grammar = CommandGrammar()\n\n        self.get_logger().info('Voice Command Integrator initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Process incoming voice commands\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received voice command: {command_text}')\n\n        # Parse the command\n        command = self.grammar.parse_command(command_text)\n\n        if command:\n            self.get_logger().info(f'Parsed command: {command.action}')\n            self.execute_command(command)\n        else:\n            self.get_logger().warn(f'Could not parse command: {command_text}')\n            self.status_pub.publish(String(data='command_not_recognized'))\n\n    def execute_command(self, command):\n        \"\"\"Execute parsed voice command\"\"\"\n        if command.action == 'move':\n            self.execute_move_command(command)\n        elif command.action == 'turn':\n            self.execute_turn_command(command)\n        elif command.action == 'stop':\n            self.execute_stop_command()\n        elif command.action == 'follow':\n            self.execute_follow_command(command)\n        else:\n            self.get_logger().warn(f'Unknown command action: {command.action}')\n            self.status_pub.publish(String(data='command_not_supported'))\n\n    def execute_move_command(self, command):\n        \"\"\"Execute move commands\"\"\"\n        twist = Twist()\n\n        if command.direction == 'forward':\n            twist.linear.x = 0.5  # Default speed\n        elif command.direction == 'backward':\n            twist.linear.x = -0.5\n        elif command.direction == 'left':\n            twist.linear.y = 0.5\n        elif command.direction == 'right':\n            twist.linear.y = -0.5\n\n        # Apply distance if specified\n        if command.distance:\n            # This would require a more complex system to move for specific distance\n            self.get_logger().info(f'Moving {command.distance} meters')\n\n        self.cmd_vel_pub.publish(twist)\n        self.status_pub.publish(String(data='moving'))\n\n    def execute_turn_command(self, command):\n        \"\"\"Execute turn commands\"\"\"\n        twist = Twist()\n\n        if command.direction == 'left':\n            twist.angular.z = 0.5\n        elif command.direction == 'right':\n            twist.angular.z = -0.5\n\n        self.cmd_vel_pub.publish(twist)\n        self.status_pub.publish(String(data='turning'))\n\n    def execute_stop_command(self):\n        \"\"\"Stop robot movement\"\"\"\n        twist = Twist()  # Zero velocities\n        self.cmd_vel_pub.publish(twist)\n        self.status_pub.publish(String(data='stopped'))\n\n    def execute_follow_command(self, command):\n        \"\"\"Execute follow commands (requires additional perception systems)\"\"\"\n        self.get_logger().info('Follow command received - requires perception integration')\n        self.status_pub.publish(String(data='following'))\n\ndef main(args=None):\n    rclpy.init(args=args)\n    integrator = VoiceCommandIntegrator()\n\n    try:\n        rclpy.spin(integrator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        integrator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"For real-time humanoid robot applications, Whisper performance must be optimized:"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class OptimizedWhisperProcessor:\n    def __init__(self, model_size="small"):\n        # Load model with optimizations\n        self.model = whisper.load_model(model_size)\n\n        # Enable GPU processing if available\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n            self.use_gpu = True\n        else:\n            self.use_gpu = False\n\n    def transcribe_with_options(self, audio_tensor):\n        """Transcribe with performance options"""\n        # Use FP16 for faster inference on GPU\n        if self.use_gpu:\n            result = self.model.transcribe(\n                audio_tensor,\n                fp16=True,\n                language=\'en\',\n                temperature=0.0  # For consistent results\n            )\n        else:\n            result = self.model.transcribe(\n                audio_tensor,\n                language=\'en\',\n                temperature=0.0\n            )\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h3,{id:"caching-and-batching",children:"Caching and Batching"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\nimport time\n\nclass CachedWhisperProcessor:\n    def __init__(self):\n        self.model = whisper.load_model("small")\n        self.cache = {}\n        self.cache_timeout = 300  # 5 minutes\n\n    @lru_cache(maxsize=128)\n    def cached_transcribe(self, audio_hash):\n        """Cached transcription for repeated audio"""\n        # This is a simplified example - in practice, you\'d need to\n        # store the actual audio data associated with the hash\n        pass\n\n    def transcribe_with_caching(self, audio_data):\n        """Transcribe with caching for performance"""\n        # Create hash of audio data for caching\n        audio_hash = hash(audio_data.tobytes())\n        current_time = time.time()\n\n        # Check cache\n        if audio_hash in self.cache:\n            cached_result, timestamp = self.cache[audio_hash]\n            if current_time - timestamp < self.cache_timeout:\n                return cached_result\n\n        # Process with Whisper\n        result = self.model.transcribe(audio_data)\n\n        # Store in cache\n        self.cache[audio_hash] = (result, current_time)\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"voice-command-system-launch-configuration",children:"Voice Command System Launch Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete launch file for the voice command system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# voice_command_system.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    model_size = LaunchConfiguration('model_size', default='small')\n\n    # Whisper voice processor\n    whisper_processor = Node(\n        package='voice_command_system',\n        executable='whisper_voice_processor',\n        name='whisper_voice_processor',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'model_size': model_size}\n        ],\n        output='screen'\n    )\n\n    # Voice command integrator\n    command_integrator = Node(\n        package='voice_command_system',\n        executable='voice_command_integrator',\n        name='voice_command_integrator',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Audio preprocessing node\n    audio_preprocessor = Node(\n        package='voice_command_system',\n        executable='audio_preprocessor',\n        name='audio_preprocessor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        whisper_processor,\n        command_integrator,\n        audio_preprocessor\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercise-41-implement-a-whisper-voice-command-system",children:"Hands-on Exercise 4.1: Implement a Whisper Voice Command System"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete Whisper-based voice command system for humanoid robot control:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install Whisper and dependencies"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper torch torchaudio pyaudio sounddevice\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create the voice processing node"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Create whisper_voice_processor.py with the code from earlier examples\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test the voice command system"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch the voice command system\nros2 launch voice_command_system.launch.py\n\n# Terminal 2: Listen to voice commands\nros2 topic echo /voice_command\n\n# Terminal 3: Listen to system status\nros2 topic echo /voice_status\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice commands to test"}),":"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Move forward 1 meter"'}),"\n",(0,s.jsx)(n.li,{children:'"Turn left"'}),"\n",(0,s.jsx)(n.li,{children:'"Stop"'}),"\n",(0,s.jsx)(n.li,{children:'"Find the ball"'}),"\n",(0,s.jsx)(n.li,{children:'"Come to me"'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"})," provides robust speech recognition capabilities for humanoid robot voice commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time processing"})," requires optimization and efficient audio pipeline design"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise reduction"})," is essential for reliable operation in robot environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command grammars"})," help structure voice input for reliable parsing and execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 integration"})," enables seamless connection between voice processing and robot control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance optimization"})," is crucial for maintaining responsive voice interaction"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"How does Whisper's accuracy compare to other ASR systems in noisy robot environments?"}),"\n",(0,s.jsx)(n.li,{children:"What are the computational requirements for real-time Whisper processing on humanoid robots?"}),"\n",(0,s.jsx)(n.li,{children:"How can voice command grammars be designed to balance natural language with reliability?"}),"\n",(0,s.jsx)(n.li,{children:"What safety considerations are important when using voice commands for robot control?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,s.jsxs)(n.p,{children:["Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). Robust speech recognition via large-scale weak supervision. ",(0,s.jsx)(n.em,{children:"arXiv preprint arXiv:2212.04356"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,s.jsx)(n.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,s.jsxs)(n.p,{children:["Griffin, R. J., Wiedebach, G., Malhotra, S., Leonessa, A., & Pratt, J. (2017). Walking with reduced foot rotation on compliant terrain using the humanoid robot ATLAS. ",(0,s.jsx)(n.em,{children:"Proceedings of the IEEE-RAS International Conference on Humanoid Robots"}),", 508-513. ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/HUMANOIDS.2017.8246925",children:"https://doi.org/10.1109/HUMANOIDS.2017.8246925"})]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This lesson covered Whisper-based speech-to-command processing for humanoid robots, including installation, real-time audio processing, noise reduction, command grammars, and ROS 2 integration. The voice processing system enables natural human-robot interaction through spoken commands, which is essential for intuitive humanoid robot control."}),"\n",(0,s.jsx)(n.p,{children:"In the next lesson, we'll explore LLM cognitive planning systems that translate high-level instructions into specific robot behaviors."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>t});var o=i(6540);const s={},a=o.createContext(s);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);