"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[0],{8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),i.createElement(o.Provider,{value:e},n.children)}},9247:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4/lesson-2-llm-cognitive-planning","title":"Lesson 2 - LLM Cognitive Planning - Action Graphs, Task Decomposition, Planning","description":"Learning Objectives","source":"@site/docs/module-4/lesson-2-llm-cognitive-planning.md","sourceDirName":"module-4","slug":"/module-4/lesson-2-llm-cognitive-planning","permalink":"/hackathon-robotics-book/docs/module-4/lesson-2-llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-4/lesson-2-llm-cognitive-planning.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 2 - LLM Cognitive Planning - Action Graphs, Task Decomposition, Planning","sidebar_label":"LLM Cognitive Planning"},"sidebar":"textbookSidebar","previous":{"title":"Whisper Speech-to-Command","permalink":"/hackathon-robotics-book/docs/module-4/lesson-1-whisper-speech-to-command"},"next":{"title":"Capstone - Full Digital Humanoid Agent","permalink":"/hackathon-robotics-book/docs/module-4/lesson-3-capstone-full-digital-humanoid-agent"}}');var a=t(4848),o=t(8453);const s={title:"Lesson 2 - LLM Cognitive Planning - Action Graphs, Task Decomposition, Planning",sidebar_label:"LLM Cognitive Planning"},r="Lesson 2: LLM Cognitive Planning - Action Graphs, Task Decomposition, Planning",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"LLM Architecture for Cognitive Planning",id:"llm-architecture-for-cognitive-planning",level:2},{value:"Transformer-Based Reasoning",id:"transformer-based-reasoning",level:3},{value:"Planning-Specific LLM Considerations",id:"planning-specific-llm-considerations",level:3},{value:"Action Graph Representation",id:"action-graph-representation",level:2},{value:"Action Graph Structure",id:"action-graph-structure",level:3},{value:"Action Graph Generation with LLMs",id:"action-graph-generation-with-llms",level:3},{value:"Cognitive Planning Node Implementation",id:"cognitive-planning-node-implementation",level:2},{value:"Prompt Engineering for Robotic Planning",id:"prompt-engineering-for-robotic-planning",level:2},{value:"System Prompt Design",id:"system-prompt-design",level:3},{value:"Task Decomposition Strategies",id:"task-decomposition-strategies",level:2},{value:"Hierarchical Task Networks (HTN)",id:"hierarchical-task-networks-htn",level:3},{value:"Integration with Behavior Trees",id:"integration-with-behavior-trees",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Pre-computation",id:"caching-and-pre-computation",level:3},{value:"Hands-on Exercise 4.2: Implement an LLM Cognitive Planning System",id:"hands-on-exercise-42-implement-an-llm-cognitive-planning-system",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"lesson-2-llm-cognitive-planning---action-graphs-task-decomposition-planning",children:"Lesson 2: LLM Cognitive Planning - Action Graphs, Task Decomposition, Planning"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design and implement LLM-based cognitive planning systems for humanoid robots"}),"\n",(0,a.jsx)(e.li,{children:"Create action graphs that decompose complex tasks into executable robot behaviors"}),"\n",(0,a.jsx)(e.li,{children:"Engineer effective prompts for robotic planning and control using LLMs"}),"\n",(0,a.jsx)(e.li,{children:"Integrate LLM cognitive planning with ROS 2 action servers and behavior trees"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate and optimize cognitive planning performance for real-time robot applications"}),"\n",(0,a.jsx)(e.li,{children:"Implement task decomposition strategies for complex humanoid robot behaviors"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models (LLMs) represent a paradigm shift in robotic cognitive planning, enabling robots to understand and execute complex, high-level instructions expressed in natural language. For humanoid robots, LLM cognitive planning bridges the gap between human communication and robotic action, allowing robots to interpret abstract goals and decompose them into sequences of specific behaviors."}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning with LLMs involves several key components: understanding natural language instructions, decomposing complex tasks into manageable subtasks, generating executable action sequences, and adapting plans based on environmental feedback. This lesson explores how to implement these capabilities using modern LLMs and integrate them with humanoid robot control systems."}),"\n",(0,a.jsx)(e.h2,{id:"llm-architecture-for-cognitive-planning",children:"LLM Architecture for Cognitive Planning"}),"\n",(0,a.jsx)(e.h3,{id:"transformer-based-reasoning",children:"Transformer-Based Reasoning"}),"\n",(0,a.jsx)(e.p,{children:"LLMs use transformer architectures that excel at understanding context and generating structured outputs. For cognitive planning, this means they can:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Parse complex natural language instructions"}),"\n",(0,a.jsx)(e.li,{children:"Maintain context across multiple planning steps"}),"\n",(0,a.jsx)(e.li,{children:"Generate structured action sequences"}),"\n",(0,a.jsx)(e.li,{children:"Adapt plans based on new information"}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-mermaid",children:"graph TD\n    A[Natural Language Instruction] --\x3e B[LLM Encoder]\n    B --\x3e C[Context Understanding]\n    C --\x3e D[Task Decomposition]\n    D --\x3e E[Action Graph Generation]\n    E --\x3e F[Executable Actions]\n    F --\x3e G[Robot Execution]\n\n    H[Environmental Feedback] --\x3e I[Plan Adaptation]\n    I --\x3e E\n    G --\x3e H\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.em,{children:"Figure 1: LLM cognitive planning architecture showing the flow from natural language to robot actions with environmental feedback."})}),"\n",(0,a.jsx)(e.h3,{id:"planning-specific-llm-considerations",children:"Planning-Specific LLM Considerations"}),"\n",(0,a.jsx)(e.p,{children:"For cognitive planning applications, several factors affect LLM performance:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Structured Output"}),": LLMs need to generate well-structured action sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Knowledge"}),": Models need to understand robot capabilities and constraints"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context Length"}),": Complex tasks may require long planning horizons"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reliability"}),": Planning outputs must be consistent and predictable"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"action-graph-representation",children:"Action Graph Representation"}),"\n",(0,a.jsx)(e.p,{children:"Action graphs provide a structured representation of tasks and their dependencies, enabling complex task decomposition and execution planning."}),"\n",(0,a.jsx)(e.h3,{id:"action-graph-structure",children:"Action Graph Structure"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom typing import List, Dict, Any, Optional\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport json\n\nclass ActionStatus(Enum):\n    PENDING = "pending"\n    RUNNING = "running"\n    SUCCESS = "success"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\n@dataclass\nclass ActionNode:\n    """Represents a single action in the action graph"""\n    id: str\n    name: str\n    action_type: str  # e.g., "navigation", "manipulation", "perception"\n    parameters: Dict[str, Any]\n    prerequisites: List[str]  # IDs of actions that must complete first\n    dependencies: List[str]   # IDs of actions that depend on this one\n    status: ActionStatus = ActionStatus.PENDING\n    result: Optional[Any] = None\n\nclass ActionGraph:\n    """Represents a graph of interconnected actions"""\n    def __init__(self):\n        self.nodes: Dict[str, ActionNode] = {}\n        self.start_nodes: List[str] = []  # Nodes with no prerequisites\n        self.end_nodes: List[str] = []    # Nodes that no other nodes depend on\n\n    def add_node(self, node: ActionNode):\n        """Add a node to the action graph"""\n        self.nodes[node.id] = node\n\n        # Update start and end nodes\n        if not node.prerequisites:\n            if node.id not in self.start_nodes:\n                self.start_nodes.append(node.id)\n\n        # Check if this node is an end node\n        is_end = True\n        for other_node in self.nodes.values():\n            if node.id in other_node.prerequisites:\n                is_end = False\n                break\n\n        if is_end and node.id not in self.end_nodes:\n            self.end_nodes.append(node.id)\n\n    def get_ready_actions(self, completed_actions: List[str]) -> List[ActionNode]:\n        """Get actions that are ready to execute"""\n        ready = []\n        for node_id, node in self.nodes.items():\n            if node.status == ActionStatus.PENDING:\n                # Check if all prerequisites are completed\n                all_prereqs_met = True\n                for prereq in node.prerequisites:\n                    if prereq not in completed_actions:\n                        all_prereqs_met = False\n                        break\n\n                if all_prereqs_met:\n                    ready.append(node)\n\n        return ready\n\n    def update_node_status(self, node_id: str, status: ActionStatus, result: Any = None):\n        """Update the status of a node"""\n        if node_id in self.nodes:\n            self.nodes[node_id].status = status\n            if result is not None:\n                self.nodes[node_id].result = result\n\n    def is_complete(self) -> bool:\n        """Check if all nodes are completed"""\n        for node in self.nodes.values():\n            if node.status not in [ActionStatus.SUCCESS, ActionStatus.FAILED]:\n                return False\n        return True\n\n    def get_execution_order(self) -> List[str]:\n        """Get a valid execution order for the graph"""\n        # Topological sort implementation\n        visited = set()\n        order = []\n\n        def dfs(node_id):\n            if node_id in visited:\n                return\n            visited.add(node_id)\n\n            node = self.nodes[node_id]\n            for dep_id in node.dependencies:\n                if dep_id in self.nodes:\n                    dfs(dep_id)\n\n            order.append(node_id)\n\n        for start_node in self.start_nodes:\n            dfs(start_node)\n\n        return order\n'})}),"\n",(0,a.jsx)(e.h3,{id:"action-graph-generation-with-llms",children:"Action Graph Generation with LLMs"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import Dict, List\n\nclass LLMActionGraphGenerator:\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        openai.api_key = api_key\n        self.model = model\n        self.robot_capabilities = self.get_robot_capabilities()\n\n    def get_robot_capabilities(self) -> Dict:\n        """Define what the robot can do"""\n        return {\n            "navigation": {\n                "move_to": {\n                    "parameters": ["location", "speed"],\n                    "description": "Move the robot to a specified location"\n                },\n                "turn": {\n                    "parameters": ["direction", "angle"],\n                    "description": "Turn the robot in a specified direction"\n                }\n            },\n            "manipulation": {\n                "pick_up": {\n                    "parameters": ["object", "location"],\n                    "description": "Pick up an object from a location"\n                },\n                "place": {\n                    "parameters": ["object", "location"],\n                    "description": "Place an object at a location"\n                },\n                "grasp": {\n                    "parameters": ["object", "grasp_type"],\n                    "description": "Grasp an object"\n                }\n            },\n            "perception": {\n                "detect_object": {\n                    "parameters": ["object_type", "search_area"],\n                    "description": "Detect objects of a specific type in an area"\n                },\n                "identify": {\n                    "parameters": ["target", "properties"],\n                    "description": "Identify specific properties of a target"\n                }\n            }\n        }\n\n    def generate_action_graph(self, instruction: str) -> ActionGraph:\n        """Generate an action graph from a natural language instruction"""\n        prompt = self.create_planning_prompt(instruction)\n\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self.get_system_prompt()},\n                {"role": "user", "content": prompt}\n            ],\n            temperature=0.1,  # Low temperature for consistent outputs\n            functions=[\n                {\n                    "name": "create_action_graph",\n                    "description": "Create an action graph for the robot to execute",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "actions": {\n                                "type": "array",\n                                "items": {\n                                    "type": "object",\n                                    "properties": {\n                                        "id": {"type": "string"},\n                                        "name": {"type": "string"},\n                                        "action_type": {"type": "string"},\n                                        "parameters": {"type": "object"},\n                                        "prerequisites": {\n                                            "type": "array",\n                                            "items": {"type": "string"}\n                                        }\n                                    },\n                                    "required": ["id", "name", "action_type", "parameters"]\n                                }\n                            }\n                        },\n                        "required": ["actions"]\n                    }\n                }\n            ],\n            function_call={"name": "create_action_graph"}\n        )\n\n        # Parse the response\n        function_call = response.choices[0].message.function_call\n        if function_call:\n            graph_data = json.loads(function_call.arguments)\n            return self.create_action_graph_from_data(graph_data)\n\n        return ActionGraph()\n\n    def create_planning_prompt(self, instruction: str) -> str:\n        """Create a prompt for the LLM to generate an action graph"""\n        return f"""\n        Instruction: {instruction}\n\n        Robot Capabilities:\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Please create an action graph to execute this instruction. The graph should:\n        1. Break down the task into specific, executable actions\n        2. Define dependencies between actions\n        3. Use only the robot capabilities listed above\n        4. Ensure the actions are in a logical order\n        5. Include all necessary parameters for each action\n\n        Create the action graph with appropriate prerequisite relationships.\n        """\n\n    def get_system_prompt(self) -> str:\n        """System prompt for the LLM"""\n        return """\n        You are an expert robot cognitive planner. Your task is to break down natural language instructions into executable action graphs for humanoid robots.\n\n        Each action should be specific, executable, and use the provided robot capabilities. Define proper dependencies between actions where one action must complete before another can start. Always ensure the action graph represents a valid plan that achieves the given instruction.\n        """\n\n    def create_action_graph_from_data(self, graph_data: Dict) -> ActionGraph:\n        """Create an ActionGraph from LLM response data"""\n        graph = ActionGraph()\n\n        for action_data in graph_data["actions"]:\n            node = ActionNode(\n                id=action_data["id"],\n                name=action_data["name"],\n                action_type=action_data["action_type"],\n                parameters=action_data.get("parameters", {}),\n                prerequisites=action_data.get("prerequisites", []),\n                dependencies=[]  # Will be filled in based on prerequisites\n            )\n            graph.add_node(node)\n\n        # Fill in dependencies based on prerequisites\n        for node_id, node in graph.nodes.items():\n            for prereq_id in node.prerequisites:\n                if prereq_id in graph.nodes:\n                    graph.nodes[prereq_id].dependencies.append(node_id)\n\n        return graph\n'})}),"\n",(0,a.jsx)(e.h2,{id:"cognitive-planning-node-implementation",children:"Cognitive Planning Node Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nimport threading\nimport time\nfrom typing import Dict, Any\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__('cognitive_planner')\n\n        # Publishers and subscribers\n        self.instruction_sub = self.create_subscription(\n            String, '/natural_language_instruction', self.instruction_callback, 10)\n        self.status_pub = self.create_publisher(String, '/cognitive_planner_status', 10)\n        self.action_status_pub = self.create_publisher(String, '/action_status', 10)\n\n        # Action clients for different robot capabilities\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.manipulation_client = ActionClient(self, ManipulateObject, 'manipulate_object')\n        self.perception_client = ActionClient(self, DetectObjects, 'detect_objects')\n\n        # Planning components\n        self.graph_generator = LLMActionGraphGenerator(api_key=\"your-api-key\")\n        self.current_graph = None\n        self.completed_actions = []\n        self.execution_thread = None\n        self.planning_lock = threading.Lock()\n\n        # Planning parameters\n        self.max_replanning_attempts = 3\n        self.action_timeout = 30.0  # seconds\n\n        self.get_logger().info('Cognitive Planner Node initialized')\n\n    def instruction_callback(self, msg):\n        \"\"\"Process natural language instruction\"\"\"\n        instruction = msg.data\n        self.get_logger().info(f'Received instruction: {instruction}')\n\n        # Generate action graph\n        try:\n            with self.planning_lock:\n                self.current_graph = self.graph_generator.generate_action_graph(instruction)\n\n                if not self.current_graph.nodes:\n                    self.get_logger().error('Failed to generate action graph')\n                    self.status_pub.publish(String(data='planning_failed'))\n                    return\n\n                self.get_logger().info(f'Generated action graph with {len(self.current_graph.nodes)} actions')\n                self.status_pub.publish(String(data='planning_completed'))\n\n                # Start execution\n                self.completed_actions = []\n                self.execute_action_graph()\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating action graph: {e}')\n            self.status_pub.publish(String(data='planning_error'))\n\n    def execute_action_graph(self):\n        \"\"\"Execute the action graph\"\"\"\n        if self.execution_thread and self.execution_thread.is_alive():\n            self.get_logger().warn('Execution already in progress')\n            return\n\n        self.execution_thread = threading.Thread(target=self._execute_graph)\n        self.execution_thread.daemon = True\n        self.execution_thread.start()\n\n    def _execute_graph(self):\n        \"\"\"Execute the action graph in a separate thread\"\"\"\n        while not self.current_graph.is_complete():\n            # Get ready actions\n            ready_actions = self.current_graph.get_ready_actions(self.completed_actions)\n\n            if not ready_actions:\n                # Check if we're waiting for actions to complete\n                time.sleep(0.1)\n                continue\n\n            # Execute ready actions\n            for action_node in ready_actions:\n                if action_node.status == ActionStatus.PENDING:\n                    self.execute_action(action_node)\n\n            # Check for timeout\n            time.sleep(0.1)\n\n        self.get_logger().info('Action graph execution completed')\n        self.status_pub.publish(String(data='execution_completed'))\n\n    def execute_action(self, action_node: ActionNode):\n        \"\"\"Execute a single action\"\"\"\n        self.get_logger().info(f'Executing action: {action_node.name}')\n        self.current_graph.update_node_status(action_node.id, ActionStatus.RUNNING)\n        self.action_status_pub.publish(String(data=f'executing_{action_node.id}'))\n\n        try:\n            # Route to appropriate action handler based on type\n            if action_node.action_type == 'navigation':\n                success = self.execute_navigation_action(action_node)\n            elif action_node.action_type == 'manipulation':\n                success = self.execute_manipulation_action(action_node)\n            elif action_node.action_type == 'perception':\n                success = self.execute_perception_action(action_node)\n            else:\n                self.get_logger().error(f'Unknown action type: {action_node.action_type}')\n                success = False\n\n            # Update status\n            if success:\n                self.current_graph.update_node_status(action_node.id, ActionStatus.SUCCESS)\n                self.completed_actions.append(action_node.id)\n                self.action_status_pub.publish(String(data=f'success_{action_node.id}'))\n            else:\n                self.current_graph.update_node_status(action_node.id, ActionStatus.FAILED)\n                self.action_status_pub.publish(String(data=f'failed_{action_node.id}'))\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing action {action_node.id}: {e}')\n            self.current_graph.update_node_status(action_node.id, ActionStatus.FAILED)\n            self.action_status_pub.publish(String(data=f'error_{action_node.id}'))\n\n    def execute_navigation_action(self, action_node: ActionNode):\n        \"\"\"Execute navigation action\"\"\"\n        # Extract parameters\n        location = action_node.parameters.get('location')\n        speed = action_node.parameters.get('speed', 0.5)\n\n        if not location:\n            self.get_logger().error('Navigation action missing location parameter')\n            return False\n\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.pose.position.x = float(location.get('x', 0.0))\n        goal_msg.pose.pose.position.y = float(location.get('y', 0.0))\n        goal_msg.pose.pose.position.z = float(location.get('z', 0.0))\n\n        # Set orientation (assuming facing forward)\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        # Send goal\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(lambda f: self.navigation_done_callback(f, action_node.id))\n\n        # Wait for result with timeout\n        start_time = time.time()\n        while time.time() - start_time < self.action_timeout:\n            if action_node.status == ActionStatus.SUCCESS or action_node.status == ActionStatus.FAILED:\n                return action_node.status == ActionStatus.SUCCESS\n            time.sleep(0.1)\n\n        # Timeout\n        self.get_logger().warn(f'Navigation action {action_node.id} timed out')\n        return False\n\n    def execute_manipulation_action(self, action_node: ActionNode):\n        \"\"\"Execute manipulation action\"\"\"\n        # Similar to navigation but for manipulation tasks\n        # Implementation would depend on specific manipulation capabilities\n        self.get_logger().info(f'Executing manipulation: {action_node.parameters}')\n        # For now, return success\n        return True\n\n    def execute_perception_action(self, action_node: ActionNode):\n        \"\"\"Execute perception action\"\"\"\n        # Similar to navigation but for perception tasks\n        self.get_logger().info(f'Executing perception: {action_node.parameters}')\n        # For now, return success\n        return True\n\n    def navigation_done_callback(self, future, action_id):\n        \"\"\"Handle navigation action completion\"\"\"\n        try:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                result_future.add_done_callback(\n                    lambda f: self.navigation_result_callback(f, action_id)\n                )\n            else:\n                self.get_logger().error(f'Navigation goal for action {action_id} was rejected')\n                self.current_graph.update_node_status(action_id, ActionStatus.FAILED)\n        except Exception as e:\n            self.get_logger().error(f'Error in navigation callback: {e}')\n            self.current_graph.update_node_status(action_id, ActionStatus.FAILED)\n\n    def navigation_result_callback(self, future, action_id):\n        \"\"\"Handle navigation result\"\"\"\n        try:\n            result = future.result().result\n            if result:\n                self.current_graph.update_node_status(action_id, ActionStatus.SUCCESS)\n                self.completed_actions.append(action_id)\n            else:\n                self.current_graph.update_node_status(action_id, ActionStatus.FAILED)\n        except Exception as e:\n            self.get_logger().error(f'Error in navigation result callback: {e}')\n            self.current_graph.update_node_status(action_id, ActionStatus.FAILED)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = CognitivePlannerNode()\n\n    # Use multi-threaded executor to handle callbacks\n    executor = MultiThreadedExecutor()\n    executor.add_node(planner)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"prompt-engineering-for-robotic-planning",children:"Prompt Engineering for Robotic Planning"}),"\n",(0,a.jsx)(e.p,{children:"Effective prompt engineering is crucial for getting reliable outputs from LLMs for robotic planning:"}),"\n",(0,a.jsx)(e.h3,{id:"system-prompt-design",children:"System Prompt Design"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PromptEngineer:\n    def __init__(self):\n        self.base_system_prompt = """\n        You are an expert robotic cognitive planner. Your task is to decompose natural language instructions into executable action sequences for humanoid robots.\n\n        Guidelines:\n        1. Always use the robot\'s available capabilities\n        2. Create logical action dependencies\n        3. Include all necessary parameters for each action\n        4. Consider robot safety and physical constraints\n        5. Handle ambiguous instructions by making reasonable assumptions\n        6. Break down complex tasks into simple, executable steps\n        """\n\n    def create_task_decomposition_prompt(self, instruction: str, context: Dict = None) -> str:\n        """Create a prompt for task decomposition"""\n        context_str = json.dumps(context, indent=2) if context else "No additional context provided"\n\n        return f"""\n        {self.base_system_prompt}\n\n        Natural Language Instruction: {instruction}\n\n        Robot Context:\n        {context_str}\n\n        Available Capabilities:\n        - Navigation: move_to(location), turn(direction)\n        - Manipulation: pick_up(object), place(object, location), grasp(object)\n        - Perception: detect_object(type, area), identify(target)\n\n        Please decompose this instruction into a sequence of executable actions with proper dependencies.\n        Each action should have:\n        - A unique ID\n        - A descriptive name\n        - An action type (navigation, manipulation, perception)\n        - Required parameters\n        - Prerequisites (other actions that must complete first)\n\n        Output the action graph in JSON format.\n        """\n\n    def create_safety_aware_prompt(self, instruction: str) -> str:\n        """Create a safety-aware planning prompt"""\n        return f"""\n        {self.base_system_prompt}\n\n        SAFETY CONSTRAINTS (CRITICAL):\n        - Always check for obstacles before navigation\n        - Verify object stability before manipulation\n        - Maintain balance during all movements\n        - Respect human safety zones\n        - Avoid dangerous areas\n\n        Instruction: {instruction}\n\n        Decompose this task with safety considerations integrated into the action sequence.\n        Include safety checks as separate actions where appropriate.\n        """\n\n    def create_context_aware_prompt(self, instruction: str, world_state: Dict) -> str:\n        """Create a context-aware planning prompt"""\n        return f"""\n        {self.base_system_prompt}\n\n        Current World State:\n        {json.dumps(world_state, indent=2)}\n\n        Instruction: {instruction}\n\n        Generate an action graph that takes the current world state into account.\n        Consider object locations, robot position, and environmental conditions.\n        """\n'})}),"\n",(0,a.jsx)(e.h2,{id:"task-decomposition-strategies",children:"Task Decomposition Strategies"}),"\n",(0,a.jsx)(e.h3,{id:"hierarchical-task-networks-htn",children:"Hierarchical Task Networks (HTN)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HTNPlanner:\n    def __init__(self):\n        self.methods = self.define_methods()\n        self.operators = self.define_operators()\n\n    def define_methods(self):\n        \"\"\"Define methods for decomposing high-level tasks\"\"\"\n        return {\n            'fetch_object': [\n                # Method 1: Navigate to object, then grasp\n                [\n                    {'action': 'navigate_to', 'params': {'target': 'object_location'}},\n                    {'action': 'grasp_object', 'params': {'object': 'target_object'}}\n                ],\n                # Method 2: Ask for object if not visible\n                [\n                    {'action': 'search_for_object', 'params': {'object': 'target_object'}},\n                    {'action': 'request_assistance', 'params': {'request': 'locate_object'}}\n                ]\n            ],\n            'serve_beverage': [\n                # Method: Go to kitchen, get beverage, bring to person\n                [\n                    {'action': 'navigate_to', 'params': {'target': 'kitchen'}},\n                    {'action': 'pick_up', 'params': {'object': 'beverage'}},\n                    {'action': 'navigate_to', 'params': {'target': 'person_location'}},\n                    {'action': 'place', 'params': {'object': 'beverage', 'location': 'person_hand'}}\n                ]\n            ]\n        }\n\n    def define_operators(self):\n        \"\"\"Define primitive operators (leaf actions)\"\"\"\n        return {\n            'navigate_to': {\n                'preconditions': ['robot_operational', 'path_clear'],\n                'effects': ['robot_at_location']\n            },\n            'grasp_object': {\n                'preconditions': ['object_reachable', 'gripper_free'],\n                'effects': ['object_grasped', 'gripper_occupied']\n            },\n            'pick_up': {\n                'preconditions': ['object_detectable', 'arm_free'],\n                'effects': ['object_held']\n            }\n        }\n\n    def decompose_task(self, task_name: str, params: Dict) -> List[Dict]:\n        \"\"\"Decompose a high-level task using HTN methods\"\"\"\n        if task_name in self.methods:\n            # Use the first available method for simplicity\n            # In practice, you'd evaluate conditions to choose the best method\n            method = self.methods[task_name][0]\n\n            # Substitute parameters\n            instantiated_method = []\n            for action in method:\n                instantiated_action = action.copy()\n                for key, value in instantiated_action['params'].items():\n                    if isinstance(value, str) and value in params:\n                        instantiated_action['params'][key] = params[value]\n                instantiated_method.append(instantiated_action)\n\n            return instantiated_method\n\n        # If no method exists, return as primitive action\n        return [{'action': task_name, 'params': params}]\n"})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-behavior-trees",children:"Integration with Behavior Trees"}),"\n",(0,a.jsx)(e.p,{children:"LLM cognitive planning can be integrated with behavior trees for more sophisticated control:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class LLMBehaviorTreeIntegrator:\n    def __init__(self):\n        self.llm_planner = LLMActionGraphGenerator(api_key="your-api-key")\n        self.behavior_tree = None\n\n    def create_adaptive_behavior_tree(self, instruction: str) -> Dict:\n        """Create a behavior tree based on LLM-generated action graph"""\n        # Generate action graph from instruction\n        action_graph = self.llm_planner.generate_action_graph(instruction)\n\n        # Convert to behavior tree structure\n        bt_root = {\n            "type": "sequence",\n            "children": []\n        }\n\n        # Get execution order from action graph\n        execution_order = action_graph.get_execution_order()\n\n        for action_id in execution_order:\n            if action_id in action_graph.nodes:\n                node = action_graph.nodes[action_id]\n                bt_action = self.convert_to_bt_node(node)\n                bt_root["children"].append(bt_action)\n\n        return bt_root\n\n    def convert_to_bt_node(self, action_node: ActionNode) -> Dict:\n        """Convert an action graph node to a behavior tree node"""\n        return {\n            "type": "action",\n            "name": action_node.name,\n            "action_type": action_node.action_type,\n            "parameters": action_node.parameters,\n            "id": action_node.id\n        }\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.p,{children:"For real-time cognitive planning, performance optimization is crucial:"}),"\n",(0,a.jsx)(e.h3,{id:"caching-and-pre-computation",children:"Caching and Pre-computation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from functools import lru_cache\nimport pickle\nimport hashlib\n\nclass OptimizedLLMPlanner:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.cache = {}\n        self.cache_size = 1000\n        self.cache_timeout = 3600  # 1 hour\n\n    @lru_cache(maxsize=128)\n    def cached_plan_generation(self, instruction_hash: str, instruction: str) -> ActionGraph:\n        """Cached LLM planning to avoid repeated API calls"""\n        # This would call the LLM to generate the action graph\n        # Implementation depends on specific LLM provider\n        pass\n\n    def get_cached_plan(self, instruction: str) -> Optional[ActionGraph]:\n        """Get cached plan or generate new one"""\n        instruction_hash = hashlib.md5(instruction.encode()).hexdigest()\n\n        if instruction_hash in self.cache:\n            plan, timestamp = self.cache[instruction_hash]\n            if time.time() - timestamp < self.cache_timeout:\n                return plan\n\n        # Generate new plan\n        plan = self.generate_plan(instruction)\n\n        # Store in cache\n        if len(self.cache) >= self.cache_size:\n            # Remove oldest entry\n            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])\n            del self.cache[oldest_key]\n\n        self.cache[instruction_hash] = (plan, time.time())\n        return plan\n\n    def generate_plan(self, instruction: str) -> ActionGraph:\n        """Generate a new plan using LLM"""\n        # Implementation would call LLM API\n        # For now, return empty graph\n        return ActionGraph()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-exercise-42-implement-an-llm-cognitive-planning-system",children:"Hands-on Exercise 4.2: Implement an LLM Cognitive Planning System"}),"\n",(0,a.jsx)(e.p,{children:"Create a complete LLM-based cognitive planning system:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Set up the LLM planning node"}),":"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Install required packages\npip install openai torch transformers\n"})}),"\n",(0,a.jsxs)(e.ol,{start:"2",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Create the cognitive planner"}),":"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# cognitive_planner.py - Implementation from earlier examples\n"})}),"\n",(0,a.jsxs)(e.ol,{start:"3",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Test the system with various instructions"}),":"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Launch the cognitive planner\nros2 run cognitive_planning cognitive_planner_node\n\n# Terminal 2: Send test instructions\nros2 topic pub /natural_language_instruction std_msgs/String \"data: 'Go to the kitchen and bring me a cup'\"\n\n# Terminal 3: Monitor status\nros2 topic echo /cognitive_planner_status\nros2 topic echo /action_status\n"})}),"\n",(0,a.jsxs)(e.ol,{start:"4",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Implement a simple action server for testing"}),":"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom your_interfaces.action import NavigateToPose  # Define your action interface\n\nclass MockActionServer(Node):\n    def __init__(self):\n        super().__init__('mock_action_server')\n        self._action_server = ActionServer(\n            self,\n            NavigateToPose,\n            'navigate_to_pose',\n            self.execute_callback)\n\n    def execute_callback(self, goal_handle):\n        self.get_logger().info('Executing navigation goal...')\n        # Simulate navigation\n        feedback_msg = NavigateToPose.Feedback()\n\n        # Send feedback\n        for i in range(0, 100, 10):\n            feedback_msg.progress = i\n            goal_handle.publish_feedback(feedback_msg)\n            time.sleep(0.5)\n\n        goal_handle.succeed()\n        result = NavigateToPose.Result()\n        result.success = True\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    server = MockActionServer()\n    rclpy.spin(server)\n    server.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"LLM cognitive planning"})," enables natural language task decomposition for humanoid robots"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action graphs"})," provide structured representation of tasks and their dependencies"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Prompt engineering"})," is crucial for reliable LLM outputs in robotic applications"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task decomposition strategies"})," help break down complex instructions into executable actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Performance optimization"})," is essential for real-time cognitive planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Integration with behavior trees"})," enables sophisticated control architectures"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"How can LLM cognitive planning be made more reliable for safety-critical robot applications?"}),"\n",(0,a.jsx)(e.li,{children:"What are the limitations of current LLMs for robotic planning and how can they be addressed?"}),"\n",(0,a.jsx)(e.li,{children:"How does the quality of prompt engineering affect the performance of cognitive planning systems?"}),"\n",(0,a.jsx)(e.li,{children:"What strategies can be used to handle ambiguous or underspecified natural language instructions?"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,a.jsxs)(e.p,{children:["Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. ",(0,a.jsx)(e.em,{children:"Advances in Neural Information Processing Systems"}),", 30, 5998-6008."]}),"\n",(0,a.jsxs)(e.p,{children:["Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. ",(0,a.jsx)(e.em,{children:"Advances in Neural Information Processing Systems"}),", 33, 1877-1901."]}),"\n",(0,a.jsxs)(e.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,a.jsx)(e.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This lesson covered LLM cognitive planning systems for humanoid robots, including action graph representation, task decomposition strategies, prompt engineering, and integration with behavior trees. LLM-based cognitive planning enables robots to understand and execute complex natural language instructions by decomposing them into structured action sequences. The combination of LLMs with structured planning approaches provides a powerful framework for intelligent robot behavior."}),"\n",(0,a.jsx)(e.p,{children:"In the next lesson, we'll explore the complete integration of all VLA components into a full digital humanoid agent."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);