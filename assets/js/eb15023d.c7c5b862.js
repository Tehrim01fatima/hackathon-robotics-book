"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[707],{3603:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3/lesson-3-nav2-path-planning-humanoids","title":"Lesson 3 - Nav2 Path Planning for Humanoids - Biped Locomotion, Path Planning, Navigation","description":"Learning Objectives","source":"@site/docs/module-3/lesson-3-nav2-path-planning-humanoids.md","sourceDirName":"module-3","slug":"/module-3/lesson-3-nav2-path-planning-humanoids","permalink":"/hackathon-robotics-book/docs/module-3/lesson-3-nav2-path-planning-humanoids","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-3/lesson-3-nav2-path-planning-humanoids.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 3 - Nav2 Path Planning for Humanoids - Biped Locomotion, Path Planning, Navigation","sidebar_label":"Nav2 Path Planning for Humanoids"},"sidebar":"textbookSidebar","previous":{"title":"Isaac ROS & Perception VSLAM","permalink":"/hackathon-robotics-book/docs/module-3/lesson-2-isaac-ros-perception-vslam"},"next":{"title":"Overview","permalink":"/hackathon-robotics-book/docs/module-4/"}}');var t=o(4848),i=o(8453);const r={title:"Lesson 3 - Nav2 Path Planning for Humanoids - Biped Locomotion, Path Planning, Navigation",sidebar_label:"Nav2 Path Planning for Humanoids"},s="Lesson 3: Nav2 Path Planning for Humanoids - Biped Locomotion, Path Planning, Navigation",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Nav2 Architecture for Humanoid Robots",id:"nav2-architecture-for-humanoid-robots",level:2},{value:"Standard Nav2 Components",id:"standard-nav2-components",level:3},{value:"Humanoid-Specific Modifications",id:"humanoid-specific-modifications",level:3},{value:"Humanoid-Specific Navigation Configuration",id:"humanoid-specific-navigation-configuration",level:2},{value:"Costmap Configuration for Humanoids",id:"costmap-configuration-for-humanoids",level:3},{value:"Global Planner Configuration",id:"global-planner-configuration",level:3},{value:"Humanoid Path Planning Algorithms",id:"humanoid-path-planning-algorithms",level:2},{value:"Footstep Planning for Biped Locomotion",id:"footstep-planning-for-biped-locomotion",level:3},{value:"Balance-Aware Path Planning",id:"balance-aware-path-planning",level:3},{value:"Humanoid Navigation Controller",id:"humanoid-navigation-controller",level:2},{value:"Integration with Perception Systems",id:"integration-with-perception-systems",level:2},{value:"Safety and Recovery Behaviors",id:"safety-and-recovery-behaviors",level:2},{value:"Navigation Launch Configuration",id:"navigation-launch-configuration",level:2},{value:"Hands-on Exercise 3.3: Configure Nav2 for Humanoid Navigation",id:"hands-on-exercise-33-configure-nav2-for-humanoid-navigation",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function p(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lesson-3-nav2-path-planning-for-humanoids---biped-locomotion-path-planning-navigation",children:"Lesson 3: Nav2 Path Planning for Humanoids - Biped Locomotion, Path Planning, Navigation"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Configure Nav2 for humanoid-specific navigation requirements and constraints"}),"\n",(0,t.jsx)(e.li,{children:"Implement path planning algorithms that account for biped locomotion kinematics"}),"\n",(0,t.jsx)(e.li,{children:"Design navigation behaviors that maintain humanoid robot balance and stability"}),"\n",(0,t.jsx)(e.li,{children:"Integrate perception data from Isaac ROS with Nav2 for autonomous navigation"}),"\n",(0,t.jsx)(e.li,{children:"Optimize navigation parameters for humanoid robot dynamics and safety"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"Navigation in Nav2 (Navigation 2) is the standard navigation stack for mobile robots in ROS 2, providing path planning, obstacle avoidance, and localization capabilities. For humanoid robots, Nav2 requires specialized configuration to account for the unique challenges of bipedal locomotion, balance constraints, and complex kinematics. Unlike wheeled robots that can move in any direction with simple differential or holonomic drives, humanoid robots must navigate while maintaining balance, considering their multi-link structure, joint limits, and the need for stable foot placement."}),"\n",(0,t.jsx)(e.p,{children:"This lesson explores how to adapt Nav2 for humanoid robot navigation, addressing the specific requirements of bipedal locomotion, balance maintenance, and the integration with perception systems learned in previous lessons. We'll cover the configuration of Nav2 for humanoid-specific constraints, path planning algorithms that consider humanoid kinematics, and safety considerations for autonomous humanoid navigation."}),"\n",(0,t.jsx)(e.h2,{id:"nav2-architecture-for-humanoid-robots",children:"Nav2 Architecture for Humanoid Robots"}),"\n",(0,t.jsx)(e.p,{children:"The Nav2 stack consists of several key components that must be adapted for humanoid robot navigation:"}),"\n",(0,t.jsx)(e.h3,{id:"standard-nav2-components",children:"Standard Nav2 Components"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-mermaid",children:"graph TB\n    A[Nav2 Stack] --\x3e B[Navigation Server]\n    A --\x3e C[Local Planner]\n    A --\x3e D[Global Planner]\n    A --\x3e E[Controller Server]\n    A --\x3e F[Recovery Server]\n\n    B --\x3e G[Action Interface]\n    C --\x3e H[Trajectory Rollout]\n    D --\x3e I[Path Planning]\n    E --\x3e J[Path Following]\n    F --\x3e K[Recovery Behaviors]\n\n    G --\x3e L[Navigation Actions]\n    H --\x3e M[Costmap Generation]\n    I --\x3e N[Map Representation]\n    J --\x3e O[Velocity Control]\n    K --\x3e P[Spin, Back Up, Wait]\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:"Figure 1: Standard Nav2 architecture showing the main components and their relationships."})}),"\n",(0,t.jsx)(e.h3,{id:"humanoid-specific-modifications",children:"Humanoid-Specific Modifications"}),"\n",(0,t.jsx)(e.p,{children:"For humanoid robots, several Nav2 components require modification:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Local Planner"}),": Must consider biped kinematics and balance constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Global Planner"}),": Should account for terrain traversability and foot placement"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Controller"}),": Needs to generate footstep plans rather than simple velocity commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Costmap"}),": Should include balance and stability considerations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Recovery Behaviors"}),": Must be safe for humanoid robots with balance constraints"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"humanoid-specific-navigation-configuration",children:"Humanoid-Specific Navigation Configuration"}),"\n",(0,t.jsx)(e.h3,{id:"costmap-configuration-for-humanoids",children:"Costmap Configuration for Humanoids"}),"\n",(0,t.jsx)(e.p,{children:"The costmap in Nav2 needs to account for humanoid-specific requirements:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'# costmap_common_params_humanoid.yaml\nglobal_costmap:\n  global_frame: map\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  transform_tolerance: 0.5\n  resolution: 0.05  # Higher resolution for precise foot placement\n\n  plugins:\n    - {name: static_layer, type: "nav2_costmap_2d::StaticLayer"}\n    - {name: obstacle_layer, type: "nav2_costmap_2d::ObstacleLayer"}\n    - {name: inflation_layer, type: "nav2_costmap_2d::InflationLayer"}\n\n  static_layer:\n    map_subscribe_transient_local: true\n\n  obstacle_layer:\n    enabled: true\n    observation_sources: scan\n    scan:\n      topic: /scan\n      max_obstacle_height: 2.0  # Humanoid height consideration\n      clearing: true\n      marking: true\n      data_type: "LaserScan"\n      inf_is_valid: true\n\n  inflation_layer:\n    enabled: true\n    cost_scaling_factor: 3.0  # Higher inflation for humanoid safety\n    inflation_radius: 0.8     # Larger safety margin for biped locomotion\n    inflate_to_robot_footprint: false  # Consider full body dynamics\n\n  robot_radius: 0.3  # Consider humanoid width for collision avoidance\n\nlocal_costmap:\n  global_frame: odom\n  robot_base_frame: base_link\n  update_frequency: 10.0\n  publish_frequency: 5.0\n  resolution: 0.025  # Even higher resolution for local planning\n  width: 6.0\n  height: 6.0\n  origin_x: -3.0\n  origin_y: -3.0\n  rolling_window: true\n\n  plugins:\n    - {name: static_layer, type: "nav2_costmap_2d::StaticLayer"}\n    - {name: obstacle_layer, type: "nav2_costmap_2d::ObstacleLayer"}\n    - {name: inflation_layer, type: "nav2_costmap_2d::InflationLayer"}\n\n  inflation_layer:\n    enabled: true\n    cost_scaling_factor: 5.0  # Higher scaling for local safety\n    inflation_radius: 0.5\n'})}),"\n",(0,t.jsx)(e.h3,{id:"global-planner-configuration",children:"Global Planner Configuration"}),"\n",(0,t.jsx)(e.p,{children:"The global planner needs to account for humanoid-specific path requirements:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'# global_planner_params_humanoid.yaml\nbt_navigator:\n  ros__parameters:\n    use_sim_time: false\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: true\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    # Specify the behavior tree XML to use\n    default_nav_through_poses_bt_xml: "navigate_w_replanning_and_recovery.xml"\n    default_nav_to_pose_bt_xml: "navigate_w_replanning_and_recovery.xml"\n\n    # Recovery nodes\n    recovery_plugins: ["spin", "backup", "wait"]\n    spin:\n      plugin: "nav2_recoveries/Spin"\n    backup:\n      plugin: "nav2_recoveries/BackUp"\n    wait:\n      plugin: "nav2_recoveries/Wait"\n\n    # Navigator parameters\n    navigate_to_pose:\n      plugin: "nav2_navfn_planner/NavfnPlanner"\n      tolerance: 0.5  # Larger tolerance for humanoid navigation\n      use_astar: false\n      allow_unknown: true\n\nglobal_costmap_client:\n  ros__parameters:\n    use_sim_time: false\n\nglobal_costmap_rclcpp_node:\n  ros__parameters:\n    use_sim_time: false\n'})}),"\n",(0,t.jsx)(e.h2,{id:"humanoid-path-planning-algorithms",children:"Humanoid Path Planning Algorithms"}),"\n",(0,t.jsx)(e.h3,{id:"footstep-planning-for-biped-locomotion",children:"Footstep Planning for Biped Locomotion"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid navigation requires specialized path planning that considers footstep sequences:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom nav_msgs.msg import Path\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport numpy as np\nfrom scipy.spatial import KDTree\nimport math\n\nclass FootstepPlanner(Node):\n    def __init__(self):\n        super().__init__(\'footstep_planner\')\n\n        # Publishers for path and footsteps\n        self.path_pub = self.create_publisher(Path, \'/humanoid_global_plan\', 10)\n        self.footsteps_pub = self.create_publisher(MarkerArray, \'/footsteps_visualization\', 10)\n\n        # Footstep parameters\n        self.step_length = 0.3  # Maximum step length for humanoid\n        self.step_width = 0.2   # Lateral step distance\n        self.max_turn = math.radians(30)  # Maximum turning angle per step\n\n    def plan_footsteps(self, start_pose, goal_pose):\n        """Plan a sequence of footsteps for humanoid navigation"""\n        # Calculate straight-line path\n        dx = goal_pose.position.x - start_pose.position.x\n        dy = goal_pose.position.y - start_pose.position.y\n        distance = math.sqrt(dx*dx + dy*dy)\n        angle = math.atan2(dy, dx)\n\n        # Generate footsteps along the path\n        footsteps = []\n        current_x = start_pose.position.x\n        current_y = start_pose.position.y\n        current_yaw = self.quaternion_to_yaw(start_pose.orientation)\n\n        step_count = int(distance / self.step_length) + 1\n\n        for i in range(step_count):\n            # Calculate next step position\n            step_x = current_x + min(self.step_length, distance - i * self.step_length) * math.cos(angle)\n            step_y = current_y + min(self.step_length, distance - i * self.step_length) * math.sin(angle)\n\n            # Alternate between left and right foot\n            if i % 2 == 0:\n                # Left foot step\n                foot_offset_x = 0.0\n                foot_offset_y = self.step_width / 2.0\n            else:\n                # Right foot step\n                foot_offset_x = 0.0\n                foot_offset_y = -self.step_width / 2.0\n\n            # Apply offset in robot\'s frame\n            rotated_offset_x = foot_offset_x * math.cos(current_yaw) - foot_offset_y * math.sin(current_yaw)\n            rotated_offset_y = foot_offset_x * math.sin(current_yaw) + foot_offset_y * math.cos(current_yaw)\n\n            foot_pose = PoseStamped()\n            foot_pose.header.frame_id = \'map\'\n            foot_pose.pose.position.x = step_x + rotated_offset_x\n            foot_pose.pose.position.y = step_y + rotated_offset_y\n            foot_pose.pose.position.z = 0.0  # Ground level\n\n            # Set orientation to match path direction\n            foot_pose.pose.orientation = self.yaw_to_quaternion(angle)\n\n            footsteps.append(foot_pose)\n\n            # Update current position\n            current_x = step_x\n            current_y = step_y\n            current_yaw = angle\n\n        return footsteps\n\n    def quaternion_to_yaw(self, orientation):\n        """Convert quaternion to yaw angle"""\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def yaw_to_quaternion(self, yaw):\n        """Convert yaw angle to quaternion"""\n        from geometry_msgs.msg import Quaternion\n        q = Quaternion()\n        q.z = math.sin(yaw / 2.0)\n        q.w = math.cos(yaw / 2.0)\n        return q\n\n    def create_path_from_footsteps(self, footsteps):\n        """Create a Path message from footsteps for visualization"""\n        path = Path()\n        path.header.frame_id = \'map\'\n\n        for footstep in footsteps:\n            path.poses.append(footstep)\n\n        return path\n\n    def visualize_footsteps(self, footsteps):\n        """Create visualization markers for footsteps"""\n        marker_array = MarkerArray()\n\n        for i, footstep in enumerate(footsteps):\n            # Create marker for each footstep\n            marker = Marker()\n            marker.header.frame_id = \'map\'\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = \'footsteps\'\n            marker.id = i\n            marker.type = Marker.CYLINDER\n            marker.action = Marker.ADD\n\n            marker.pose = footstep.pose\n            marker.pose.position.z = 0.02  # Slightly above ground\n\n            # Size based on foot size\n            marker.scale.x = 0.15  # Foot length\n            marker.scale.y = 0.1   # Foot width\n            marker.scale.z = 0.01  # Height\n\n            # Color based on foot (left=blue, right=red)\n            if i % 2 == 0:\n                marker.color.r = 0.0\n                marker.color.g = 0.0\n                marker.color.b = 1.0  # Blue for left foot\n            else:\n                marker.color.r = 1.0\n                marker.color.g = 0.0\n                marker.color.b = 0.0  # Red for right foot\n\n            marker.color.a = 0.7\n\n            marker_array.markers.append(marker)\n\n        return marker_array\n'})}),"\n",(0,t.jsx)(e.h3,{id:"balance-aware-path-planning",children:"Balance-Aware Path Planning"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots must maintain balance during navigation, which affects path planning:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class BalanceAwarePlanner(Node):\n    def __init__(self):\n        super().__init__(\'balance_aware_planner\')\n\n        # Balance constraints\n        self.max_lean_angle = math.radians(15)  # Maximum lean angle\n        self.com_height = 0.8  # Center of mass height\n        self.foot_support_polygon = self.calculate_support_polygon()\n\n    def calculate_support_polygon(self):\n        """Calculate the support polygon for biped stability"""\n        # For a biped, the support polygon is the convex hull of both feet\n        # This is a simplified version - real implementation would be more complex\n        support_points = [\n            Point(x=0.1, y=0.1, z=0),   # Left foot front\n            Point(x=0.1, y=-0.1, z=0),  # Left foot back\n            Point(x=0.1, y=0.1, z=0),   # Right foot front\n            Point(x=0.1, y=-0.1, z=0)   # Right foot back\n        ]\n        return support_points\n\n    def check_balance_feasibility(self, path, robot_state):\n        """Check if a path is feasible given balance constraints"""\n        for pose in path.poses:\n            # Calculate center of mass projection\n            com_proj = self.project_com_to_ground(robot_state, pose)\n\n            # Check if COM projection is within support polygon\n            if not self.is_point_in_support_polygon(com_proj):\n                return False, "Path violates balance constraints"\n\n            # Check turning rate limits\n            if self.calculate_turn_rate(pose, robot_state) > self.max_turn_rate:\n                return False, "Turning rate exceeds humanoid capabilities"\n\n        return True, "Path is balance-feasible"\n\n    def project_com_to_ground(self, robot_state, target_pose):\n        """Project center of mass to ground plane"""\n        # Simplified COM projection\n        com_proj = Point()\n        com_proj.x = target_pose.pose.position.x\n        com_proj.y = target_pose.pose.position.y\n        com_proj.z = 0.0  # Ground level\n        return com_proj\n\n    def is_point_in_support_polygon(self, point):\n        """Check if a point is within the support polygon"""\n        # This would implement a proper point-in-polygon algorithm\n        # For now, return True as a placeholder\n        return True\n'})}),"\n",(0,t.jsx)(e.h2,{id:"humanoid-navigation-controller",children:"Humanoid Navigation Controller"}),"\n",(0,t.jsx)(e.p,{children:"The navigation controller for humanoid robots needs to generate footstep plans rather than simple velocity commands:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import JointState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport math\n\nclass HumanoidNavigationController(Node):\n    def __init__(self):\n        super().__init__('humanoid_navigation_controller')\n\n        # Publishers for navigation commands\n        self.trajectory_pub = self.create_publisher(\n            JointTrajectory, '/joint_trajectory_controller/joint_trajectory', 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10)\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10)\n\n        # Navigation parameters\n        self.linear_vel = 0.3  # m/s\n        self.angular_vel = 0.5  # rad/s\n        self.step_height = 0.05  # Foot lift height\n        self.step_duration = 0.8  # Time per step\n\n        # Robot state\n        self.current_pose = None\n        self.current_joint_positions = {}\n        self.target_pose = None\n        self.navigation_active = False\n\n        # Timer for control loop\n        self.control_timer = self.create_timer(0.1, self.control_loop)\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose from odometry\"\"\"\n        self.current_pose = msg.pose.pose\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update joint positions\"\"\"\n        for name, pos in zip(msg.name, msg.position):\n            self.current_joint_positions[name] = pos\n\n    def set_navigation_target(self, target_pose):\n        \"\"\"Set navigation target\"\"\"\n        self.target_pose = target_pose\n        self.navigation_active = True\n\n    def control_loop(self):\n        \"\"\"Main control loop for humanoid navigation\"\"\"\n        if not self.navigation_active or self.target_pose is None:\n            return\n\n        if self.current_pose is None:\n            return\n\n        # Calculate distance and angle to target\n        dx = self.target_pose.position.x - self.current_pose.position.x\n        dy = self.target_pose.position.y - self.current_pose.position.y\n        distance = math.sqrt(dx*dx + dy*dy)\n        target_angle = math.atan2(dy, dx)\n\n        # Calculate current robot angle\n        current_angle = self.quaternion_to_yaw(self.current_pose.orientation)\n\n        # Check if reached target\n        if distance < 0.2:  # 20cm tolerance\n            self.navigation_active = False\n            self.stop_navigation()\n            return\n\n        # Generate footstep trajectory\n        self.execute_footstep_trajectory(target_angle, distance)\n\n    def execute_footstep_trajectory(self, target_angle, distance):\n        \"\"\"Execute a single footstep trajectory\"\"\"\n        # Calculate step direction\n        step_angle = target_angle  # Simplified - in reality, this would be more complex\n\n        # Create joint trajectory for step\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\n            'left_hip_yaw', 'left_hip_roll', 'left_hip_pitch',\n            'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_yaw', 'right_hip_roll', 'right_hip_pitch',\n            'right_knee', 'right_ankle_pitch', 'right_ankle_roll'\n        ]\n\n        # Generate trajectory points for the step\n        num_points = 20  # Number of intermediate points\n        for i in range(num_points + 1):\n            point = JointTrajectoryPoint()\n\n            # Calculate intermediate joint positions for the step\n            progress = i / num_points\n\n            # Simplified joint position calculation\n            # In reality, this would involve inverse kinematics\n            # and balance maintenance calculations\n            joint_positions = self.calculate_step_joints(progress, step_angle)\n\n            point.positions = joint_positions\n            point.time_from_start = Duration(\n                sec=0,\n                nanosec=int(progress * self.step_duration * 1e9)\n            )\n\n            # Add velocity and acceleration for smoother motion\n            if i > 0:\n                prev_positions = trajectory.points[-1].positions\n                dt = self.step_duration / num_points\n                velocities = [(pos - prev) / dt for pos, prev in zip(joint_positions, prev_positions)]\n                point.velocities = velocities\n\n            trajectory.points.append(point)\n\n        # Publish the trajectory\n        self.trajectory_pub.publish(trajectory)\n\n    def calculate_step_joints(self, progress, step_direction):\n        \"\"\"Calculate joint positions for a step at given progress\"\"\"\n        # This is a simplified implementation\n        # Real implementation would use inverse kinematics and\n        # consider balance, step dynamics, and robot kinematics\n\n        # Base joint positions (standing pose)\n        joints = [0.0] * 12  # 12 joints for left and right legs\n\n        # Apply step motion based on progress\n        if progress < 0.3:  # Lift phase\n            # Lift swing foot\n            if progress / 0.3 < 0.5:  # First half of lift\n                joints[3] = -0.3 * (progress / 0.3) * 2  # Left knee\n                joints[9] = -0.3 * (progress / 0.3) * 2  # Right knee\n            else:  # Second half of lift\n                joints[3] = -0.3 + 0.3 * ((progress / 0.3) - 0.5) * 2  # Left knee\n                joints[9] = -0.3 + 0.3 * ((progress / 0.3) - 0.5) * 2  # Right knee\n\n        elif progress > 0.7:  # Lower phase\n            # Lower swing foot\n            lower_progress = (progress - 0.7) / 0.3\n            joints[3] = 0.3 - 0.3 * lower_progress  # Left knee\n            joints[9] = 0.3 - 0.3 * lower_progress  # Right knee\n\n        else:  # Swing phase\n            # Swing foot forward\n            joints[0] = step_direction * (progress - 0.3) / 0.4  # Left hip yaw\n            joints[6] = -step_direction * (progress - 0.3) / 0.4  # Right hip yaw\n\n        return joints\n\n    def stop_navigation(self):\n        \"\"\"Stop navigation and return to standing pose\"\"\"\n        # Publish zero joint positions to stop\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\n            'left_hip_yaw', 'left_hip_roll', 'left_hip_pitch',\n            'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_yaw', 'right_hip_roll', 'right_hip_pitch',\n            'right_knee', 'right_ankle_pitch', 'right_ankle_roll'\n        ]\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.0] * 12  # Return to zero position\n        point.time_from_start = Duration(sec=1, nanosec=0)\n\n        trajectory.points.append(point)\n        self.trajectory_pub.publish(trajectory)\n\n    def quaternion_to_yaw(self, orientation):\n        \"\"\"Convert quaternion to yaw angle\"\"\"\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"integration-with-perception-systems",children:"Integration with Perception Systems"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid navigation must integrate with perception systems for obstacle avoidance and safe navigation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom visualization_msgs.msg import MarkerArray\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Bool\nimport numpy as np\nfrom sensor_msgs_py import point_cloud2\nfrom scipy.spatial import KDTree\n\nclass HumanoidPerceptionIntegrator(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_perception_integrator\')\n\n        # Subscriptions for perception data\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, \'/points\', self.pointcloud_callback, 10)\n\n        # Publishers for navigation safety\n        self.safety_pub = self.create_publisher(Bool, \'/navigation_safety\', 10)\n        self.obstacle_viz_pub = self.create_publisher(MarkerArray, \'/obstacles\', 10)\n\n        # Perception processing parameters\n        self.safety_distance = 0.5  # Minimum safe distance\n        self.humanoid_width = 0.6   # Width of humanoid for collision checking\n        self.scan_tree = None\n\n    def scan_callback(self, msg):\n        """Process laser scan data for obstacle detection"""\n        # Convert scan to points\n        angles = np.arange(msg.angle_min, msg.angle_max, msg.angle_increment)\n        ranges = np.array(msg.ranges)\n\n        # Filter out invalid ranges\n        valid_mask = (ranges > msg.range_min) & (ranges < msg.range_max)\n        valid_angles = angles[valid_mask]\n        valid_ranges = ranges[valid_mask]\n\n        # Convert to Cartesian coordinates\n        x_points = valid_ranges * np.cos(valid_angles)\n        y_points = valid_ranges * np.sin(valid_angles)\n\n        # Create KDTree for fast nearest neighbor search\n        points = np.column_stack((x_points, y_points))\n        if len(points) > 0:\n            self.scan_tree = KDTree(points)\n\n        # Check for obstacles in path\n        self.check_path_safety()\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud data for 3D obstacle detection"""\n        # Extract points from point cloud\n        points_list = []\n        for point in point_cloud2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True):\n            points_list.append([point[0], point[1], point[2]])\n\n        if points_list:\n            self.pointcloud_points = np.array(points_list)\n\n    def check_path_safety(self):\n        """Check if the planned path is safe from obstacles"""\n        if self.scan_tree is None:\n            return\n\n        # Check safety in front of robot (simplified)\n        # In reality, this would check along the entire planned path\n        safety_check_points = self.generate_safety_check_positions()\n\n        min_distance = float(\'inf\')\n        for point in safety_check_points:\n            if self.scan_tree is not None:\n                distance, _ = self.scan_tree.query([point[0], point[1]])\n                min_distance = min(min_distance, distance)\n\n        # Publish safety status\n        safety_msg = Bool()\n        safety_msg.data = min_distance > self.safety_distance\n        self.safety_pub.publish(safety_msg)\n\n        # Visualize obstacles if too close\n        if min_distance <= self.safety_distance:\n            self.visualize_obstacles()\n\n    def generate_safety_check_positions(self):\n        """Generate positions to check for obstacles along path"""\n        # Generate points along potential walking path\n        check_positions = []\n        for step in range(10):  # Check next 10 potential steps\n            step_distance = step * 0.3  # 30cm per step\n            for angle in [-0.2, 0, 0.2]:  # Check left, center, right\n                x = step_distance * math.cos(angle)\n                y = step_distance * math.sin(angle)\n                check_positions.append([x, y])\n\n        return check_positions\n\n    def visualize_obstacles(self):\n        """Visualize detected obstacles"""\n        marker_array = MarkerArray()\n\n        if self.scan_tree is not None:\n            # Create markers for each obstacle point\n            points = self.scan_tree.data\n            for i, point in enumerate(points):\n                if np.linalg.norm(point) < self.safety_distance:\n                    marker = Marker()\n                    marker.header.frame_id = \'base_link\'\n                    marker.header.stamp = self.get_clock().now().to_msg()\n                    marker.ns = \'obstacles\'\n                    marker.id = i\n                    marker.type = Marker.SPHERE\n                    marker.action = Marker.ADD\n\n                    marker.pose.position.x = point[0]\n                    marker.pose.position.y = point[1]\n                    marker.pose.position.z = 0.1  # Height above ground\n\n                    marker.scale.x = 0.1\n                    marker.scale.y = 0.1\n                    marker.scale.z = 0.2\n\n                    marker.color.r = 1.0\n                    marker.color.g = 0.0\n                    marker.color.b = 0.0\n                    marker.color.a = 0.8\n\n                    marker_array.markers.append(marker)\n\n        self.obstacle_viz_pub.publish(marker_array)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"safety-and-recovery-behaviors",children:"Safety and Recovery Behaviors"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots require specialized safety and recovery behaviors due to their balance constraints:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class HumanoidRecoveryBehaviors(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_recovery_behaviors\')\n\n        # Recovery behavior parameters\n        self.balance_threshold = 0.2  # Balance error threshold\n        self.max_recovery_attempts = 3\n        self.recovery_timeout = 10.0\n\n        # Publishers for recovery actions\n        self.balance_pub = self.create_publisher(Bool, \'/balance_control\', 10)\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/emergency_stop\', 10)\n\n        # Balance monitoring\n        self.balance_monitor = self.create_timer(0.1, self.check_balance)\n\n    def check_balance(self):\n        """Monitor robot balance and trigger recovery if needed"""\n        # In a real implementation, this would check IMU data,\n        # foot pressure sensors, or other balance indicators\n        balance_error = self.calculate_balance_error()\n\n        if balance_error > self.balance_threshold:\n            self.trigger_balance_recovery()\n\n    def calculate_balance_error(self):\n        """Calculate balance error from sensor data"""\n        # Simplified balance error calculation\n        # In reality, this would use IMU, joint encoders, and other sensors\n        return 0.0  # Placeholder\n\n    def trigger_balance_recovery(self):\n        """Trigger balance recovery behavior"""\n        self.get_logger().warn(\'Balance recovery triggered\')\n\n        # Enable balance control\n        balance_msg = Bool()\n        balance_msg.data = True\n        self.balance_pub.publish(balance_msg)\n\n        # Stop navigation\n        stop_msg = Bool()\n        stop_msg.data = True\n        self.emergency_stop_pub.publish(stop_msg)\n\n        # Wait for balance recovery\n        recovery_start = self.get_clock().now()\n        while (self.get_clock().now() - recovery_start).nanoseconds / 1e9 < self.recovery_timeout:\n            current_error = self.calculate_balance_error()\n            if current_error < self.balance_threshold * 0.5:\n                # Balance recovered\n                self.get_logger().info(\'Balance recovered\')\n                balance_msg.data = False\n                self.balance_pub.publish(balance_msg)\n                return\n\n        # If recovery failed, request human intervention\n        self.get_logger().error(\'Balance recovery failed - requesting assistance\')\n'})}),"\n",(0,t.jsx)(e.h2,{id:"navigation-launch-configuration",children:"Navigation Launch Configuration"}),"\n",(0,t.jsx)(e.p,{children:"Create a complete launch file for humanoid navigation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# humanoid_navigation.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    params_file = LaunchConfiguration('params_file')\n\n    # Costmap parameters\n    costmap_params = PathJoinSubstitution([\n        get_package_share_directory('humanoid_navigation'),\n        'config',\n        'costmap_common_params_humanoid.yaml'\n    ])\n\n    # Global planner parameters\n    planner_params = PathJoinSubstitution([\n        get_package_share_directory('humanoid_navigation'),\n        'config',\n        'global_planner_params_humanoid.yaml'\n    ])\n\n    # Navigation server\n    navigation_server = Node(\n        package='nav2_navigation',\n        executable='navigation_server',\n        name='navigation_server',\n        parameters=[\n            costmap_params,\n            planner_params,\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Local planner\n    local_planner = Node(\n        package='nav2_navigation',\n        executable='local_planner',\n        name='local_planner',\n        parameters=[\n            costmap_params,\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Footstep planner\n    footstep_planner = Node(\n        package='humanoid_navigation',\n        executable='footstep_planner',\n        name='footstep_planner',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Humanoid controller\n    humanoid_controller = Node(\n        package='humanoid_navigation',\n        executable='humanoid_navigation_controller',\n        name='humanoid_controller',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Perception integrator\n    perception_integrator = Node(\n        package='humanoid_navigation',\n        executable='perception_integrator',\n        name='perception_integrator',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Recovery behaviors\n    recovery_behaviors = Node(\n        package='humanoid_navigation',\n        executable='recovery_behaviors',\n        name='recovery_behaviors',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        navigation_server,\n        local_planner,\n        footstep_planner,\n        humanoid_controller,\n        perception_integrator,\n        recovery_behaviors\n    ])\n"})}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercise-33-configure-nav2-for-humanoid-navigation",children:"Hands-on Exercise 3.3: Configure Nav2 for Humanoid Navigation"}),"\n",(0,t.jsx)(e.p,{children:"Create a complete Nav2 configuration for humanoid robot navigation:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create configuration files"}),":"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Create configuration directory\nmkdir -p ~/ros2_ws/src/humanoid_navigation/config\n\n# Create costmap configuration (as shown in previous examples)\n# Create planner configuration (as shown in previous examples)\n"})}),"\n",(0,t.jsxs)(e.ol,{start:"2",children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Implement the navigation nodes"}),":"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Create the Python files for each component:\n# - footstep_planner.py\n# - humanoid_controller.py\n# - perception_integrator.py\n# - recovery_behaviors.py\n"})}),"\n",(0,t.jsxs)(e.ol,{start:"3",children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Test the navigation system"}),":"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:'# Terminal 1: Launch navigation stack\nros2 launch humanoid_navigation.launch.py\n\n# Terminal 2: Send navigation goal\nros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose "{pose: {position: {x: 5.0, y: 5.0, z: 0.0}, orientation: {z: 0.0, w: 1.0}}}"\n\n# Terminal 3: Visualize in RViz\nrviz2\n'})}),"\n",(0,t.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Humanoid navigation"})," requires specialized path planning that considers biped kinematics and balance constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Footstep planning"})," is essential for humanoid locomotion, unlike simple velocity commands for wheeled robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Balance awareness"})," must be integrated into path planning and execution for safe humanoid navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception integration"})," enables obstacle avoidance while maintaining humanoid-specific constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety and recovery"})," behaviors are critical for humanoid robots with balance limitations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"How does footstep planning differ from traditional path planning for wheeled robots?"}),"\n",(0,t.jsx)(e.li,{children:"What balance constraints must be considered when planning paths for humanoid robots?"}),"\n",(0,t.jsx)(e.li,{children:"How can Nav2 be modified to account for humanoid-specific kinematic constraints?"}),"\n",(0,t.jsx)(e.li,{children:"What safety measures are essential for autonomous humanoid navigation in human environments?"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,t.jsxs)(e.p,{children:["Kuffner, J., & LaValle, S. M. (2000). RRT-connect: An efficient approach to single-query path planning. ",(0,t.jsx)(e.em,{children:"Proceedings of the IEEE International Conference on Robotics and Automation"}),", 995-1001. ",(0,t.jsx)(e.a,{href:"https://doi.org/10.1109/ROBOT.2000.844730",children:"https://doi.org/10.1109/ROBOT.2000.844730"})]}),"\n",(0,t.jsxs)(e.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,t.jsx)(e.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,t.jsxs)(e.p,{children:["Englsberger, J., Ott, C., & Dietrich, A. (2014). Three-dimensional bipedal walking control using Divergent Component of Motion. ",(0,t.jsx)(e.em,{children:"Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),", 895-902. ",(0,t.jsx)(e.a,{href:"https://doi.org/10.1109/IROS.2014.6942666",children:"https://doi.org/10.1109/IROS.2014.6942666"})]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This lesson covered Nav2 path planning specifically designed for humanoid robots, addressing the unique challenges of bipedal locomotion, balance constraints, and humanoid-specific navigation requirements. We explored specialized configurations, footstep planning algorithms, balance-aware path planning, and integration with perception systems. The combination of these elements enables safe and effective navigation for humanoid robots in complex environments."}),"\n",(0,t.jsx)(e.p,{children:"In the next module, we'll explore Vision-Language-Action (VLA) systems that integrate perception, language understanding, and physical action for advanced humanoid robot capabilities."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>r,x:()=>s});var a=o(6540);const t={},i=a.createContext(t);function r(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);