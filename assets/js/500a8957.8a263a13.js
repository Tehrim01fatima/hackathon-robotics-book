"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[128],{641:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/lesson-3-capstone-full-digital-humanoid-agent","title":"Lesson 3 - Capstone - Full Digital Humanoid Agent - Complete System Integration","description":"Learning Objectives","source":"@site/docs/module-4/lesson-3-capstone-full-digital-humanoid-agent.md","sourceDirName":"module-4","slug":"/module-4/lesson-3-capstone-full-digital-humanoid-agent","permalink":"/hackathon-robotics-book/docs/module-4/lesson-3-capstone-full-digital-humanoid-agent","draft":false,"unlisted":false,"editUrl":"https://github.com/Tehrim01fatima/hackathon-robotics-book/edit/main/my-website/docs/module-4/lesson-3-capstone-full-digital-humanoid-agent.md","tags":[],"version":"current","frontMatter":{"title":"Lesson 3 - Capstone - Full Digital Humanoid Agent - Complete System Integration","sidebar_label":"Capstone - Full Digital Humanoid Agent"},"sidebar":"textbookSidebar","previous":{"title":"LLM Cognitive Planning","permalink":"/hackathon-robotics-book/docs/module-4/lesson-2-llm-cognitive-planning"},"next":{"title":"Glossary","permalink":"/hackathon-robotics-book/docs/endmatter/glossary"}}');var o=t(4848),a=t(8453);const r={title:"Lesson 3 - Capstone - Full Digital Humanoid Agent - Complete System Integration",sidebar_label:"Capstone - Full Digital Humanoid Agent"},s="Lesson 3: Capstone - Full Digital Humanoid Agent - Complete System Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:2},{value:"System Integration Components",id:"system-integration-components",level:2},{value:"Main Integration Node",id:"main-integration-node",level:3},{value:"Voice-to-Action Pipeline Integration",id:"voice-to-action-pipeline-integration",level:2},{value:"System Deployment and Configuration",id:"system-deployment-and-configuration",level:2},{value:"Complete Launch File",id:"complete-launch-file",level:3},{value:"Performance Optimization and Monitoring",id:"performance-optimization-and-monitoring",level:2},{value:"System Performance Monitor",id:"system-performance-monitor",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Integration Test Suite",id:"integration-test-suite",level:3},{value:"Deployment Scenarios",id:"deployment-scenarios",level:2},{value:"Simulation Deployment",id:"simulation-deployment",level:3},{value:"Real Robot Deployment",id:"real-robot-deployment",level:3},{value:"Hands-on Exercise 4.3: Deploy the Complete Digital Humanoid Agent",id:"hands-on-exercise-43-deploy-the-complete-digital-humanoid-agent",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2},{value:"APA Citations",id:"apa-citations",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-3-capstone---full-digital-humanoid-agent---complete-system-integration",children:"Lesson 3: Capstone - Full Digital Humanoid Agent - Complete System Integration"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate all components from previous modules into a complete digital humanoid agent"}),"\n",(0,o.jsx)(n.li,{children:"Implement a voice-to-action pipeline connecting Whisper, LLM planning, and robot execution"}),"\n",(0,o.jsx)(n.li,{children:"Design and implement a comprehensive system architecture for the integrated agent"}),"\n",(0,o.jsx)(n.li,{children:"Deploy and test the complete digital humanoid agent in simulation and real environments"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the performance and capabilities of the integrated system"}),"\n",(0,o.jsx)(n.li,{children:"Troubleshoot and optimize the complete system for reliable operation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"This capstone lesson brings together all the concepts learned throughout this textbook into a complete digital humanoid agent that can understand natural language instructions, plan complex behaviors, and execute them in the physical world. The digital humanoid agent represents the ultimate goal of Physical AI & Humanoid Robotics: a system that can perceive its environment, understand human commands in natural language, reason about how to achieve goals, and execute complex physical tasks."}),"\n",(0,o.jsx)(n.p,{children:"The integration of Vision-Language-Action (VLA) systems creates a truly intelligent agent capable of natural human-robot interaction. This lesson focuses on the complete system integration, connecting perception systems from Module 3, navigation capabilities from Module 2, communication frameworks from Module 1, and the voice processing and cognitive planning systems from this module."}),"\n",(0,o.jsx)(n.h2,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The full digital humanoid agent integrates all components from the previous modules into a cohesive system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "User Interface"\n        A[Human User] --\x3e B{Voice Command}\n    end\n\n    subgraph "Perception Layer"\n        C[Cameras] --\x3e D[Isaac ROS Perception]\n        E[LiDAR] --\x3e D\n        F[IMU] --\x3e G[Sensor Fusion]\n        G --\x3e H[Environment Understanding]\n    end\n\n    subgraph "Voice Processing Layer"\n        B --\x3e I[Whisper ASR]\n        I --\x3e J[Natural Language Understanding]\n    end\n\n    subgraph "Cognitive Layer"\n        J --\x3e K[LLM Cognitive Planner]\n        K --\x3e L[Action Graph Generator]\n        L --\x3e M[Task Decomposition]\n    end\n\n    subgraph "Navigation Layer"\n        M --\x3e N[Nav2 Path Planning]\n        N --\x3e O[Humanoid Controller]\n        O --\x3e P[Balance Control]\n    end\n\n    subgraph "Execution Layer"\n        D --\x3e Q[Object Recognition]\n        Q --\x3e R[Manipulation Planning]\n        R --\x3e S[ROS 2 Action Servers]\n        S --\x3e T[Robot Hardware]\n    end\n\n    subgraph "Communication Layer"\n        U[ROS 2 Middleware]\n    end\n\n    H --\x3e U\n    P --\x3e U\n    T --\x3e U\n    U --\x3e V[System Status]\n    V --\x3e W[RViz Visualization]\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Figure 1: Complete digital humanoid agent architecture showing all integrated components."})}),"\n",(0,o.jsx)(n.h2,{id:"system-integration-components",children:"System Integration Components"}),"\n",(0,o.jsx)(n.h3,{id:"main-integration-node",children:"Main Integration Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nimport threading\nimport time\nimport json\nfrom typing import Dict, Any, Optional\n\nclass DigitalHumanoidAgent(Node):\n    def __init__(self):\n        super().__init__('digital_humanoid_agent')\n\n        # Initialize component managers\n        self.voice_processor = None\n        self.cognitive_planner = None\n        self.perception_manager = None\n        self.navigation_manager = None\n\n        # System state\n        self.system_ready = False\n        self.current_task = None\n        self.robot_pose = None\n        self.environment_map = None\n\n        # Callback group for multi-threading\n        self.callback_group = ReentrantCallbackGroup()\n\n        # Publishers and subscribers\n        self.status_pub = self.create_publisher(String, '/system_status', 10)\n        self.command_pub = self.create_publisher(String, '/system_command', 10)\n        self.voice_sub = self.create_subscription(\n            String, '/voice_command', self.voice_callback, 10, callback_group=self.callback_group)\n        self.odom_sub = self.create_subscription(\n            String, '/robot_pose', self.odom_callback, 10, callback_group=self.callback_group)\n        self.perception_sub = self.create_subscription(\n            String, '/perception_output', self.perception_callback, 10, callback_group=self.callback_group)\n\n        # Action clients for different capabilities\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.manipulation_client = ActionClient(self, ManipulateObject, 'manipulate_object')\n        self.perception_client = ActionClient(self, DetectObjects, 'detect_objects')\n\n        # Initialize components\n        self.initialize_components()\n\n        # Timer for system monitoring\n        self.system_timer = self.create_timer(1.0, self.system_monitor, callback_group=self.callback_group)\n\n        self.get_logger().info('Digital Humanoid Agent initialized')\n\n    def initialize_components(self):\n        \"\"\"Initialize all system components\"\"\"\n        try:\n            # Initialize voice processing\n            from voice_processor import WhisperVoiceProcessor\n            self.voice_processor = WhisperVoiceProcessor()\n\n            # Initialize cognitive planning\n            from cognitive_planner import CognitivePlannerNode\n            self.cognitive_planner = CognitivePlannerNode()\n\n            # Initialize perception manager\n            from perception_manager import PerceptionManager\n            self.perception_manager = PerceptionManager()\n\n            # Initialize navigation manager\n            from navigation_manager import NavigationManager\n            self.navigation_manager = NavigationManager()\n\n            self.system_ready = True\n            self.status_pub.publish(String(data='system_initialized'))\n            self.get_logger().info('All components initialized successfully')\n\n        except Exception as e:\n            self.get_logger().error(f'Error initializing components: {e}')\n            self.status_pub.publish(String(data='initialization_failed'))\n\n    def voice_callback(self, msg):\n        \"\"\"Handle voice commands from the voice processing system\"\"\"\n        if not self.system_ready:\n            self.get_logger().warn('System not ready to process voice commands')\n            return\n\n        command = msg.data\n        self.get_logger().info(f'Received voice command: {command}')\n\n        # Publish to cognitive planner\n        if self.cognitive_planner:\n            # In a real implementation, this would call the planner directly\n            # or publish to a topic that the planner subscribes to\n            self.process_voice_command(command)\n\n    def process_voice_command(self, command: str):\n        \"\"\"Process a voice command through the cognitive system\"\"\"\n        self.get_logger().info(f'Processing command: {command}')\n        self.status_pub.publish(String(data=f'processing_command: {command}'))\n\n        # Generate action plan using LLM\n        try:\n            action_graph = self.cognitive_planner.generate_action_graph(command)\n\n            if action_graph and len(action_graph.nodes) > 0:\n                self.get_logger().info(f'Generated action graph with {len(action_graph.nodes)} actions')\n\n                # Execute the action graph\n                self.execute_action_graph(action_graph)\n            else:\n                self.get_logger().error('Failed to generate valid action graph')\n                self.status_pub.publish(String(data='command_failed: invalid_action_graph'))\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n            self.status_pub.publish(String(data=f'command_failed: {str(e)}'))\n\n    def execute_action_graph(self, action_graph):\n        \"\"\"Execute an action graph\"\"\"\n        self.get_logger().info('Starting action graph execution')\n        self.status_pub.publish(String(data='executing_action_graph'))\n\n        # Execute actions in dependency order\n        execution_thread = threading.Thread(\n            target=self._execute_graph_thread,\n            args=(action_graph,)\n        )\n        execution_thread.daemon = True\n        execution_thread.start()\n\n    def _execute_graph_thread(self, action_graph):\n        \"\"\"Execute action graph in a separate thread\"\"\"\n        completed_actions = []\n\n        while not action_graph.is_complete():\n            # Get ready actions\n            ready_actions = action_graph.get_ready_actions(completed_actions)\n\n            if not ready_actions:\n                time.sleep(0.1)\n                continue\n\n            # Execute ready actions\n            for action_node in ready_actions:\n                if action_node.status == ActionStatus.PENDING:\n                    success = self.execute_action_node(action_node)\n\n                    if success:\n                        action_graph.update_node_status(action_node.id, ActionStatus.SUCCESS)\n                        completed_actions.append(action_node.id)\n                        self.get_logger().info(f'Action {action_node.id} completed successfully')\n                    else:\n                        action_graph.update_node_status(action_node.id, ActionStatus.FAILED)\n                        self.get_logger().error(f'Action {action_node.id} failed')\n                        break  # Stop execution on failure\n\n            time.sleep(0.1)\n\n        self.get_logger().info('Action graph execution completed')\n        self.status_pub.publish(String(data='action_graph_completed'))\n\n    def execute_action_node(self, action_node):\n        \"\"\"Execute a single action node\"\"\"\n        self.get_logger().info(f'Executing action: {action_node.name} ({action_node.action_type})')\n\n        try:\n            if action_node.action_type == 'navigation':\n                return self.execute_navigation_action(action_node)\n            elif action_node.action_type == 'manipulation':\n                return self.execute_manipulation_action(action_node)\n            elif action_node.action_type == 'perception':\n                return self.execute_perception_action(action_node)\n            elif action_node.action_type == 'communication':\n                return self.execute_communication_action(action_node)\n            else:\n                self.get_logger().error(f'Unknown action type: {action_node.action_type}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing action {action_node.id}: {e}')\n            return False\n\n    def execute_navigation_action(self, action_node):\n        \"\"\"Execute navigation action\"\"\"\n        location = action_node.parameters.get('location')\n        if not location:\n            self.get_logger().error('Navigation action missing location')\n            return False\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.pose.position.x = float(location.get('x', 0.0))\n        goal_msg.pose.pose.position.y = float(location.get('y', 0.0))\n        goal_msg.pose.pose.position.z = float(location.get('z', 0.0))\n\n        # Set orientation\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        # Wait for server and send goal\n        if self.nav_client.wait_for_server(timeout_sec=5.0):\n            future = self.nav_client.send_goal_async(goal_msg)\n            future.add_done_callback(lambda f: self.navigation_done_callback(f, action_node.id))\n            return True\n        else:\n            self.get_logger().error('Navigation action server not available')\n            return False\n\n    def execute_manipulation_action(self, action_node):\n        \"\"\"Execute manipulation action\"\"\"\n        # Implementation for manipulation actions\n        self.get_logger().info(f'Executing manipulation: {action_node.parameters}')\n        # In a real implementation, this would call manipulation services\n        return True\n\n    def execute_perception_action(self, action_node):\n        \"\"\"Execute perception action\"\"\"\n        # Implementation for perception actions\n        self.get_logger().info(f'Executing perception: {action_node.parameters}')\n        # In a real implementation, this would call perception services\n        return True\n\n    def execute_communication_action(self, action_node):\n        \"\"\"Execute communication action\"\"\"\n        # Implementation for communication actions\n        self.get_logger().info(f'Executing communication: {action_node.parameters}')\n        # In a real implementation, this might trigger speech synthesis\n        return True\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose\"\"\"\n        try:\n            pose_data = json.loads(msg.data)\n            self.robot_pose = pose_data\n        except json.JSONDecodeError:\n            self.get_logger().error('Error parsing pose data')\n\n    def perception_callback(self, msg):\n        \"\"\"Update environment understanding\"\"\"\n        try:\n            perception_data = json.loads(msg.data)\n            self.environment_map = perception_data\n        except json.JSONDecodeError:\n            self.get_logger().error('Error parsing perception data')\n\n    def system_monitor(self):\n        \"\"\"Monitor system status and health\"\"\"\n        if self.system_ready:\n            status_msg = String()\n            status_msg.data = f'system_operational:pose_known:{self.robot_pose is not None}'\n            self.status_pub.publish(status_msg)\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        if self.voice_processor:\n            self.voice_processor.destroy_node()\n        if self.cognitive_planner:\n            self.cognitive_planner.destroy_node()\n\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    agent = DigitalHumanoidAgent()\n\n    # Use multi-threaded executor to handle all callbacks\n    executor = MultiThreadedExecutor()\n    executor.add_node(agent)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        agent.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-pipeline-integration",children:"Voice-to-Action Pipeline Integration"}),"\n",(0,o.jsx)(n.p,{children:"The voice-to-action pipeline connects all VLA components in a seamless flow:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class VoiceToActionPipeline:\n    def __init__(self):\n        self.whisper_processor = WhisperVoiceProcessor()\n        self.llm_planner = LLMActionGraphGenerator(api_key="your-api-key")\n        self.action_executor = ActionGraphExecutor()\n\n    def process_voice_command(self, audio_input):\n        """Complete voice-to-action pipeline"""\n        # Step 1: Voice recognition\n        text_command = self.whisper_processor.transcribe(audio_input)\n\n        if not text_command:\n            return {"status": "failed", "reason": "voice_recognition_failed"}\n\n        # Step 2: Cognitive planning\n        action_graph = self.llm_planner.generate_action_graph(text_command)\n\n        if not action_graph or len(action_graph.nodes) == 0:\n            return {"status": "failed", "reason": "planning_failed"}\n\n        # Step 3: Action execution\n        execution_result = self.action_executor.execute_graph(action_graph)\n\n        return {\n            "status": "completed" if execution_result else "failed",\n            "command": text_command,\n            "action_graph": action_graph,\n            "execution_result": execution_result\n        }\n\nclass ActionGraphExecutor:\n    def __init__(self):\n        self.ros_node = None  # Will be set by the main agent\n\n    def execute_graph(self, action_graph):\n        """Execute an action graph with proper dependency management"""\n        completed_actions = []\n        max_attempts = 5  # Maximum attempts for each action\n        timeout_per_action = 60.0  # Timeout for each action in seconds\n\n        while not action_graph.is_complete():\n            ready_actions = action_graph.get_ready_actions(completed_actions)\n\n            if not ready_actions:\n                # Check if we\'re waiting for ongoing actions\n                time.sleep(0.1)\n                continue\n\n            # Execute ready actions\n            for action_node in ready_actions:\n                if action_node.status == ActionStatus.PENDING:\n                    success = self.execute_single_action(action_node, max_attempts, timeout_per_action)\n\n                    if success:\n                        action_graph.update_node_status(action_node.id, ActionStatus.SUCCESS)\n                        completed_actions.append(action_node.id)\n                    else:\n                        action_graph.update_node_status(action_node.id, ActionStatus.FAILED)\n                        return False  # Stop execution on failure\n\n        return True\n\n    def execute_single_action(self, action_node, max_attempts, timeout):\n        """Execute a single action with retry logic"""\n        attempts = 0\n\n        while attempts < max_attempts:\n            try:\n                # Execute the action based on its type\n                success = self._execute_action_by_type(action_node)\n\n                if success:\n                    return True\n                else:\n                    attempts += 1\n                    time.sleep(1.0)  # Wait before retry\n\n            except Exception as e:\n                self.get_logger().error(f\'Action execution error: {e}\')\n                attempts += 1\n                time.sleep(1.0)\n\n        return False\n\n    def _execute_action_by_type(self, action_node):\n        """Execute action based on its type"""\n        # This would route to appropriate ROS services/actions\n        # based on the action type and parameters\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"system-deployment-and-configuration",children:"System Deployment and Configuration"}),"\n",(0,o.jsx)(n.h3,{id:"complete-launch-file",children:"Complete Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# digital_humanoid_agent.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    whisper_model = LaunchConfiguration('whisper_model', default='small')\n    llm_api_key = LaunchConfiguration('llm_api_key')\n\n    # Include Isaac Sim launch if needed\n    isaac_sim_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            get_package_share_directory('isaac_ros_launch'),\n            '/isaac_ros_perceptor.launch.py'\n        ]),\n        launch_arguments={'use_sim_time': use_sim_time}.items()\n    )\n\n    # Digital humanoid agent main node\n    digital_agent = Node(\n        package='digital_humanoid_agent',\n        executable='digital_humanoid_agent',\n        name='digital_humanoid_agent',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'whisper_model': whisper_model}\n        ],\n        output='screen'\n    )\n\n    # Voice processing node\n    voice_processor = Node(\n        package='voice_processing',\n        executable='whisper_voice_processor',\n        name='voice_processor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Cognitive planning node\n    cognitive_planner = Node(\n        package='cognitive_planning',\n        executable='cognitive_planner',\n        name='cognitive_planner',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'llm_api_key': llm_api_key}\n        ],\n        output='screen'\n    )\n\n    # Perception processing node\n    perception_processor = Node(\n        package='perception_processing',\n        executable='perception_processor',\n        name='perception_processor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Navigation node\n    navigation_system = Node(\n        package='nav2_bringup',\n        executable='nav2_launch',\n        name='navigation_system',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # RViz for visualization\n    rviz = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz',\n        arguments=['-d', PathJoinSubstitution([\n            get_package_share_directory('digital_humanoid_agent'),\n            'rviz',\n            'digital_agent.rviz'\n        ])],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        # isaac_sim_launch,  # Uncomment if using Isaac Sim\n        digital_agent,\n        voice_processor,\n        cognitive_planner,\n        perception_processor,\n        navigation_system,\n        rviz\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization-and-monitoring",children:"Performance Optimization and Monitoring"}),"\n",(0,o.jsx)(n.h3,{id:"system-performance-monitor",children:"System Performance Monitor"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class SystemPerformanceMonitor:\n    def __init__(self, agent_node):\n        self.agent = agent_node\n        self.metrics = {\n            'voice_recognition_latency': [],\n            'planning_time': [],\n            'action_execution_time': [],\n            'system_uptime': 0,\n            'command_success_rate': 0.0\n        }\n        self.command_count = 0\n        self.successful_commands = 0\n\n    def start_monitoring(self):\n        \"\"\"Start system performance monitoring\"\"\"\n        self.monitor_timer = self.agent.create_timer(5.0, self.log_performance_metrics)\n\n    def record_voice_latency(self, latency):\n        \"\"\"Record voice recognition latency\"\"\"\n        self.metrics['voice_recognition_latency'].append(latency)\n        if len(self.metrics['voice_recognition_latency']) > 100:\n            self.metrics['voice_recognition_latency'].pop(0)\n\n    def record_planning_time(self, time_taken):\n        \"\"\"Record planning time\"\"\"\n        self.metrics['planning_time'].append(time_taken)\n        if len(self.metrics['planning_time']) > 100:\n            self.metrics['planning_time'].pop(0)\n\n    def record_action_time(self, action_id, time_taken):\n        \"\"\"Record action execution time\"\"\"\n        if 'action_times' not in self.metrics:\n            self.metrics['action_times'] = {}\n        self.metrics['action_times'][action_id] = time_taken\n\n    def record_command_outcome(self, success):\n        \"\"\"Record command outcome for success rate calculation\"\"\"\n        self.command_count += 1\n        if success:\n            self.successful_commands += 1\n        self.metrics['command_success_rate'] = self.successful_commands / max(self.command_count, 1)\n\n    def log_performance_metrics(self):\n        \"\"\"Log current performance metrics\"\"\"\n        if self.metrics['voice_recognition_latency']:\n            avg_latency = sum(self.metrics['voice_recognition_latency']) / len(self.metrics['voice_recognition_latency'])\n            self.agent.get_logger().info(f'Avg voice latency: {avg_latency:.3f}s')\n\n        if self.metrics['planning_time']:\n            avg_planning = sum(self.metrics['planning_time']) / len(self.metrics['planning_time'])\n            self.agent.get_logger().info(f'Avg planning time: {avg_planning:.3f}s')\n\n        self.agent.get_logger().info(f'Command success rate: {self.metrics[\"command_success_rate\"]:.2%}')\n        self.agent.get_logger().info(f'Total commands: {self.command_count}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,o.jsx)(n.p,{children:"Robust error handling is essential for a complete digital humanoid agent:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ErrorRecoveryManager:\n    def __init__(self, agent_node):\n        self.agent = agent_node\n        self.error_history = []\n        self.max_error_history = 50\n\n    def handle_error(self, error_type, error_message, context=None):\n        \"\"\"Handle system errors with appropriate recovery strategies\"\"\"\n        error_record = {\n            'timestamp': time.time(),\n            'type': error_type,\n            'message': error_message,\n            'context': context,\n            'handled': False\n        }\n\n        self.error_history.append(error_record)\n        if len(self.error_history) > self.max_error_history:\n            self.error_history.pop(0)\n\n        # Handle specific error types\n        if error_type == 'voice_recognition':\n            self.handle_voice_recognition_error(error_message)\n        elif error_type == 'planning_failure':\n            self.handle_planning_failure(error_message)\n        elif error_type == 'action_execution':\n            self.handle_action_execution_error(error_message, context)\n        elif error_type == 'perception_failure':\n            self.handle_perception_failure(error_message)\n        else:\n            self.handle_general_error(error_type, error_message)\n\n    def handle_voice_recognition_error(self, error_message):\n        \"\"\"Handle voice recognition errors\"\"\"\n        self.agent.get_logger().warn(f'Voice recognition error: {error_message}')\n        # Could trigger text-based input or request repetition\n        self.agent.status_pub.publish(String(data='request_voice_repeat'))\n\n    def handle_planning_failure(self, error_message):\n        \"\"\"Handle planning system failures\"\"\"\n        self.agent.get_logger().error(f'Planning failure: {error_message}')\n        # Could try alternative planning approaches or ask for clarification\n        self.agent.status_pub.publish(String(data='request_instruction_clarification'))\n\n    def handle_action_execution_error(self, error_message, context):\n        \"\"\"Handle action execution errors\"\"\"\n        self.agent.get_logger().error(f'Action execution error: {error_message}')\n\n        # Implement recovery based on action type\n        if context and 'action_type' in context:\n            action_type = context['action_type']\n            if action_type == 'navigation':\n                self.attempt_navigation_recovery(context)\n            elif action_type == 'manipulation':\n                self.attempt_manipulation_recovery(context)\n\n    def attempt_navigation_recovery(self, context):\n        \"\"\"Attempt recovery from navigation errors\"\"\"\n        # Try alternative path planning, obstacle avoidance, etc.\n        self.agent.get_logger().info('Attempting navigation recovery...')\n        # Implementation would include retry with different parameters\n        pass\n\n    def attempt_manipulation_recovery(self, context):\n        \"\"\"Attempt recovery from manipulation errors\"\"\"\n        # Try different grasp points, approach angles, etc.\n        self.agent.get_logger().info('Attempting manipulation recovery...')\n        # Implementation would include retry with different parameters\n        pass\n\n    def handle_perception_failure(self, error_message):\n        \"\"\"Handle perception system failures\"\"\"\n        self.agent.get_logger().error(f'Perception failure: {error_message}')\n        # Could request human assistance or move to better sensing position\n        self.agent.status_pub.publish(String(data='perception_degraded'))\n\n    def handle_general_error(self, error_type, error_message):\n        \"\"\"Handle general system errors\"\"\"\n        self.agent.get_logger().error(f'General error [{error_type}]: {error_message}')\n        # Could trigger safe shutdown or emergency procedures\n        pass\n\n    def get_system_health_report(self):\n        \"\"\"Generate system health report\"\"\"\n        recent_errors = [e for e in self.error_history if time.time() - e['timestamp'] < 300]  # Last 5 minutes\n        error_types = [e['type'] for e in recent_errors]\n\n        health_report = {\n            'error_count': len(recent_errors),\n            'error_types': list(set(error_types)),\n            'most_recent_error': recent_errors[-1] if recent_errors else None,\n            'system_stable': len(recent_errors) < 5  # Consider stable if fewer than 5 errors recently\n        }\n\n        return health_report\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsx)(n.h3,{id:"integration-test-suite",children:"Integration Test Suite"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import unittest\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport time\n\nclass TestDigitalHumanoidAgent(unittest.TestCase):\n    def setUp(self):\n        """Set up test environment"""\n        self.node = DigitalHumanoidAgent()\n        self.executor = rclpy.executors.SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def test_voice_command_processing(self):\n        """Test voice command processing pipeline"""\n        # Publish a voice command\n        test_command = String()\n        test_command.data = "Go to the kitchen and bring me a cup"\n\n        # In a real test, you would publish this to the appropriate topic\n        # and verify that the appropriate actions are generated\n\n        # Wait for processing\n        time.sleep(2.0)\n\n        # Verify that action graph was generated\n        # This would depend on the specific implementation details\n        self.assertTrue(True)  # Placeholder - implement actual test\n\n    def test_navigation_action(self):\n        """Test navigation action execution"""\n        # Test that navigation commands result in proper action execution\n        # This would involve checking that navigation goals are sent\n        # and that the system reports proper status\n\n        self.assertTrue(True)  # Placeholder - implement actual test\n\n    def test_perception_integration(self):\n        """Test perception system integration"""\n        # Test that perception data is properly integrated\n        # into the cognitive planning process\n\n        self.assertTrue(True)  # Placeholder - implement actual test\n\n    def test_error_recovery(self):\n        """Test error recovery mechanisms"""\n        # Test that the system properly handles and recovers from errors\n\n        self.assertTrue(True)  # Placeholder - implement actual test\n\ndef run_tests():\n    """Run the integration test suite"""\n    test_suite = unittest.TestSuite()\n    test_suite.addTest(unittest.makeSuite(TestDigitalHumanoidAgent))\n\n    runner = unittest.TextTestRunner(verbosity=2)\n    result = runner.run(test_suite)\n\n    return result.wasSuccessful()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"deployment-scenarios",children:"Deployment Scenarios"}),"\n",(0,o.jsx)(n.h3,{id:"simulation-deployment",children:"Simulation Deployment"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# docker-compose.simulation.yml\nversion: \'3.8\'\n\nservices:\n  digital-humanoid-agent:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - ROS_DOMAIN_ID=0\n      - NVIDIA_VISIBLE_DEVICES=all\n      - NVIDIA_DRIVER_CAPABILITIES=all\n    volumes:\n      - /tmp/.X11-unix:/tmp/.X11-unix:rw\n      - ./config:/app/config\n    devices:\n      - /dev/snd:/dev/snd\n    network_mode: host\n    command: ["ros2", "launch", "digital_humanoid_agent", "digital_humanoid_agent.launch.py"]\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-robot-deployment",children:"Real Robot Deployment"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# docker-compose.robot.yml\nversion: \'3.8\'\n\nservices:\n  digital-humanoid-agent:\n    build:\n      context: .\n      dockerfile: Dockerfile.robot\n    environment:\n      - ROS_DOMAIN_ID=0\n      - NVIDIA_VISIBLE_DEVICES=all\n      - NVIDIA_DRIVER_CAPABILITIES=all\n    volumes:\n      - /dev:/dev\n      - ./config:/app/config\n      - ./logs:/app/logs\n    devices:\n      - /dev/snd:/dev/snd\n      - /dev/video0:/dev/video0  # Camera access\n    network_mode: host\n    privileged: true  # Required for hardware access\n    command: ["ros2", "launch", "digital_humanoid_agent", "digital_humanoid_agent.launch.py"]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise-43-deploy-the-complete-digital-humanoid-agent",children:"Hands-on Exercise 4.3: Deploy the Complete Digital Humanoid Agent"}),"\n",(0,o.jsx)(n.p,{children:"Deploy and test the complete integrated system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prepare the system configuration"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Create configuration directory\nmkdir -p ~/ros2_ws/src/digital_humanoid_agent/config\n\n# Create main configuration file\ncat > ~/ros2_ws/src/digital_humanoid_agent/config/agent_config.yaml << EOF\ndigital_humanoid_agent:\n  ros__parameters:\n    use_sim_time: false\n    whisper_model: "small"\n    llm_api_key: "your-api-key-here"\n    system_timeout: 30.0\n    max_replanning_attempts: 3\nEOF\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Build and launch the system"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Build the workspace\ncd ~/ros2_ws\ncolcon build --packages-select digital_humanoid_agent\n\n# Source the workspace\nsource install/setup.bash\n\n# Launch the complete system\nros2 launch digital_humanoid_agent digital_humanoid_agent.launch.py\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Test the complete system"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch the agent\nros2 launch digital_humanoid_agent digital_humanoid_agent.launch.py\n\n# Terminal 2: Send voice commands (simulated)\nros2 topic pub /voice_command std_msgs/String \"data: 'Go to the kitchen and bring me a cup'\"\n\n# Terminal 3: Monitor system status\nros2 topic echo /system_status\n\n# Terminal 4: Visualize in RViz\nrviz2\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Test scenarios to try"}),":"]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Move forward 2 meters"'}),"\n",(0,o.jsx)(n.li,{children:'"Turn left and look for the red ball"'}),"\n",(0,o.jsx)(n.li,{children:'"Go to the table and pick up the book"'}),"\n",(0,o.jsx)(n.li,{children:'"Find John and follow him"'}),"\n",(0,o.jsx)(n.li,{children:'"Navigate to the living room and wait there"'}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complete system integration"})," combines all modules into a functional digital humanoid agent"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice-to-action pipeline"})," connects natural language understanding to physical execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance monitoring"})," is essential for maintaining system reliability"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error handling and recovery"})," mechanisms ensure robust operation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Testing and validation"})," verify that the integrated system functions as expected"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment strategies"})," differ between simulation and real robot environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How does the complete integrated system compare to the sum of its individual components?"}),"\n",(0,o.jsx)(n.li,{children:"What challenges arise when integrating multiple complex AI systems into a single agent?"}),"\n",(0,o.jsx)(n.li,{children:"How can the digital humanoid agent be made more robust to real-world uncertainties?"}),"\n",(0,o.jsx)(n.li,{children:"What additional capabilities would you add to make the agent more useful in practical applications?"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,o.jsxs)(n.p,{children:["Siciliano, B., & Khatib, O. (Eds.). (2016). ",(0,o.jsx)(n.em,{children:"Springer handbook of robotics"})," (2nd ed.). Springer."]}),"\n",(0,o.jsxs)(n.p,{children:["LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. ",(0,o.jsx)(n.em,{children:"Nature"}),", 521(7553), 436-444. ",(0,o.jsx)(n.a,{href:"https://doi.org/10.1038/nature14539",children:"https://doi.org/10.1038/nature14539"})]}),"\n",(0,o.jsxs)(n.p,{children:["Goodfellow, I., Bengio, Y., & Courville, A. (2016). ",(0,o.jsx)(n.em,{children:"Deep learning"}),". MIT Press."]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This capstone lesson brought together all the concepts from the textbook into a complete digital humanoid agent. We explored the complete system architecture, implemented the voice-to-action pipeline, configured deployment scenarios, and established testing procedures. The integrated system demonstrates how Vision-Language-Action components work together to create an intelligent humanoid robot capable of natural human interaction and autonomous task execution."}),"\n",(0,o.jsx)(n.p,{children:"This concludes Module 4 and the Physical AI & Humanoid Robotics textbook. You now have the knowledge to develop, implement, and deploy sophisticated humanoid robot systems that can perceive, understand, and act in the physical world."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);